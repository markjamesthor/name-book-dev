# server.py (ìµœì í™” ë²„ì „: FP16 + Warmup + ë³´ì•ˆ ê°•í™”)
import sys
import os

# === ë¡œê·¸ íŒŒì¼ ì„¤ì • (stdout/stderr â†’ íŒŒì¼ì— ê¸°ë¡) ===
LOG_PATH = os.environ.get("SERVER_LOG", r"C:\Users\taeho\server.log")

class _LogWriter:
    """stdout/stderrë¥¼ íŒŒì¼ì— ê¸°ë¡. ì½˜ì†”ì´ ìˆìœ¼ë©´ ì½˜ì†”ì—ë„ ì¶œë ¥."""
    def __init__(self, log_file, original=None):
        self.log_file = log_file
        self.original = original
        self._has_console = False
        if original is not None:
            try:
                original.write("")
                self._has_console = True
            except Exception:
                pass
    def write(self, data):
        if not data:
            return
        self.log_file.write(data)
        self.log_file.flush()
        if self._has_console:
            try:
                self.original.write(data)
                self.original.flush()
            except Exception:
                self._has_console = False
    def flush(self):
        self.log_file.flush()
        if self._has_console:
            try:
                self.original.flush()
            except Exception:
                pass
    def isatty(self):
        return False

try:
    _log_file = open(LOG_PATH, "a", encoding="utf-8", buffering=1)
    sys.stdout = _LogWriter(_log_file, sys.__stdout__)
    sys.stderr = _LogWriter(_log_file, sys.__stderr__)
    print(f"\n{'='*60}")
    from datetime import datetime
    print(f"ğŸ“‹ ì„œë²„ ì‹œì‘: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ“‹ ë¡œê·¸ íŒŒì¼: {LOG_PATH}")
    print(f"{'='*60}")
except Exception as e:
    # ë¡œê·¸ íŒŒì¼ ì—´ê¸° ì‹¤íŒ¨ ì‹œì—ë„ ì„œë²„ëŠ” ì •ìƒ ë™ì‘í•´ì•¼ í•¨
    try:
        sys.__stderr__.write(f"âš ï¸ ë¡œê·¸ íŒŒì¼ ì—´ê¸° ì‹¤íŒ¨: {e}\n")
    except Exception:
        pass

from fastapi import FastAPI, File, UploadFile, HTTPException, Query, Form, Body
from fastapi.responses import Response, JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from transformers import AutoModelForImageSegmentation
from torchvision import transforms
from pydantic import BaseModel
from typing import Optional, List, Dict, Any, Literal
from PIL import Image, ImageOps
from pillow_heif import register_heif_opener
register_heif_opener()
import torch
import gc
import io
import time
import asyncio
import threading
import numpy as np
import os
import json
import re
import traceback
import httpx
import base64
from pathlib import Path

# Ryan Engine ì„í¬íŠ¸
sys.path.insert(0, str(Path(__file__).parent))
try:
    from ryan_engine import JosaUtils, BookGenerator
    RYAN_ENGINE_AVAILABLE = True
    print("âœ… Ryan Engine ë¡œë“œ ì™„ë£Œ")
except ImportError as e:
    RYAN_ENGINE_AVAILABLE = False
    print(f"âš ï¸ Ryan Engine ë¡œë“œ ì‹¤íŒ¨: {e}")

# BGQA (ë°°ê²½ ì œê±° í’ˆì§ˆ í‰ê°€) ì„í¬íŠ¸
try:
    from bgqa import evaluate as bgqa_evaluate
    BGQA_AVAILABLE = True
    print("âœ… BGQA ë¡œë“œ ì™„ë£Œ")
except ImportError as e:
    BGQA_AVAILABLE = False
    print(f"âš ï¸ BGQA ë¡œë“œ ì‹¤íŒ¨: {e}")

# SAM2 ì„í¬íŠ¸
try:
    from sam2.sam2_image_predictor import SAM2ImagePredictor
    from sam2.automatic_mask_generator import SAM2AutomaticMaskGenerator
    SAM2_AVAILABLE = True
    print("âœ… SAM2 ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ")
except ImportError as e:
    SAM2_AVAILABLE = False
    print(f"âš ï¸ SAM2 ëª¨ë“ˆ ì—†ìŒ: {e}")

# Grounding DINO ì„í¬íŠ¸
try:
    from transformers import AutoProcessor as GDinoProcessor, AutoModelForZeroShotObjectDetection
    GDINO_AVAILABLE = True
    print("âœ… Grounding DINO ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ")
except ImportError as e:
    GDINO_AVAILABLE = False
    print(f"âš ï¸ Grounding DINO ëª¨ë“ˆ ì—†ìŒ: {e}")

# Florence-2 ì„í¬íŠ¸
try:
    from transformers import AutoModelForCausalLM as Florence2Model, AutoProcessor as Florence2Processor
    FLORENCE2_AVAILABLE = True
    print("âœ… Florence-2 ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ")
except ImportError as e:
    FLORENCE2_AVAILABLE = False
    print(f"âš ï¸ Florence-2 ëª¨ë“ˆ ì—†ìŒ: {e}")

# flash_attn ë¯¸ì„¤ì¹˜ ëŒ€ì‘ íŒ¨ì¹˜ (Windows ë“±)
# íŒ¨ì¹˜ ì „ì— ì›ë³¸ í•¨ìˆ˜ ì°¸ì¡°ë¥¼ ìº¡ì²˜ (mock.patch í›„ ì¬ì„í¬íŠ¸ ì‹œ ìê¸° ìì‹  ì°¸ì¡° ë°©ì§€)
try:
    from transformers.dynamic_module_utils import get_imports as _original_get_imports
except ImportError:
    _original_get_imports = None

def _fixed_get_imports(filename):
    """flash_attn ì„í¬íŠ¸ë¥¼ ì œê±°í•˜ëŠ” íŒ¨ì¹˜ â€” transformers.dynamic_module_utils.get_imports ëŒ€ì²´"""
    if _original_get_imports is None:
        return []
    imports = _original_get_imports(filename)
    if "flash_attn" in imports:
        imports.remove("flash_attn")
    return imports

# ViTMatte ì„í¬íŠ¸
try:
    from transformers import VitMatteForImageMatting, VitMatteImageProcessor
    VITMATTE_AVAILABLE = True
    print("âœ… ViTMatte ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ")
except ImportError as e:
    VITMATTE_AVAILABLE = False
    print(f"âš ï¸ ViTMatte ëª¨ë“ˆ ì—†ìŒ: {e}")

# PNG ì €ì¥ í´ë” ì„¤ì •
PNG_OUTPUT_DIR = Path("./png")
PNG_OUTPUT_DIR.mkdir(exist_ok=True)

# ViTPose transformers ë²„ê·¸ íŒ¨ì¹˜ (inv í•¨ìˆ˜ ëˆ„ë½ ë¬¸ì œ)
try:
    import transformers.models.vitpose.image_processing_vitpose as vitpose_module
    import numpy.linalg
    # ëª¨ë“ˆì˜ ê¸€ë¡œë²Œ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì— inv í•¨ìˆ˜ ì£¼ì…
    vitpose_module.__dict__['inv'] = numpy.linalg.inv
    # scipy_warp_affine í•¨ìˆ˜ì˜ ê¸€ë¡œë²Œì—ë„ ì£¼ì…
    if hasattr(vitpose_module, 'scipy_warp_affine'):
        vitpose_module.scipy_warp_affine.__globals__['inv'] = numpy.linalg.inv
    print("âœ… ViTPose íŒ¨ì¹˜ ì ìš© ì™„ë£Œ (inv í•¨ìˆ˜ ì£¼ì…)")
except Exception as e:
    print(f"âš ï¸ ViTPose íŒ¨ì¹˜ ìŠ¤í‚µ: {e}")

from starlette.requests import Request as StarletteRequest
from starlette.middleware.base import BaseHTTPMiddleware

app = FastAPI()

class RequestLogMiddleware(BaseHTTPMiddleware):
    async def dispatch(self, request: StarletteRequest, call_next):
        client = request.client.host if request.client else "unknown"
        path = request.url.path
        qs = str(request.url.query)
        cl = request.headers.get("content-length", "?")
        print(f"ğŸ”µ [{client}] {request.method} {path}{'?' + qs if qs else ''} (body: {cl} bytes)")
        try:
            response = await call_next(request)
            print(f"ğŸŸ¢ [{client}] {request.method} {path} â†’ {response.status_code}")
            return response
        except Exception as e:
            print(f"ğŸ”´ [{client}] {request.method} {path} â†’ ERROR: {e}")
            raise

app.add_middleware(RequestLogMiddleware)

# í—ˆìš©ëœ Origin ëª©ë¡ (í”„ë¡œë•ì…˜ì—ì„œëŠ” ì‹¤ì œ ë„ë©”ì¸ìœ¼ë¡œ ë³€ê²½)
ALLOWED_ORIGINS = [
    "http://localhost:3000",
    "http://localhost:5500",
    "http://localhost:8080",
    "http://localhost:8888",
    "http://127.0.0.1:3000",
    "http://127.0.0.1:5500",
    "http://127.0.0.1:8080",
    "http://127.0.0.1:8888",
    "null",  # file:// í”„ë¡œí† ì½œìš©
]

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"] if os.environ.get("CORS_ALLOW_ALL", "1") == "1" else ALLOWED_ORIGINS,
    allow_credentials=True,
    allow_methods=["GET", "POST"],  # GET ì¶”ê°€ (í—¬ìŠ¤ì²´í¬ ë“±)
    allow_headers=["Content-Type"],  # í•„ìš”í•œ í—¤ë”ë§Œ í—ˆìš©
    expose_headers=["X-Original-Width", "X-Original-Height", "X-Crop-X", "X-Crop-Y", "X-Crop-Width", "X-Crop-Height", "X-BGQA-Score", "X-BGQA-Passed", "X-BGQA-Issues", "X-BGQA-CaseType", "X-SAM2-Score", "X-Mask-Width", "X-Mask-Height"],  # í´ë¼ì´ì–¸íŠ¸ì—ì„œ ì½ì„ ìˆ˜ ìˆëŠ” ì»¤ìŠ¤í…€ í—¤ë”
)

# íŒŒì¼ ê²€ì¦ ìƒìˆ˜
MAX_FILE_SIZE = 50 * 1024 * 1024  # 50MB
ALLOWED_CONTENT_TYPES = {"image/jpeg", "image/png", "image/webp", "image/gif", "image/heic", "image/heif"}
ALLOWED_EXTENSIONS = {".jpg", ".jpeg", ".png", ".webp", ".gif", ".heic", ".heif"}

def is_allowed_image(file) -> bool:
    """content_type ë˜ëŠ” í™•ì¥ìë¡œ ì´ë¯¸ì§€ íŒŒì¼ ì—¬ë¶€ í™•ì¸"""
    if file.content_type in ALLOWED_CONTENT_TYPES:
        return True
    if file.filename:
        ext = os.path.splitext(file.filename)[1].lower()
        if ext in ALLOWED_EXTENSIONS:
            return True
    return False

# 1. ë””ë°”ì´ìŠ¤ ì„¤ì •
if torch.backends.mps.is_available():
    device = "mps"
    dtype = torch.float16  # [ìµœì í™”] ë§¥ë¶ì€ float16ì´ í›¨ì”¬ ë¹ ë¦„
elif torch.cuda.is_available():
    device = "cuda"
    dtype = torch.float16
else:
    device = "cpu"
    dtype = torch.float32

# Portrait ëª¨ë¸ì„ CPUë¡œ ëŒë ¤ BEN2(GPU)ì™€ ë³‘ë ¬ ì²˜ë¦¬
PORTRAIT_ON_CPU = False

print(f"\nğŸš€ ì´ˆê³ ì† AI ì„œë²„ ëŒ€ê¸° ì¤‘... (Device: {device}, Type: {dtype})")
if PORTRAIT_ON_CPU:
    print(f"   â†³ Portrait ëª¨ë¸: CPU (float32) â€” BEN2ì™€ ë³‘ë ¬ ì²˜ë¦¬ ê°€ëŠ¥")

def clear_gpu_memory():
    """GPU ë©”ëª¨ë¦¬ ìºì‹œ í•´ì œ"""
    gc.collect()
    if device == "mps":
        torch.mps.empty_cache()
    elif device == "cuda":
        torch.cuda.empty_cache()

# BEN2 ì„í¬íŠ¸
try:
    from ben2 import BEN_Base
    BEN2_AVAILABLE = True
    print("âœ… BEN2 ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ")
except ImportError:
    BEN2_AVAILABLE = False
    print("âš ï¸ BEN2 ëª¨ë“ˆ ì—†ìŒ (pip install ben2)")

# SAM2 ëª¨ë¸ (Lazy Loading)
sam2_predictor = None
sam2_lock = threading.Lock()  # set_image â†’ predict ì›ìì„± ë³´ì¥

def get_sam2_predictor():
    """SAM2 ëª¨ë¸ ë¡œë“œ (Lazy Loading)"""
    global sam2_predictor
    if sam2_predictor is not None:
        return sam2_predictor
    if not SAM2_AVAILABLE:
        raise ValueError("SAM2 ëª¨ë“ˆì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install sam2")
    print("ğŸ“‚ SAM2 ëª¨ë¸ ë¡œë”© ì¤‘ (sam2.1-hiera-large)...")
    sam2_predictor = SAM2ImagePredictor.from_pretrained("facebook/sam2.1-hiera-large", device=device)
    print(f"âœ… SAM2 ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (device: {device})")
    return sam2_predictor

# SAM2 AutomaticMaskGenerator (Lazy Loading â€” predictor.model ê³µìœ )
sam2_mask_generator = None

def get_sam2_mask_generator():
    """SAM2 AutomaticMaskGenerator ë¡œë“œ (ê¸°ì¡´ predictorì˜ model ê³µìœ )"""
    global sam2_mask_generator
    if sam2_mask_generator is not None:
        return sam2_mask_generator
    predictor = get_sam2_predictor()  # ëª¨ë¸ ê³µìœ 
    print("ğŸ“‚ SAM2 AutomaticMaskGenerator ì´ˆê¸°í™” ì¤‘...")
    sam2_mask_generator = SAM2AutomaticMaskGenerator.from_pretrained(
        "facebook/sam2.1-hiera-large",
        points_per_side=32,
        pred_iou_thresh=0.7,
        stability_score_thresh=0.85,
        min_mask_region_area=100,
    )
    print("âœ… SAM2 AutomaticMaskGenerator ì¤€ë¹„ ì™„ë£Œ")
    return sam2_mask_generator

# Grounding DINO ëª¨ë¸ (Lazy Loading)
gdino_model = None
gdino_processor = None

def get_gdino_model():
    """Grounding DINO ëª¨ë¸ ë¡œë“œ (Lazy Loading)"""
    global gdino_model, gdino_processor
    if gdino_model is not None:
        return gdino_model, gdino_processor
    if not GDINO_AVAILABLE:
        raise ValueError("Grounding DINOê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    print("ğŸ“‚ Grounding DINO ëª¨ë¸ ë¡œë”© ì¤‘ (grounding-dino-tiny)...")
    gdino_processor = GDinoProcessor.from_pretrained("IDEA-Research/grounding-dino-tiny")
    gdino_model = AutoModelForZeroShotObjectDetection.from_pretrained("IDEA-Research/grounding-dino-tiny")
    gdino_model.to(device)
    gdino_model.eval()
    print(f"âœ… Grounding DINO ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (device: {device})")
    return gdino_model, gdino_processor

# MM-DINO ëª¨ë¸ (Lazy Loading)
mmdino_model = None
mmdino_processor = None

def get_mmdino_model():
    """MM-DINO ëª¨ë¸ ë¡œë“œ (Lazy Loading) â€” 50.6 AP, Swin-Tiny ë°±ë³¸"""
    global mmdino_model, mmdino_processor
    if mmdino_model is not None:
        return mmdino_model, mmdino_processor
    if not GDINO_AVAILABLE:
        raise ValueError("Grounding DINOê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    print("ğŸ“‚ MM-DINO ëª¨ë¸ ë¡œë”© ì¤‘ (mm_grounding_dino_tiny)...")
    mmdino_processor = GDinoProcessor.from_pretrained("openmmlab-community/mm_grounding_dino_tiny_o365v1_goldg_v3det")
    mmdino_model = AutoModelForZeroShotObjectDetection.from_pretrained("openmmlab-community/mm_grounding_dino_tiny_o365v1_goldg_v3det")
    mmdino_model.to(device)
    mmdino_model.eval()
    print(f"âœ… MM-DINO ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (device: {device})")
    return mmdino_model, mmdino_processor

# Grounding DINO Base ëª¨ë¸ (Lazy Loading)
gdino_base_model = None
gdino_base_processor = None

def get_gdino_base_model():
    """Grounding DINO Base ëª¨ë¸ ë¡œë“œ (Lazy Loading) â€” 52.5 AP, Swin-Base ë°±ë³¸"""
    global gdino_base_model, gdino_base_processor
    if gdino_base_model is not None:
        return gdino_base_model, gdino_base_processor
    if not GDINO_AVAILABLE:
        raise ValueError("Grounding DINOê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    print("ğŸ“‚ Grounding DINO Base ëª¨ë¸ ë¡œë”© ì¤‘ (grounding-dino-base)...")
    gdino_base_processor = GDinoProcessor.from_pretrained("IDEA-Research/grounding-dino-base")
    gdino_base_model = AutoModelForZeroShotObjectDetection.from_pretrained("IDEA-Research/grounding-dino-base")
    gdino_base_model.to(device)
    gdino_base_model.eval()
    print(f"âœ… Grounding DINO Base ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (device: {device})")
    return gdino_base_model, gdino_base_processor

# Florence-2 ëª¨ë¸ (Lazy Loading)
florence2_model = None
florence2_processor = None

def get_florence2_model():
    """Florence-2-large-ft ëª¨ë¸ ë¡œë“œ (Lazy Loading) â€” FP16, SDPA attention"""
    global florence2_model, florence2_processor
    if florence2_model is not None:
        return florence2_model, florence2_processor
    if not FLORENCE2_AVAILABLE:
        raise ValueError("Florence-2ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    print("ğŸ“‚ Florence-2-large-ft ëª¨ë¸ ë¡œë”© ì¤‘...")
    import unittest.mock
    # flash_attn ë¯¸ì„¤ì¹˜ í™˜ê²½ ëŒ€ì‘: get_imports íŒ¨ì¹˜
    with unittest.mock.patch("transformers.dynamic_module_utils.get_imports", _fixed_get_imports):
        florence2_model = Florence2Model.from_pretrained(
            "microsoft/Florence-2-large-ft",
            torch_dtype=torch.float16,
            attn_implementation="sdpa",
            trust_remote_code=True,
        )
    florence2_model.to(device)
    florence2_model.eval()
    florence2_processor = Florence2Processor.from_pretrained(
        "microsoft/Florence-2-large-ft",
        trust_remote_code=True,
    )
    print(f"âœ… Florence-2-large-ft ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (device: {device})")
    return florence2_model, florence2_processor

# ViTMatte ëª¨ë¸ (Lazy Loading)
vitmatte_model = None
vitmatte_processor = None

def get_vitmatte_model():
    """ViTMatte ëª¨ë¸ ë¡œë“œ (Lazy Loading)"""
    global vitmatte_model, vitmatte_processor
    if vitmatte_model is not None:
        return vitmatte_model, vitmatte_processor
    if not VITMATTE_AVAILABLE:
        raise ValueError("ViTMatteê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    print("ğŸ“‚ ViTMatte ëª¨ë¸ ë¡œë”© ì¤‘ (vitmatte-small)...")
    vitmatte_processor = VitMatteImageProcessor.from_pretrained("hustvl/vitmatte-small-composition-1k")
    vitmatte_model = VitMatteForImageMatting.from_pretrained("hustvl/vitmatte-small-composition-1k")
    vitmatte_model.to(device)
    vitmatte_model.half()
    vitmatte_model.eval()
    print(f"âœ… ViTMatte ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (device: {device})")
    return vitmatte_model, vitmatte_processor

# remove.bg API ì„¤ì •
REMOVEBG_API_KEY = os.environ.get("REMOVEBG_API_KEY", "D8B2GQyMvmfbXXfH2mZukPi4")
REMOVEBG_ENABLED = os.environ.get("REMOVEBG_ENABLED", "true").lower() == "true"

# 2. ëª¨ë¸ ì„¤ì • (Lazy Loading)
# ì§€ì›ë˜ëŠ” BiRefNet ëª¨ë¸ë“¤ (ëª¨ë‘ ë¡œì»¬)
BIREFNET_MODELS = {
    "portrait": "./models/birefnet-portrait",
    "hr": "./models/birefnet-hr",
    "hr-matting": "./models/birefnet-hr-matting",
    "dynamic": "./models/birefnet-dynamic",
    "rmbg2": "./models/rmbg2",
    # Alpha matting ëª¨ë¸ (soft alpha, ë¨¸ë¦¬ì¹´ë½ í•œ ì˜¬ê¹Œì§€ ì²˜ë¦¬)
    "matting": "./models/birefnet-matting",
    "hr-matting-alpha": "./models/birefnet-hr-matting-alpha",
    "dynamic-matting": "./models/birefnet-dynamic-matting",
}

# torch.compile ê°€ìš©ì„± ì²´í¬ (Triton í•„ìš”)
TORCH_COMPILE_OK = False
if os.environ.get("TORCH_COMPILE", "1") == "1":
    try:
        import triton
        TORCH_COMPILE_OK = True
        print("âœ… Triton ê°ì§€ â€” torch.compile í™œì„±í™”")
    except ImportError:
        print("âš ï¸ Triton ë¯¸ì„¤ì¹˜ â€” torch.compile ë¹„í™œì„±í™” (WindowsëŠ” ë¯¸ì§€ì›)")

# ë¡œë“œëœ ëª¨ë¸ ìºì‹œ
loaded_models = {}
ben2_model = None

def get_ben2_model():
    """BEN2 ëª¨ë¸ ë¡œë“œ (Lazy Loading)"""
    global ben2_model
    if ben2_model is not None:
        return ben2_model
    if not BEN2_AVAILABLE:
        raise ValueError("BEN2 ëª¨ë“ˆì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install ben2")
    print("ğŸ“‚ BEN2 ëª¨ë¸ ë¡œë”© ì¤‘...")
    ben2_model = BEN_Base.from_pretrained("PramaLLC/BEN2")
    ben2_model.to(device)
    ben2_model.eval()
    print("âœ… BEN2 ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    return ben2_model

async def call_removebg_api(image_data: bytes, size: str = "preview") -> Image.Image:
    """remove.bg API í˜¸ì¶œí•˜ì—¬ ë°°ê²½ ì œê±°ëœ RGBA ì´ë¯¸ì§€ ë°˜í™˜"""
    if not REMOVEBG_API_KEY:
        raise ValueError("REMOVEBG_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    if size not in ("preview", "full"):
        size = "preview"
    async with httpx.AsyncClient(timeout=60.0) as client:
        resp = await client.post(
            "https://api.remove.bg/v1.0/removebg",
            headers={"X-Api-Key": REMOVEBG_API_KEY},
            files={"image_file": ("image.jpg", image_data, "image/jpeg")},
            data={"size": size, "format": "png", "channels": "rgba"},
        )
    if resp.status_code != 200:
        error_detail = resp.json().get("errors", [{}])[0].get("title", resp.text) if resp.headers.get("content-type", "").startswith("application/json") else resp.text[:200]
        raise ValueError(f"remove.bg API ì˜¤ë¥˜ ({resp.status_code}): {error_detail}")
    return Image.open(io.BytesIO(resp.content)).convert("RGBA")

def get_birefnet_model(model_type: str = "portrait") -> AutoModelForImageSegmentation:
    """BiRefNet ëª¨ë¸ ë¡œë“œ (Lazy Loading)"""
    global loaded_models

    if model_type in loaded_models:
        return loaded_models[model_type]

    model_path = BIREFNET_MODELS.get(model_type)
    if not model_path:
        raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸: {model_type}")

    print(f"ğŸ“‚ {model_type} ëª¨ë¸ ë¡œë”© ì¤‘... ({model_path})")

    try:
        model = AutoModelForImageSegmentation.from_pretrained(
            model_path,
            trust_remote_code=True,
            local_files_only=True
        )
    except OSError as e:
        print(f"âŒ ì˜¤ë¥˜: ëª¨ë¸ í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤. ({model_path})")
        raise e

    # Portrait â†’ CPU(float32), ë‚˜ë¨¸ì§€ â†’ GPU(float16)ë¡œ ë³‘ë ¬ ì²˜ë¦¬ ê°€ëŠ¥
    target_device = device
    if PORTRAIT_ON_CPU and model_type == "portrait":
        target_device = "cpu"

    model.to(target_device)
    if target_device != "cpu":
        model.half()
    model.eval()
    print(f"   â†³ ë””ë°”ì´ìŠ¤: {target_device}")

    # torch.compile ìµœì í™” (Triton í•„ìš” â€” Linux/WSLë§Œ ì§€ì›)
    if TORCH_COMPILE_OK:
        try:
            model = torch.compile(model)
            print(f"   â†³ torch.compile ì ìš©")
        except Exception as e:
            print(f"   âš ï¸ torch.compile ìŠ¤í‚µ: {e}")

    loaded_models[model_type] = model
    print(f"âœ… {model_type} ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    return model

# 3. ëª¨ë“  ëª¨ë¸ ì‚¬ì „ ë¡œë“œ + ì›Œë°ì—…
# ì„œë²„ ì‹œì‘ ì‹œ 3ê°œ ëª¨ë¸ ëª¨ë‘ VRAMì— ì˜¬ë ¤ë‘ê¸° (ì²« ìš”ì²­ ì§€ì—° ì œê±°)

def warmup_birefnet(model, name):
    """BiRefNet ëª¨ë¸ ì›Œë°ì—… (torch.compile ì²« ì‹¤í–‰ ê·¸ë˜í”„ ìƒì„± í¬í•¨)"""
    model_device = next(model.parameters()).device
    print(f"ğŸ”¥ {name} ì›Œë°ì—… ì¤‘ ({model_device})...")
    with torch.no_grad():
        dummy = torch.randn(1, 3, 1024, 1024).to(model_device)
        if model_device.type != "cpu":
            dummy = dummy.half()
        model(dummy)
        del dummy
    clear_gpu_memory()
    print(f"   âœ… {name} ì›Œë°ì—… ì™„ë£Œ")

# Portrait ëª¨ë¸
print("ğŸ“‚ ëª¨ë“  ë°°ê²½ ì œê±° ëª¨ë¸ ì‚¬ì „ ë¡œë”© ì¤‘...")
try:
    portrait_model = get_birefnet_model("portrait")
except OSError:
    print(f"âŒ ì˜¤ë¥˜: portrait ëª¨ë¸ í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤.")
    exit()
warmup_birefnet(portrait_model, "portrait")

# hr-matting ëª¨ë¸
try:
    hrmatting_model = get_birefnet_model("hr-matting")
    warmup_birefnet(hrmatting_model, "hr-matting")
except Exception as e:
    print(f"âš ï¸ hr-matting ì‚¬ì „ ë¡œë“œ ì‹¤íŒ¨: {e}")

# BEN2 ëª¨ë¸
if BEN2_AVAILABLE:
    try:
        ben2 = get_ben2_model()
        # BEN2 ì›Œë°ì—…: ë”ë¯¸ ì´ë¯¸ì§€ë¡œ inference í•œ ë²ˆ
        print(f"ğŸ”¥ BEN2 ì›Œë°ì—… ì¤‘ ({device})...")
        dummy_img = Image.new("RGB", (512, 512), (128, 128, 128))
        with torch.no_grad():
            ben2.inference(dummy_img)
        del dummy_img
        clear_gpu_memory()
        print(f"   âœ… BEN2 ì›Œë°ì—… ì™„ë£Œ")
    except Exception as e:
        print(f"âš ï¸ BEN2 ì‚¬ì „ ë¡œë“œ ì‹¤íŒ¨: {e}")

print("âœ… ëª¨ë“  ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ!")

# ì •ê·œí™” ì„¤ì •
transform_normalize = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])

def process_image_fast(image: Image.Image, max_size: int = 1440, model_type: str = "portrait") -> Image.Image:
    """
    ì´ë¯¸ì§€ ë°°ê²½ ì œê±° ì²˜ë¦¬
    max_size: ì²˜ë¦¬ í•´ìƒë„ (720=ë¹ ë¦„, 1024=ì¤‘ê°„, 1440=ê¶Œì¥, 2048=ìµœê³ í’ˆì§ˆ, 9999=ì›ë³¸)
    model_type: BiRefNet ëª¨ë¸ ì¢…ë¥˜ (portrait, hr, hr-matting, dynamic)
    """
    w, h = image.size

    # ì›ë³¸ í™”ì§ˆ ëª¨ë“œ (9999 ì´ìƒì´ë©´ ë¦¬ì‚¬ì´ì¦ˆ ì•ˆí•¨)
    if max_size >= 9999:
        # ì›ë³¸ í¬ê¸° ì‚¬ìš© (32ì˜ ë°°ìˆ˜ë¡œë§Œ ì¡°ì •)
        new_w = (w // 32) * 32
        new_h = (h // 32) * 32
        print(f"ğŸ“ ì›ë³¸ í™”ì§ˆ ëª¨ë“œ: {w}x{h} â†’ {new_w}x{new_h}")
    else:
        # í—ˆìš©ëœ í•´ìƒë„ ë²”ìœ„ë¡œ ì œí•œ (ë³´ì•ˆ)
        max_size = max(512, min(2500, max_size))
        scale = min(max_size / w, max_size / h)
        new_w = int(w * scale)
        new_h = int(h * scale)
        new_w = (new_w // 32) * 32
        new_h = (new_h // 32) * 32

    # MPSëŠ” ê³ í•´ìƒë„ convolution ë¯¸ì§€ì› â†’ ì•ˆì „í•œ ìµœëŒ€ í•´ìƒë„ë¡œ í´ë¨í•‘
    # Portrait on CPUë©´ ì´ ì œí•œ ì ìš© ì•ˆ í•¨
    MPS_MAX_SIDE = 2560
    model_on_mps = not (PORTRAIT_ON_CPU and model_type == "portrait") and device == "mps"
    if model_on_mps and max(new_w, new_h) > MPS_MAX_SIDE:
        scale_down = MPS_MAX_SIDE / max(new_w, new_h)
        new_w = (int(new_w * scale_down) // 32) * 32
        new_h = (int(new_h * scale_down) // 32) * 32
        print(f"âš ï¸ MPS í•œê³„ â†’ ì²˜ë¦¬ í•´ìƒë„ ì¶•ì†Œ: {new_w}x{new_h}")

    # ë¦¬ì‚¬ì´ì§•
    image_resized = image.resize((new_w, new_h), Image.Resampling.LANCZOS)

    # ëª¨ë¸ ê°€ì ¸ì˜¤ê¸° (Lazy Loading)
    model = get_birefnet_model(model_type)

    # ëª¨ë¸ ë””ë°”ì´ìŠ¤ ìë™ ê°ì§€ (portrait=CPU, ë‚˜ë¨¸ì§€=GPU)
    model_device = next(model.parameters()).device

    # í…ì„œ ë³€í™˜ â€” ëª¨ë¸ ë””ë°”ì´ìŠ¤ì— ë§ì¶¤
    input_tensor = transform_normalize(image_resized).unsqueeze(0).to(model_device)

    # GPU(float16) / CPU(float32) ìë™ íŒë³„
    if model_device.type != "cpu":
        input_tensor = input_tensor.half()

    # ì¶”ë¡ 
    with torch.no_grad():
        preds = model(input_tensor)[-1].sigmoid().cpu()

    # ë§ˆìŠ¤í¬ ë³µì›
    pred = preds[0].squeeze().float() # ë‹¤ì‹œ float32ë¡œ ë³€í™˜ (ì´ë¯¸ì§€ ì €ì¥ìš©)
    pred_pil = transforms.ToPILImage()(pred)
    mask = pred_pil.resize((w, h), Image.Resampling.LANCZOS)

    return mask

# ========== ë§ˆìŠ¤í¬ ë¦¬íŒŒì¸ í•¨ìˆ˜ë“¤ ==========
def refine_guided_filter(image: Image.Image, mask: Image.Image, r: int = 8, eps: float = 1e-3) -> Image.Image:
    """Guided Filter: ì›ë³¸ ì´ë¯¸ì§€ ì—£ì§€ë¥¼ ì°¸ì¡°í•˜ì—¬ ë§ˆìŠ¤í¬ ê²½ê³„ ì •ì œ"""
    import cv2
    guide = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2GRAY).astype(np.float32) / 255.0
    src = np.array(mask).astype(np.float32) / 255.0
    refined = cv2.ximgproc.guidedFilter(guide, src, radius=r, eps=eps)
    refined = np.clip(refined * 255, 0, 255).astype(np.uint8)
    return Image.fromarray(refined)

def refine_pymatting(image: Image.Image, mask: Image.Image) -> Image.Image:
    """PyMatting: ì•ŒíŒŒ ë§¤íŒ…ìœ¼ë¡œ ë°˜íˆ¬ëª… ê²½ê³„ ì •ë°€ ì²˜ë¦¬"""
    from pymatting import estimate_alpha_cf
    # ì²˜ë¦¬ ì†ë„ë¥¼ ìœ„í•´ ìµœëŒ€ 1024ë¡œ ì¶•ì†Œ í›„ ë‹¤ì‹œ ë³µì›
    orig_size = mask.size
    max_side = 1024
    if max(orig_size) > max_side:
        scale = max_side / max(orig_size)
        small_size = (int(orig_size[0] * scale), int(orig_size[1] * scale))
        image_small = image.resize(small_size, Image.Resampling.LANCZOS)
        mask_small = mask.resize(small_size, Image.Resampling.LANCZOS)
    else:
        image_small = image
        mask_small = mask
        small_size = orig_size

    img_np = np.array(image_small).astype(np.float64) / 255.0
    mask_np = np.array(mask_small).astype(np.float64) / 255.0
    # trimap ìƒì„±: í™•ì‹¤í•œ ì „ê²½/ë°°ê²½ + ë¶ˆí™•ì‹¤ ì˜ì—­
    trimap = np.zeros_like(mask_np)
    trimap[mask_np > 0.9] = 1.0
    trimap[(mask_np > 0.1) & (mask_np <= 0.9)] = 0.5

    alpha = estimate_alpha_cf(img_np, trimap)
    alpha = np.clip(alpha * 255, 0, 255).astype(np.uint8)
    result = Image.fromarray(alpha)
    if small_size != orig_size:
        result = result.resize(orig_size, Image.Resampling.LANCZOS)
    return result

def refine_foreground_color(image: Image.Image, mask: Image.Image, r: int = 90) -> Image.Image:
    """Fast Foreground Estimation: ì „ê²½ ìƒ‰ìƒ ì¶”ì •ìœ¼ë¡œ ë°˜íˆ¬ëª… ì˜ì—­ ê°œì„ 
    ë§ˆìŠ¤í¬ê°€ ì•„ë‹Œ ì „ê²½ ì´ë¯¸ì§€ë¥¼ ë°˜í™˜ (ì•ŒíŒŒ í•©ì„± ì‹œ ìƒ‰ë²ˆì§ ì œê±°)"""
    import cv2
    # ì†ë„ë¥¼ ìœ„í•´ ìµœëŒ€ 1024ë¡œ ì¶•ì†Œ í›„ ì²˜ë¦¬
    orig_size = image.size
    max_side = 1024
    if max(orig_size) > max_side:
        scale = max_side / max(orig_size)
        small_size = (int(orig_size[0] * scale), int(orig_size[1] * scale))
        image_s = image.resize(small_size, Image.Resampling.LANCZOS)
        mask_s = mask.resize(small_size, Image.Resampling.LANCZOS)
    else:
        image_s = image
        mask_s = mask
        small_size = orig_size

    img_np = np.array(image_s).astype(np.float32) / 255.0
    mask_np = np.array(mask_s).astype(np.float32) / 255.0
    if mask_np.ndim == 2:
        mask_np = mask_np[:, :, np.newaxis]

    # Blur fusion (2íšŒ ë°˜ë³µì´ë©´ ì¶©ë¶„)
    for _ in range(2):
        blurred_img = cv2.GaussianBlur(img_np, (0, 0), sigmaX=r, sigmaY=r)
        blurred_mask = cv2.GaussianBlur(mask_np[:, :, 0], (0, 0), sigmaX=r, sigmaY=r)
        blurred_mask = np.maximum(blurred_mask, 1e-6)[:, :, np.newaxis]
        foreground = np.clip(blurred_img / blurred_mask, 0, 1)
        img_np = img_np * mask_np + foreground * (1 - mask_np)

    result = Image.fromarray((img_np * 255).astype(np.uint8))
    if small_size != orig_size:
        result = result.resize(orig_size, Image.Resampling.LANCZOS)
    return result

@app.post("/remove-bg")
async def remove_background(
    file: UploadFile = File(...),
    max_size: int = Query(default=1440, ge=512, le=9999, description="ì²˜ë¦¬ í•´ìƒë„ (512-2500, 9999=ì›ë³¸)"),
    model: str = Query(default="portrait", pattern="^(portrait|hr|hr-matting|dynamic|rmbg2|ben2|removebg|matting|hr-matting-alpha|dynamic-matting)$", description="ë°°ê²½ ì œê±° ëª¨ë¸"),
    removebg_size: str = Query(default="preview", pattern="^(preview|full)$", description="remove.bg í¬ê¸°: preview(ì €í•´ìƒë„) ë˜ëŠ” full(ì›ë³¸)"),
    case_type: str = Query(default="auto", description="í”¼ì‚¬ì²´ ìœ í˜•: auto, KID_PERSON, ADULT_PERSON, TOY_OBJECT"),
    has_face: bool = Query(default=True, description="ì–¼êµ´ ê°ì§€ ì—¬ë¶€ (Face API ê²°ê³¼)"),
    refine: str = Query(default="none", pattern="^(none|guided|pymatting|fg_estimate)$", description="ë§ˆìŠ¤í¬ ë¦¬íŒŒì¸ ë°©ë²•")
):
    print("-" * 40)
    print(f"ğŸ“¸ ìš”ì²­: {file.filename} (í’ˆì§ˆ: {max_size}px, ëª¨ë¸: {model}, ë¦¬íŒŒì¸: {refine})")
    start_time = time.time()

    # 1. íŒŒì¼ íƒ€ì… ê²€ì¦
    if file.content_type not in ALLOWED_CONTENT_TYPES:
        raise HTTPException(
            status_code=400,
            detail=f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤. í—ˆìš©: {', '.join(ALLOWED_CONTENT_TYPES)}"
        )

    # 2. íŒŒì¼ ì½ê¸° ë° í¬ê¸° ê²€ì¦
    image_data = await file.read()
    if len(image_data) > MAX_FILE_SIZE:
        raise HTTPException(
            status_code=400,
            detail=f"íŒŒì¼ì´ ë„ˆë¬´ í½ë‹ˆë‹¤. ìµœëŒ€ {MAX_FILE_SIZE // (1024*1024)}MBê¹Œì§€ í—ˆìš©ë©ë‹ˆë‹¤."
        )

    # 3. ì´ë¯¸ì§€ ìœ íš¨ì„± ê²€ì¦ (to_threadë¡œ ì´ë²¤íŠ¸ ë£¨í”„ ë¸”ë¡œí‚¹ ë°©ì§€)
    def _load_image(data):
        img = Image.open(io.BytesIO(data))
        img.verify()
        img = Image.open(io.BytesIO(data))
        img = ImageOps.exif_transpose(img)
        return img.convert("RGB")
    try:
        image = await asyncio.to_thread(_load_image, image_data)
    except Exception as e:
        raise HTTPException(
            status_code=400,
            detail="ì†ìƒëœ ì´ë¯¸ì§€ íŒŒì¼ì´ê±°ë‚˜ ì˜¬ë°”ë¥¸ ì´ë¯¸ì§€ í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤."
        )

    try:
        # ì›ë³¸ í¬ê¸° ì €ì¥ (í¬ë¡­ ì •ë³´ í—¤ë”ìš©)
        original_w, original_h = image.size

        if model == "removebg":
            if not REMOVEBG_ENABLED:
                raise HTTPException(status_code=403, detail="removebg APIê°€ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤. REMOVEBG_ENABLED=trueë¡œ ì„¤ì •í•˜ì„¸ìš”.")
            # remove.bg API í˜¸ì¶œ â€” HEIC ë“± ë¹„í‘œì¤€ í¬ë§·ì€ JPEGë¡œ ë³€í™˜í•˜ì—¬ ì „ì†¡
            buf = io.BytesIO()
            image.save(buf, format="JPEG", quality=95)
            jpeg_data = buf.getvalue()
            result_rgba = await call_removebg_api(jpeg_data, size=removebg_size)
            # ì›ë³¸ê³¼ í¬ê¸°ê°€ ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì›ë³¸ í¬ê¸°ë¡œ ë¦¬ì‚¬ì´ì¦ˆ
            if result_rgba.size != (original_w, original_h):
                result_rgba = result_rgba.resize((original_w, original_h), Image.Resampling.LANCZOS)
            mask = result_rgba.split()[-1]
        elif model == "ben2":
            # BEN2ëŠ” ìì²´ inference API ì‚¬ìš© (GPUì—ì„œ ì‹¤í–‰)
            # asyncio.to_threadë¡œ ì´ë²¤íŠ¸ ë£¨í”„ ë¸”ë¡œí‚¹ ë°©ì§€ â†’ portrait(CPU)ì™€ ë³‘ë ¬ ê°€ëŠ¥
            ben2 = get_ben2_model()
            def _run_ben2():
                with torch.no_grad():
                    return ben2.inference(image)
            result_rgba = await asyncio.to_thread(_run_ben2)
            # RGBA ê²°ê³¼ì—ì„œ ì•ŒíŒŒ ì±„ë„ì„ ë§ˆìŠ¤í¬ë¡œ ì¶”ì¶œ
            mask = result_rgba.split()[-1]
        else:
            # portrait ë“± BiRefNet ëª¨ë¸ (CPU ë˜ëŠ” GPU)
            # asyncio.to_threadë¡œ ì´ë²¤íŠ¸ ë£¨í”„ ë¸”ë¡œí‚¹ ë°©ì§€ â†’ ben2(GPU)ì™€ ë³‘ë ¬ ê°€ëŠ¥
            mask = await asyncio.to_thread(process_image_fast, image, max_size, model)

        # ë§ˆìŠ¤í¬ ë¦¬íŒŒì¸ ì ìš©
        if refine != "none":
            refine_start = time.time()
            if refine == "guided":
                mask = refine_guided_filter(image, mask)
                print(f"ğŸ”§ Guided Filter ë¦¬íŒŒì¸ ì™„ë£Œ ({time.time() - refine_start:.2f}ì´ˆ)")
            elif refine == "pymatting":
                mask = refine_pymatting(image, mask)
                print(f"ğŸ”§ PyMatting ë¦¬íŒŒì¸ ì™„ë£Œ ({time.time() - refine_start:.2f}ì´ˆ)")
            elif refine == "fg_estimate":
                # ì „ê²½ ìƒ‰ìƒ ì¶”ì •ì€ ë§ˆìŠ¤í¬ ì ìš© í›„ ì²˜ë¦¬ (ì•„ë˜ì—ì„œ)
                pass

        # BGQA í’ˆì§ˆ í‰ê°€ â€” í˜„ì¬ í”„ë¦¬ë·°ì—ì„œ ë¯¸ì‚¬ìš©, ìŠ¤í‚µí•˜ì—¬ ì†ë„ í–¥ìƒ
        bgqa_score = 100.0
        bgqa_passed = True
        bgqa_issues = []
        bgqa_case_type = "KID_PERSON"

        # fg_estimate: ì „ê²½ ìƒ‰ìƒ ì¶”ì •ìœ¼ë¡œ ë°˜íˆ¬ëª… ì˜ì—­ ìƒ‰ë²ˆì§ ì œê±°
        if refine == "fg_estimate":
            refine_start = time.time()
            refined_fg = refine_foreground_color(image, mask)
            image = refined_fg
            print(f"ğŸ”§ Foreground Estimation ë¦¬íŒŒì¸ ì™„ë£Œ ({time.time() - refine_start:.2f}ì´ˆ)")

        image.putalpha(mask)

        # ì•ŒíŒŒ ì±„ë„ ê¸°ì¤€ìœ¼ë¡œ ì½˜í…ì¸  ì˜ì—­ í¬ë¡­ (ë¹ˆ ê³µê°„ ì œê±°)
        alpha = image.split()[-1]  # ì•ŒíŒŒ ì±„ë„ ì¶”ì¶œ
        # ì•ŒíŒŒê°’ 30 ë¯¸ë§Œì€ íˆ¬ëª… ì²˜ë¦¬ (ë°°ê²½ ì”ì—¬ë¬¼/ë…¸ì´ì¦ˆ ì œê±°)
        alpha_clean = alpha.point(lambda x: 0 if x < 30 else x)
        bbox = alpha_clean.getbbox()  # ë¶ˆíˆ¬ëª… í”½ì…€ì˜ ë°”ìš´ë”© ë°•ìŠ¤

        # í¬ë¡­ ì¢Œí‘œ ì´ˆê¸°í™”
        crop_x, crop_y = 0, 0

        if bbox:
            # íŒ¨ë”© ì¶”ê°€ (20px)
            padding = 20
            x1, y1, x2, y2 = bbox
            crop_x = max(0, x1 - padding)
            crop_y = max(0, y1 - padding)
            x2 = min(image.width, x2 + padding)
            y2 = min(image.height, y2 + padding)

            # í¬ë¡­
            original_size = image.size
            image = image.crop((crop_x, crop_y, x2, y2))
            print(f"âœ‚ï¸  í¬ë¡­: {original_size} â†’ {image.size} (íŒ¨ë”© {padding}px)")

        img_byte_arr = io.BytesIO()
        # WebPë¡œ ì €ì¥ (PNGë³´ë‹¤ ì¸ì½”ë”© 2ë°° ë¹ ë¦„, ìš©ëŸ‰ 50% ê°ì†Œ)
        image.save(img_byte_arr, format='WEBP', quality=90)

        # PNG ì €ì¥ ìŠ¤í‚µ â€” í”„ë¦¬ë·° ì†ë„ ìš°ì„ 

        print(f"âš¡ ì™„ë£Œ! ì†Œìš”ì‹œê°„: {time.time() - start_time:.2f}ì´ˆ")
        print("-" * 40)

        # í¬ë¡­ ì •ë³´ë¥¼ í—¤ë”ì— í¬í•¨ (ë§ˆì»¤ ì¢Œí‘œ ë³´ì •ìš©)
        headers = {
            "X-Original-Width": str(original_w),
            "X-Original-Height": str(original_h),
            "X-Crop-X": str(crop_x),
            "X-Crop-Y": str(crop_y),
            "X-Crop-Width": str(image.width),
            "X-Crop-Height": str(image.height),
            "X-BGQA-Score": str(bgqa_score),
            "X-BGQA-Passed": str(bgqa_passed).lower(),
            "X-BGQA-Issues": ",".join(bgqa_issues) if bgqa_issues else "",
            "X-BGQA-CaseType": bgqa_case_type,
        }

        clear_gpu_memory()
        return Response(content=img_byte_arr.getvalue(), media_type="image/webp", headers=headers)
    except Exception as e:
        clear_gpu_memory()
        print(f"âŒ ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail="ì´ë¯¸ì§€ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì´ë¯¸ì§€ë¥¼ ì‹œë„í•´ì£¼ì„¸ìš”."
        )

# ========== ViTPose ëª¨ë¸ (Lazy Loading) ==========
# ê° ëª¨ë¸ì€ ìì²´ processorê°€ í•„ìš” (plus ëª¨ë¸ì€ configì´ ë‹¤ë¦„)
_vitpose_cache = {}  # model_type -> (model, processor)

VITPOSE_MODELS = {
    "vitpose": "usyd-community/vitpose-plus-base",       # 86M, 77.0 AP
    "vitpose-huge": "usyd-community/vitpose-plus-huge",   # 657M, 81.1 AP
}

def load_vitpose_model(model_type="vitpose"):
    """ViTPose ëª¨ë¸ ë¡œë“œ (ì²˜ìŒ ìš”ì²­ ì‹œì—ë§Œ)"""
    global _vitpose_cache

    if model_type in _vitpose_cache:
        return _vitpose_cache[model_type]

    try:
        from transformers import AutoProcessor, VitPoseForPoseEstimation

        model_name = VITPOSE_MODELS.get(model_type)
        if not model_name:
            raise ValueError(f"ì•Œ ìˆ˜ ì—†ëŠ” ViTPose ëª¨ë¸: {model_type}")

        print(f"ğŸ“‚ ViTPose ëª¨ë¸ ë¡œë”© ì¤‘... ({model_name})")
        processor = AutoProcessor.from_pretrained(model_name)
        model = VitPoseForPoseEstimation.from_pretrained(model_name)
        model.to(device)
        model.eval()
        print(f"âœ… ViTPose ëª¨ë¸ ë¡œë“œ ì™„ë£Œ ({model_type})")

        _vitpose_cache[model_type] = (model, processor)
        return model, processor

    except ImportError as e:
        print(f"âŒ Import ì˜¤ë¥˜: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"ViTPose ëª¨ë¸ì„ ì‚¬ìš©í•˜ë ¤ë©´ transformers>=4.49.0ì´ í•„ìš”í•©ë‹ˆë‹¤. ì˜¤ë¥˜: {str(e)}"
        )
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë“œ ì˜¤ë¥˜: {str(e)}")
        traceback.print_exc()
        raise HTTPException(
            status_code=500,
            detail=f"ViTPose ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {str(e)}"
        )

# COCO 17ê°œ í‚¤í¬ì¸íŠ¸ë¥¼ BlazePose 33ê°œì— ë§¤í•‘ (í˜¸í™˜ì„±)
COCO_TO_BLAZEPOSE = {
    0: 0,    # nose
    1: 2,    # left_eye
    2: 5,    # right_eye
    3: 7,    # left_ear
    4: 8,    # right_ear
    5: 11,   # left_shoulder
    6: 12,   # right_shoulder
    7: 13,   # left_elbow
    8: 14,   # right_elbow
    9: 15,   # left_wrist
    10: 16,  # right_wrist
    11: 23,  # left_hip
    12: 24,  # right_hip
    13: 25,  # left_knee
    14: 26,  # right_knee
    15: 27,  # left_ankle
    16: 28,  # right_ankle
}

# COCO ì†ëª© í‚¤í¬ì¸íŠ¸ ì¸ë±ìŠ¤
COCO_LEFT_WRIST = 9
COCO_RIGHT_WRIST = 10


def extract_wrist_keypoints(image: Image.Image, min_score: float = 0.3) -> list:
    """
    ViTPoseë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ì—ì„œ ì†ëª© í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ

    Args:
        image: PIL Image
        min_score: ìµœì†Œ ì‹ ë¢°ë„ (ê¸°ë³¸ê°’ 0.3)

    Returns:
        [(x1, y1), (x2, y2)] í˜•íƒœì˜ ì†ëª© ì¢Œí‘œ ë¦¬ìŠ¤íŠ¸
        ì‹ ë¢°ë„ê°€ ë‚®ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
    """
    global vitpose_model, vitpose_processor

    try:
        # ëª¨ë¸ ë¡œë“œ (Lazy)
        pose_model, processor = load_vitpose_model("vitpose")

        # ì „ì²´ ì´ë¯¸ì§€ë¥¼ í•˜ë‚˜ì˜ person bboxë¡œ ì²˜ë¦¬
        boxes = [[[0, 0, image.width, image.height]]]
        inputs = processor(images=image, boxes=boxes, return_tensors="pt")
        inputs = {k: v.to(device) for k, v in inputs.items()}
        if 'dataset_index' not in inputs:
            inputs['dataset_index'] = torch.zeros(inputs['pixel_values'].shape[0], dtype=torch.long, device=device)

        # ì¶”ë¡ 
        with torch.no_grad():
            outputs = pose_model(**inputs)

        # ê²°ê³¼ ì²˜ë¦¬
        results = processor.post_process_pose_estimation(outputs, boxes=boxes)[0][0]
        keypoints_xy = results['keypoints'].cpu().numpy()
        scores = results['scores'].cpu().numpy()

        # ì†ëª© í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ
        wrist_keypoints = []

        # ì™¼ìª½ ì†ëª©
        if scores[COCO_LEFT_WRIST] >= min_score:
            kp = keypoints_xy[COCO_LEFT_WRIST]
            wrist_keypoints.append((float(kp[0]), float(kp[1])))

        # ì˜¤ë¥¸ìª½ ì†ëª©
        if scores[COCO_RIGHT_WRIST] >= min_score:
            kp = keypoints_xy[COCO_RIGHT_WRIST]
            wrist_keypoints.append((float(kp[0]), float(kp[1])))

        return wrist_keypoints

    except Exception as e:
        print(f"âš ï¸ ì†ëª© í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        return []

@app.post("/detect-pose")
async def detect_pose(
    file: UploadFile = File(...),
    model: str = Query(default="vitpose", pattern="^(vitpose|vitpose-huge)$", description="ëª¨ë¸ ì„ íƒ"),
    boxes: str = Query(default="", description="DINO bboxes JSON: [[x1,y1,x2,y2], ...] (xyxy format)")
):
    """ViTPoseë¥¼ ì‚¬ìš©í•œ í¬ì¦ˆ ê°ì§€ (ë©€í‹° person ì§€ì›)"""
    print("-" * 40)
    print(f"ğŸ¦´ í¬ì¦ˆ ê°ì§€ ìš”ì²­: {file.filename} (ëª¨ë¸: {model})")
    start_time = time.time()

    # íŒŒì¼ ê²€ì¦
    if not is_allowed_image(file):
        raise HTTPException(status_code=400, detail="ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤.")

    image_data = await file.read()
    if len(image_data) > MAX_FILE_SIZE:
        raise HTTPException(status_code=400, detail="íŒŒì¼ì´ ë„ˆë¬´ í½ë‹ˆë‹¤.")

    try:
        image = Image.open(io.BytesIO(image_data))
        image = ImageOps.exif_transpose(image)
        image = image.convert("RGB")
    except Exception:
        raise HTTPException(status_code=400, detail="ì˜¬ë°”ë¥¸ ì´ë¯¸ì§€ í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤.")

    # boxes íŒŒë¼ë¯¸í„° íŒŒì‹±
    use_multi_person = False
    person_boxes = []
    if boxes:
        try:
            parsed_boxes = json.loads(boxes)
            if isinstance(parsed_boxes, list) and len(parsed_boxes) > 0:
                for b in parsed_boxes:
                    if len(b) == 4:
                        person_boxes.append([float(b[0]), float(b[1]), float(b[2]), float(b[3])])
                if person_boxes:
                    use_multi_person = True
                    print(f"   ğŸ“¦ {len(person_boxes)}ê°œ person bbox ìˆ˜ì‹ ")
        except (json.JSONDecodeError, TypeError) as e:
            print(f"   âš ï¸ boxes íŒŒì‹± ì‹¤íŒ¨: {e}, ì „ì²´ ì´ë¯¸ì§€ ëª¨ë“œë¡œ fallback")

    try:
        # ëª¨ë¸ ë¡œë“œ (Lazy)
        pose_model, processor = load_vitpose_model(model)

        if use_multi_person:
            # ===== ë©€í‹° person ëª¨ë“œ (DINO boxes â†’ per-person keypoints) =====
            # boxes: [batch, num_persons, 4] format for processor
            boxes_for_processor = [person_boxes]  # batch of 1
            inputs = processor(images=image, boxes=boxes_for_processor, return_tensors="pt")
            inputs = {k: v.to(device) for k, v in inputs.items()}
            # vitpose-plus ëª¨ë¸ì€ dataset_index í•„ìš” (COCO = 0)
            if 'dataset_index' not in inputs:
                inputs['dataset_index'] = torch.zeros(inputs['pixel_values'].shape[0], dtype=torch.long, device=device)

            with torch.no_grad():
                outputs = pose_model(**inputs)

            # post_process returns list[list[dict]] â€” [batch][person]
            all_results = processor.post_process_pose_estimation(outputs, boxes=boxes_for_processor)[0]

            persons = []
            for idx, res in enumerate(all_results):
                kps = res['keypoints'].cpu().numpy()
                scs = res['scores'].cpu().numpy()
                persons.append({
                    "keypoints": [[float(kps[i][0]), float(kps[i][1])] for i in range(len(kps))],
                    "scores": [float(scs[i]) for i in range(len(scs))],
                    "bbox": person_boxes[idx],
                })
                valid_count = int((scs > 0.3).sum())
                print(f"   Person {idx}: {valid_count}/17 valid keypoints (bbox: [{person_boxes[idx][0]:.0f},{person_boxes[idx][1]:.0f},{person_boxes[idx][2]:.0f},{person_boxes[idx][3]:.0f}])")

            print(f"âš¡ ì™„ë£Œ! {len(persons)}ëª… í¬ì¦ˆ ê°ì§€, ì†Œìš”ì‹œê°„: {time.time() - start_time:.2f}ì´ˆ")
            print("-" * 40)

            clear_gpu_memory()
            return JSONResponse(content={
                "success": True,
                "model": model,
                "persons": persons,
                "image_width": image.width,
                "image_height": image.height,
            })

        else:
            # ===== ë‹¨ì¼ person ëª¨ë“œ (ê¸°ì¡´ í˜¸í™˜) =====
            boxes_single = [[[0, 0, image.width, image.height]]]
            inputs = processor(images=image, boxes=boxes_single, return_tensors="pt")
            inputs = {k: v.to(device) for k, v in inputs.items()}
            if 'dataset_index' not in inputs:
                inputs['dataset_index'] = torch.zeros(inputs['pixel_values'].shape[0], dtype=torch.long, device=device)

            with torch.no_grad():
                outputs = pose_model(**inputs)

            results = processor.post_process_pose_estimation(outputs, boxes=boxes_single)[0][0]
            keypoints_xy = results['keypoints'].cpu().numpy()
            scores = results['scores'].cpu().numpy()

            print(f"ğŸ¦´ ê°ì§€ëœ í‚¤í¬ì¸íŠ¸: {len(keypoints_xy)}ê°œ")

            # BlazePose í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (33ê°œ í‚¤í¬ì¸íŠ¸, ì—†ëŠ” ê±´ 0ìœ¼ë¡œ)
            blazepose_keypoints = []
            for i in range(33):
                coco_idx = None
                for coco_i, blaze_i in COCO_TO_BLAZEPOSE.items():
                    if blaze_i == i:
                        coco_idx = coco_i
                        break

                if coco_idx is not None and coco_idx < len(keypoints_xy):
                    kp = keypoints_xy[coco_idx]
                    score = float(scores[coco_idx])
                    blazepose_keypoints.append({
                        "x": float(kp[0]),
                        "y": float(kp[1]),
                        "score": score,
                        "name": f"keypoint_{i}"
                    })
                else:
                    blazepose_keypoints.append({
                        "x": 0, "y": 0, "score": 0, "name": f"keypoint_{i}"
                    })

            ankle_left = blazepose_keypoints[27]
            ankle_right = blazepose_keypoints[28]
            print(f"ğŸ¦¶ ë°œëª© í‚¤í¬ì¸íŠ¸ - ì™¼ìª½(27): score={ankle_left['score']:.3f}, ì˜¤ë¥¸ìª½(28): score={ankle_right['score']:.3f}")
            print(f"âš¡ ì™„ë£Œ! ì†Œìš”ì‹œê°„: {time.time() - start_time:.2f}ì´ˆ")
            print("-" * 40)

            clear_gpu_memory()
            return JSONResponse(content={
                "success": True,
                "model": model,
                "keypoints": blazepose_keypoints,
                "image_width": image.width,
                "image_height": image.height
            })

    except HTTPException:
        raise
    except Exception as e:
        clear_gpu_memory()
        print(f"âŒ í¬ì¦ˆ ê°ì§€ ì˜¤ë¥˜: {str(e)}")
        traceback.print_exc()
        raise HTTPException(
            status_code=500,
            detail=f"í¬ì¦ˆ ê°ì§€ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )

# ========== HEIC ë³€í™˜ API ==========

@app.post("/convert-heic")
async def convert_heic(file: UploadFile = File(...)):
    """HEIC/HEIF â†’ JPEG ë³€í™˜"""
    if not is_allowed_image(file):
        raise HTTPException(status_code=400, detail="ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤.")

    image_data = await file.read()
    try:
        image = Image.open(io.BytesIO(image_data))
        image = ImageOps.exif_transpose(image)
        image = image.convert("RGB")
    except Exception:
        raise HTTPException(status_code=400, detail="ì˜¬ë°”ë¥¸ ì´ë¯¸ì§€ í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤.")

    buf = io.BytesIO()
    image.save(buf, format="JPEG", quality=95)
    buf.seek(0)

    from starlette.responses import StreamingResponse
    return StreamingResponse(buf, media_type="image/jpeg")

# ========== Smart Crop API ==========

@app.post("/smart-crop")
async def smart_crop(
    file: UploadFile = File(...),
    padding_ratio: float = Query(default=0.25, ge=0.0, le=1.0, description="í¬ë¡­ íŒ¨ë”© ë¹„ìœ¨"),
    min_score: float = Query(default=0.3, ge=0.0, le=1.0, description="í‚¤í¬ì¸íŠ¸ ìµœì†Œ ì‹ ë¢°ë„"),
    seg_size: int = Query(default=512, ge=128, le=1024, description="ì„¸ê·¸ë©˜í…Œì´ì…˜ ë§ˆìŠ¤í¬ í•´ìƒë„"),
    crop_mode: str = Query(default="person", description="í¬ë¡­ ëª¨ë“œ: person(ì¸ë¬¼) ë˜ëŠ” object(ë¬¼ê±´)"),
):
    """ViTPose í‚¤í¬ì¸íŠ¸ + ì„¸ê·¸ë©˜í…Œì´ì…˜ ë§ˆìŠ¤í¬ ê¸°ë°˜ ìŠ¤ë§ˆíŠ¸ í¬ë¡­"""
    print("-" * 40)
    mode_label = "ì¸ë¬¼" if crop_mode == "person" else "ë¬¼ê±´"
    print(f"âœ‚ï¸ ìŠ¤ë§ˆíŠ¸ í¬ë¡­ ìš”ì²­: {file.filename} (ëª¨ë“œ: {mode_label}, seg_size: {seg_size})")
    start_time = time.time()

    # íŒŒì¼ ê²€ì¦
    if not is_allowed_image(file):
        raise HTTPException(status_code=400, detail="ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤.")

    image_data = await file.read()
    if len(image_data) > MAX_FILE_SIZE:
        raise HTTPException(status_code=400, detail="íŒŒì¼ì´ ë„ˆë¬´ í½ë‹ˆë‹¤.")

    try:
        image = Image.open(io.BytesIO(image_data))
        image.verify()
        image = Image.open(io.BytesIO(image_data))
        image = ImageOps.exif_transpose(image)
        image = image.convert("RGB")
    except Exception:
        raise HTTPException(status_code=400, detail="ì˜¬ë°”ë¥¸ ì´ë¯¸ì§€ í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤.")

    # === ë¬¼ê±´ ëª¨ë“œ: ë§ˆìŠ¤í¬ë§Œìœ¼ë¡œ í¬ë¡­ ===
    if crop_mode == "object":
        try:
            seg_start = time.time()
            seg_scale = min(seg_size / image.width, seg_size / image.height)
            seg_w = max(32, (int(image.width * seg_scale) // 32) * 32)
            seg_h = max(32, (int(image.height * seg_scale) // 32) * 32)

            seg_resized = image.resize((seg_w, seg_h), Image.Resampling.LANCZOS)
            seg_model = get_birefnet_model("portrait")
            seg_dev = next(seg_model.parameters()).device
            seg_tensor = transform_normalize(seg_resized).unsqueeze(0).to(seg_dev)
            if seg_dev.type != "cpu":
                seg_tensor = seg_tensor.half()

            with torch.no_grad():
                seg_pred = seg_model(seg_tensor)[-1].sigmoid().cpu()

            seg_mask = seg_pred[0].squeeze().float().numpy()
            mask_binary = seg_mask > 0.5
            rows = np.any(mask_binary, axis=1)
            cols = np.any(mask_binary, axis=0)

            if not rows.any() or not cols.any():
                print(f"âš ï¸ ë§ˆìŠ¤í¬ì—ì„œ ëŒ€ìƒ ë¯¸ê°ì§€")
                clear_gpu_memory()
                return JSONResponse(content={"cropped": False, "reason": "ëŒ€ìƒ ë¯¸ê°ì§€"})

            r_min, r_max = np.where(rows)[0][[0, -1]]
            c_min, c_max = np.where(cols)[0][[0, -1]]
            scale_x = image.width / seg_w
            scale_y = image.height / seg_h
            mask_x_min = c_min * scale_x
            mask_y_min = r_min * scale_y
            mask_x_max = (c_max + 1) * scale_x
            mask_y_max = (r_max + 1) * scale_y

            # ìƒí•˜ì¢Œìš° 10% íŒ¨ë”©
            mask_w = mask_x_max - mask_x_min
            mask_h = mask_y_max - mask_y_min
            mask_x_min = max(0, mask_x_min - mask_w * 0.1)
            mask_y_min = max(0, mask_y_min - mask_h * 0.1)
            mask_x_max = min(image.width, mask_x_max + mask_w * 0.1)
            mask_y_max = min(image.height, mask_y_max + mask_h * 0.1)

            mask_bbox = {"x_min": float(mask_x_min), "y_min": float(mask_y_min), "x_max": float(mask_x_max), "y_max": float(mask_y_max)}

            crop_x = max(0, int(mask_x_min))
            crop_y = max(0, int(mask_y_min))
            crop_x2 = min(image.width, int(mask_x_max))
            crop_y2 = min(image.height, int(mask_y_max))
            crop_w = crop_x2 - crop_x
            crop_h = crop_y2 - crop_y

            # í¬ë¡­ ì˜ì—­ì´ ì›ë³¸ì˜ 90% ì´ìƒì´ë©´ ìŠ¤í‚µ
            crop_area = crop_w * crop_h
            image_area = image.width * image.height
            is_cropped = crop_area < image_area * 0.9

            print(f"   ğŸ­ ë§ˆìŠ¤í¬ bbox: ({mask_x_min:.0f}, {mask_y_min:.0f})â†’({mask_x_max:.0f}, {mask_y_max:.0f}) [{time.time() - seg_start:.2f}ì´ˆ]")
            if not is_cropped:
                print(f"âš ï¸ í¬ë¡­ ì˜ì—­ì´ ì›ë³¸ì˜ {crop_area / image_area * 100:.0f}%ë¡œ í¬ë¡­ ë¶ˆí•„ìš”")
            else:
                print(f"âœ‚ï¸ í¬ë¡­ ì¢Œí‘œ: ({crop_x}, {crop_y}) {crop_w}x{crop_h}")
            print(f"âš¡ ì™„ë£Œ! ì†Œìš”ì‹œê°„: {time.time() - start_time:.2f}ì´ˆ")
            print("-" * 40)

            clear_gpu_memory()
            return JSONResponse(content={
                "cropped": is_cropped,
                "reason": None if is_cropped else "í¬ë¡­ ë¶ˆí•„ìš” (90% ì´ìƒ)",
                "crop": {"x": crop_x, "y": crop_y, "width": crop_w, "height": crop_h},
                "image_width": image.width,
                "image_height": image.height,
                "mask_bbox": mask_bbox,
            })
        except Exception as e:
            clear_gpu_memory()
            print(f"âŒ ë¬¼ê±´ í¬ë¡­ ì˜¤ë¥˜: {str(e)}")
            traceback.print_exc()
            raise HTTPException(status_code=500, detail=f"ë¬¼ê±´ í¬ë¡­ ì¤‘ ì˜¤ë¥˜: {str(e)}")

    try:
        # ViTPose ëª¨ë¸ ë¡œë“œ ë° ì¶”ë¡  (ì¸ë¬¼ ëª¨ë“œ)
        pose_model, processor = load_vitpose_model("vitpose")

        boxes = [[[0, 0, image.width, image.height]]]
        inputs = processor(images=image, boxes=boxes, return_tensors="pt")
        inputs = {k: v.to(device) for k, v in inputs.items()}
        if 'dataset_index' not in inputs:
            inputs['dataset_index'] = torch.zeros(inputs['pixel_values'].shape[0], dtype=torch.long, device=device)

        with torch.no_grad():
            outputs = pose_model(**inputs)

        results = processor.post_process_pose_estimation(outputs, boxes=boxes)[0][0]
        keypoints_xy = results['keypoints'].cpu().numpy()
        scores = results['scores'].cpu().numpy()

        # score > min_scoreì¸ í‚¤í¬ì¸íŠ¸ë§Œ ì‚¬ìš©
        valid_mask = scores > min_score
        valid_count = int(valid_mask.sum())

        if valid_count < 3:
            print(f"âš ï¸ ìœ íš¨ í‚¤í¬ì¸íŠ¸ ë¶€ì¡±: {valid_count}ê°œ (ìµœì†Œ 3ê°œ í•„ìš”)")
            clear_gpu_memory()
            return JSONResponse(content={"cropped": False, "reason": "ìœ íš¨ í‚¤í¬ì¸íŠ¸ ë¶€ì¡±"})

        valid_kps = keypoints_xy[valid_mask]

        # === ì†ê°€ë½ ë ì¶”ì •: ì–´ê¹¨â†’íŒ”ê¿ˆì¹˜ 100% AND íŒ”ê¿ˆì¹˜â†’ì†ëª© 50% ë‘˜ ë‹¤ ì¶”ê°€ ===
        # COCO: 5=L_shoulder, 7=L_elbow, 9=L_wrist, 6=R_shoulder, 8=R_elbow, 10=R_wrist
        HAND_SETS = [
            (5, 7, 9),   # ì™¼ìª½: shoulder, elbow, wrist
            (6, 8, 10),  # ì˜¤ë¥¸ìª½: shoulder, elbow, wrist
        ]

        extra_points = []
        for sh_idx, el_idx, wr_idx in HAND_SETS:
            # ì–´ê¹¨â†’íŒ”ê¿ˆì¹˜ ë°©í–¥ìœ¼ë¡œ íŒ”ê¿ˆì¹˜ì—ì„œ +100% ì—°ì¥
            if scores[sh_idx] > min_score and scores[el_idx] > min_score:
                sx, sy = keypoints_xy[sh_idx]
                ex, ey = keypoints_xy[el_idx]
                dx, dy = ex - sx, ey - sy
                cx = max(0, min(image.width, ex + dx * 1.5))
                cy = max(0, min(image.height, ey + dy * 1.5))
                extra_points.append((cx, cy))
                print(f"   ğŸ–ï¸ finger ì¶”ì •: ({cx:.0f}, {cy:.0f}) [shoulderâ†’elbow+150%]")
            # íŒ”ê¿ˆì¹˜â†’ì†ëª© ë°©í–¥ìœ¼ë¡œ ì†ëª©ì—ì„œ +50% ì—°ì¥
            if scores[el_idx] > min_score and scores[wr_idx] > min_score:
                ex, ey = keypoints_xy[el_idx]
                wx, wy = keypoints_xy[wr_idx]
                dx, dy = wx - ex, wy - ey
                cx = max(0, min(image.width, wx + dx * 1.0))
                cy = max(0, min(image.height, wy + dy * 1.0))
                extra_points.append((cx, cy))
                print(f"   ğŸ–ï¸ finger ì¶”ì •: ({cx:.0f}, {cy:.0f}) [elbowâ†’wrist+100%]")

        # === ë°œë ì¶”ì •: ë” ì•„ë˜ìª½ ë°œëª© ê¸°ì¤€, ì—‰ë©ì´â†’ë¬´ë¦ vs ë¬´ë¦â†’ë°œëª© 70% ì¤‘ ë” ë¨¼ ìª½ ===
        # COCO: 11=L_hip, 13=L_knee, 15=L_ankle, 12=R_hip, 14=R_knee, 16=R_ankle
        FOOT_SETS = [
            (11, 13, 15),  # ì™¼ìª½: hip, knee, ankle
            (12, 14, 16),  # ì˜¤ë¥¸ìª½: hip, knee, ankle
        ]

        # ë” ì•„ë˜(yê°€ í°) ë°œëª© ìª½ ì„ íƒ
        lower_foot = None
        lower_ankle_y = -1
        for hp_idx, kn_idx, ak_idx in FOOT_SETS:
            if scores[ak_idx] > min_score:
                if keypoints_xy[ak_idx][1] > lower_ankle_y:
                    lower_ankle_y = keypoints_xy[ak_idx][1]
                    lower_foot = (hp_idx, kn_idx, ak_idx)

        if lower_foot:
            hp_idx, kn_idx, ak_idx = lower_foot
            hx, hy = keypoints_xy[hp_idx]
            kx, ky = keypoints_xy[kn_idx]
            ax, ay = keypoints_xy[ak_idx]

            # ì—‰ë©ì´â†’ë¬´ë¦ 100% ì—°ì¥
            if scores[hp_idx] > min_score and scores[kn_idx] > min_score:
                dx, dy = kx - hx, ky - hy
                cx = max(0, min(image.width, kx + dx * 1.0))
                cy = max(0, min(image.height, ky + dy * 1.0))
                extra_points.append((cx, cy))
                print(f"   ğŸ¦¶ toe ì¶”ì •: ({cx:.0f}, {cy:.0f}) [hipâ†’knee 100%]")
            # ë¬´ë¦â†’ë°œëª© +150% ì—°ì¥
            if scores[kn_idx] > min_score and scores[ak_idx] > min_score:
                dx, dy = ax - kx, ay - ky
                cx = max(0, min(image.width, ax + dx * 1.5))
                cy = max(0, min(image.height, ay + dy * 1.5))
                extra_points.append((cx, cy))
                print(f"   ğŸ¦¶ toe ì¶”ì •: ({cx:.0f}, {cy:.0f}) [kneeâ†’ankle+150%]")

        # === ê·€ ì¶”ì •: ì½”â†’ê·€ ë°©í–¥ìœ¼ë¡œ ê·€ì—ì„œ +100% ì—°ì¥ ===
        # COCO: 0=nose, 3=L_ear, 4=R_ear
        for ear_idx in [3, 4]:
            if scores[0] > min_score and scores[ear_idx] > min_score:
                nx, ny = keypoints_xy[0]
                ex, ey = keypoints_xy[ear_idx]
                dx, dy = ex - nx, ey - ny
                cx = max(0, min(image.width, ex + dx * 1.0))
                cy = max(0, min(image.height, ey + dy * 1.0))
                extra_points.append((cx, cy))
                side = "L" if ear_idx == 3 else "R"
                print(f"   ğŸ‘‚ ear ì¶”ì •: ({cx:.0f}, {cy:.0f}) [noseâ†’{side}_ear+100%]")

        # === ë¨¸ë¦¬ ê¼­ëŒ€ê¸° ì¶”ì • (ì–´ê¹¨ ì¤‘ì â†’ëˆˆ ì¤‘ì  ë²¡í„° 170% ì—°ì¥) ===
        # COCO: 1=L_eye, 2=R_eye, 5=L_shoulder, 6=R_shoulder
        has_eyes = scores[1] > min_score and scores[2] > min_score
        has_shoulders = scores[5] > min_score and scores[6] > min_score

        if has_eyes and has_shoulders:
            mid_eye_x = (keypoints_xy[1][0] + keypoints_xy[2][0]) / 2
            mid_eye_y = (keypoints_xy[1][1] + keypoints_xy[2][1]) / 2
            mid_sh_x = (keypoints_xy[5][0] + keypoints_xy[6][0]) / 2
            mid_sh_y = (keypoints_xy[5][1] + keypoints_xy[6][1]) / 2
            dx = mid_eye_x - mid_sh_x
            dy = mid_eye_y - mid_sh_y
            crown_x = mid_eye_x + dx * 1.7
            crown_y = mid_eye_y + dy * 1.7
            crown_x = max(0, min(image.width, crown_x))
            crown_y = max(0, min(image.height, crown_y))
            extra_points.append((crown_x, crown_y))
            print(f"   ğŸ‘¤ ë¨¸ë¦¬ ê¼­ëŒ€ê¸° ì¶”ì •: ({crown_x:.0f}, {crown_y:.0f}) [ì–´ê¹¨â†’ëˆˆ 170%]")

        # ë°”ìš´ë”© ë°•ìŠ¤ ê³„ì‚° (ìœ íš¨ í‚¤í¬ì¸íŠ¸ + ì¶”ì • í¬ì¸íŠ¸ í•©ì‚°)
        all_x = [float(kp[0]) for kp in valid_kps] + [p[0] for p in extra_points]
        all_y = [float(kp[1]) for kp in valid_kps] + [p[1] for p in extra_points]

        kp_x_min = min(all_x)
        kp_y_min = min(all_y)
        kp_x_max = max(all_x)
        kp_y_max = max(all_y)

        # === ì €í•´ìƒë„ ì„¸ê·¸ë©˜í…Œì´ì…˜ ë§ˆìŠ¤í¬ë¡œ ì‹¤ë£¨ì—£ bbox ë³´ì™„ ===
        try:
            seg_start = time.time()
            seg_scale = min(seg_size / image.width, seg_size / image.height)
            seg_w = (int(image.width * seg_scale) // 32) * 32
            seg_h = (int(image.height * seg_scale) // 32) * 32
            seg_w = max(32, seg_w)
            seg_h = max(32, seg_h)

            seg_resized = image.resize((seg_w, seg_h), Image.Resampling.LANCZOS)
            seg_model = get_birefnet_model("portrait")
            seg_dev = next(seg_model.parameters()).device
            seg_tensor = transform_normalize(seg_resized).unsqueeze(0).to(seg_dev)
            if seg_dev.type != "cpu":
                seg_tensor = seg_tensor.half()

            with torch.no_grad():
                seg_pred = seg_model(seg_tensor)[-1].sigmoid().cpu()

            seg_mask = seg_pred[0].squeeze().float().numpy()
            # ì„ê³„ê°’ 0.5ë¡œ ì´ì§„í™”
            mask_binary = seg_mask > 0.5
            rows = np.any(mask_binary, axis=1)
            cols = np.any(mask_binary, axis=0)

            if rows.any() and cols.any():
                r_min, r_max = np.where(rows)[0][[0, -1]]
                c_min, c_max = np.where(cols)[0][[0, -1]]
                # ì›ë³¸ í•´ìƒë„ë¡œ ì¢Œí‘œ ë³€í™˜
                scale_x = image.width / seg_w
                scale_y = image.height / seg_h
                mask_x_min = c_min * scale_x
                mask_y_min = r_min * scale_y
                mask_x_max = (c_max + 1) * scale_x
                mask_y_max = (r_max + 1) * scale_y

                # ë§ˆìŠ¤í¬ bbox íŒ¨ë”©: ìœ„ 10%, ì¢Œìš° 5% (ê°€ëŠ” ë¨¸ë¦¬ì¹´ë½/íŒ” ë³´í˜¸)
                mask_h = mask_y_max - mask_y_min
                mask_w = mask_x_max - mask_x_min
                mask_y_min = max(0, mask_y_min - mask_h * 0.1)
                mask_x_min = max(0, mask_x_min - mask_w * 0.05)
                mask_x_max = min(image.width, mask_x_max + mask_w * 0.05)

                # í‚¤í¬ì¸íŠ¸ bboxì™€ ë§ˆìŠ¤í¬ bboxì˜ í•©ì§‘í•©
                x_min = min(kp_x_min, mask_x_min)
                y_min = min(kp_y_min, mask_y_min)
                x_max = max(kp_x_max, mask_x_max)
                y_max = max(kp_y_max, mask_y_max)
                mask_bbox = {"x_min": float(mask_x_min), "y_min": float(mask_y_min), "x_max": float(mask_x_max), "y_max": float(mask_y_max)}
                print(f"   ğŸ­ ë§ˆìŠ¤í¬ bbox: ({mask_x_min:.0f}, {mask_y_min:.0f})â†’({mask_x_max:.0f}, {mask_y_max:.0f}) [{time.time() - seg_start:.2f}ì´ˆ]")
            else:
                x_min, y_min, x_max, y_max = kp_x_min, kp_y_min, kp_x_max, kp_y_max
                mask_bbox = None
                print(f"   âš ï¸ ë§ˆìŠ¤í¬ì—ì„œ ì¸ë¬¼ ë¯¸ê°ì§€, í‚¤í¬ì¸íŠ¸ë§Œ ì‚¬ìš©")
        except Exception as seg_err:
            x_min, y_min, x_max, y_max = kp_x_min, kp_y_min, kp_x_max, kp_y_max
            mask_bbox = None
            print(f"   âš ï¸ ì„¸ê·¸ë©˜í…Œì´ì…˜ ì‹¤íŒ¨: {seg_err}, í‚¤í¬ì¸íŠ¸ë§Œ ì‚¬ìš©")

        kp_bbox = {"x_min": float(kp_x_min), "y_min": float(kp_y_min), "x_max": float(kp_x_max), "y_max": float(kp_y_max)}

        # íŒ¨ë”© ì—†ìŒ â€” ê´€ì ˆ ì¶”ì • + ë§ˆìŠ¤í¬ í•©ì§‘í•©ìœ¼ë¡œ ì»¤ë²„
        crop_x = max(0, int(x_min))
        crop_y = max(0, int(y_min))
        crop_x2 = min(image.width, int(x_max))
        crop_y2 = min(image.height, int(y_max))

        crop_w = crop_x2 - crop_x
        crop_h = crop_y2 - crop_y

        # í‚¤í¬ì¸íŠ¸ ì •ë³´ êµ¬ì„± (COCO 17)
        COCO_NAMES = [
            "nose", "left_eye", "right_eye", "left_ear", "right_ear",
            "left_shoulder", "right_shoulder", "left_elbow", "right_elbow",
            "left_wrist", "right_wrist", "left_hip", "right_hip",
            "left_knee", "right_knee", "left_ankle", "right_ankle",
        ]
        keypoints_list = []
        for i in range(len(keypoints_xy)):
            keypoints_list.append({
                "name": COCO_NAMES[i] if i < len(COCO_NAMES) else f"kp_{i}",
                "x": float(keypoints_xy[i][0]),
                "y": float(keypoints_xy[i][1]),
                "score": float(scores[i]),
            })

        # í¬ë¡­ ì˜ì—­ì´ ì›ë³¸ì˜ 90% ì´ìƒì´ë©´ í¬ë¡­ ë¶ˆí•„ìš” (ì •ë³´ëŠ” ë°˜í™˜)
        crop_area = crop_w * crop_h
        image_area = image.width * image.height
        if crop_area >= image_area * 0.9:
            print(f"âš ï¸ í¬ë¡­ ì˜ì—­ì´ ì›ë³¸ì˜ {crop_area / image_area * 100:.0f}%ë¡œ í¬ë¡­ ë¶ˆí•„ìš”")
            print(f"âš¡ ì™„ë£Œ! ì†Œìš”ì‹œê°„: {time.time() - start_time:.2f}ì´ˆ")
            print("-" * 40)
            clear_gpu_memory()
            response = {
                "cropped": False,
                "reason": "í¬ë¡­ ë¶ˆí•„ìš” (90% ì´ìƒ)",
                "crop": {"x": crop_x, "y": crop_y, "width": crop_w, "height": crop_h},
                "image_width": image.width,
                "image_height": image.height,
                "valid_keypoints": valid_count,
                "keypoints": keypoints_list,
                "kp_bbox": kp_bbox,
            }
            if mask_bbox:
                response["mask_bbox"] = mask_bbox
            return JSONResponse(content=response)

        print(f"âœ‚ï¸ í¬ë¡­ ì¢Œí‘œ: ({crop_x}, {crop_y}) {crop_w}x{crop_h} (ìœ íš¨ í‚¤í¬ì¸íŠ¸: {valid_count}ê°œ)")
        print(f"âš¡ ì™„ë£Œ! ì†Œìš”ì‹œê°„: {time.time() - start_time:.2f}ì´ˆ")
        print("-" * 40)

        clear_gpu_memory()
        response = {
            "cropped": True,
            "crop": {
                "x": crop_x,
                "y": crop_y,
                "width": crop_w,
                "height": crop_h,
            },
            "image_width": image.width,
            "image_height": image.height,
            "valid_keypoints": valid_count,
            "keypoints": keypoints_list,
            "kp_bbox": kp_bbox,
        }
        if mask_bbox:
            response["mask_bbox"] = mask_bbox
        return JSONResponse(content=response)

    except HTTPException:
        raise
    except Exception as e:
        clear_gpu_memory()
        print(f"âŒ ìŠ¤ë§ˆíŠ¸ í¬ë¡­ ì˜¤ë¥˜: {str(e)}")
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"ìŠ¤ë§ˆíŠ¸ í¬ë¡­ ì¤‘ ì˜¤ë¥˜: {str(e)}")

# ========== Ryan Book Automation API ==========

class ChildData(BaseModel):
    firstName: str
    lastName: str = ""
    fullName: str = ""
    gender: str = "boy"
    birthday: Optional[str] = None
    photo: Optional[str] = None
    photoNoBg: Optional[str] = None

class FavoriteObject(BaseModel):
    name: str
    photo: Optional[str] = None
    photoNoBg: Optional[str] = None
    emoji: str = "â“"
    josaMode: str = "friend"

class FamilyMember(BaseModel):
    id: str
    relation: str
    emoji: str
    photo: Optional[str] = None
    customName: Optional[str] = None

class BookRequest(BaseModel):
    child: ChildData
    objects: List[FavoriteObject] = []
    familyMembers: List[FamilyMember] = []

@app.get("/josa-preview")
async def josa_preview(name: str = Query(..., min_length=1, description="ì¡°ì‚¬ë¥¼ ì ìš©í•  ì´ë¦„")):
    """
    í•œê¸€ ì¡°ì‚¬ ë¯¸ë¦¬ë³´ê¸° API

    ì´ë¦„ì„ ì…ë ¥í•˜ë©´ 9ê°€ì§€ ì¡°ì‚¬ í˜•íƒœì˜ ì˜ˆì‹œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì‹¤ì‹œê°„ ì¡°ì‚¬ ë¯¸ë¦¬ë³´ê¸°ì— ì‚¬ìš©ë©ë‹ˆë‹¤.
    """
    if not RYAN_ENGINE_AVAILABLE:
        raise HTTPException(status_code=500, detail="Ryan Engineì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    josa = JosaUtils()
    demo = josa.generate_josa_demo(name)

    return JSONResponse(content={
        "success": True,
        "name": demo['name'],
        "hasBatchim": demo['has_batchim'],
        "examples": demo['examples']
    })

@app.post("/generate-book")
async def generate_book(request: BookRequest):
    """
    Ryan Book ìë™ ìƒì„± API

    ì‚¬ìš©ì ë°ì´í„°ë¥¼ ë°›ì•„ ì™„ì „í•œ ì±… ìŠ¤í™(final_book_spec.json)ì„ ìƒì„±í•©ë‹ˆë‹¤.

    ìš”ì²­ ì˜ˆì‹œ:
    {
        "child": {
            "firstName": "ë„í˜„",
            "lastName": "ê¹€",
            "gender": "boy"
        },
        "objects": [
            {"name": "í† ë¼", "emoji": "ğŸ°", "josaMode": "friend"},
            {"name": "í† ë§ˆí† ", "emoji": "ğŸ…", "josaMode": "object"}
        ],
        "familyMembers": [
            {"id": "mom", "relation": "ì—„ë§ˆ", "emoji": "ğŸ‘©"}
        ]
    }
    """
    if not RYAN_ENGINE_AVAILABLE:
        raise HTTPException(status_code=500, detail="Ryan Engineì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    print("-" * 40)
    print(f"ğŸ“š ì±… ìƒì„± ìš”ì²­: {request.child.firstName}")
    start_time = time.time()

    try:
        # í…Œë§ˆ íŒŒì¼ ê²½ë¡œ
        theme_path = Path(__file__).parent / "ryan_engine" / "themes" / "theme_ryan.json"

        if not theme_path.exists():
            raise HTTPException(status_code=500, detail="í…Œë§ˆ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        # BookGenerator ìƒì„±
        generator = BookGenerator(str(theme_path))

        # ìš”ì²­ ë°ì´í„°ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
        user_data = {
            'child': request.child.model_dump(),
            'objects': [obj.model_dump() for obj in request.objects],
            'familyMembers': [fam.model_dump() for fam in request.familyMembers],
        }

        # ì±… ìƒì„±
        book_spec = generator.generate_from_dict(user_data)

        # JSON ë³€í™˜
        book_json = generator.to_json(book_spec)

        # íŒŒì¼ë¡œ ì €ì¥ (ì„ íƒì )
        output_dir = Path(__file__).parent / "output"
        output_dir.mkdir(exist_ok=True)
        safe_name = re.sub(r'[^a-zA-Z0-9ê°€-í£_\-]', '_', request.child.firstName)
        output_path = output_dir / f"book_{safe_name}_{int(time.time())}.json"
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(book_json)

        print(f"âœ… ì±… ìƒì„± ì™„ë£Œ! íŒŒì¼: {output_path}")
        print(f"âš¡ ì†Œìš”ì‹œê°„: {time.time() - start_time:.2f}ì´ˆ")
        print("-" * 40)

        return JSONResponse(content={
            "success": True,
            "bookSpec": json.loads(book_json),
            "savedTo": str(output_path)
        })

    except HTTPException:
        raise
    except Exception as e:
        print(f"âŒ ì±… ìƒì„± ì˜¤ë¥˜: {str(e)}")
        traceback.print_exc()
        raise HTTPException(
            status_code=500,
            detail=f"ì±… ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )

# ========== ì¸ì‡„ ìš”ì²­ API ==========

class PrintRequest(BaseModel):
    firstName: str
    parentNames: str = ""
    version: str = "A"
    bookId: str = ""
    timestamp: str = ""

@app.post("/request-print")
async def request_print(request: PrintRequest):
    """ì¸ì‡„ ìš”ì²­ ì ‘ìˆ˜ (ë¶í† ë¦¬ ì—°ë™ì€ ì¶”í›„)"""
    print("-" * 40)
    print(f"ğŸ–¨ï¸ ì¸ì‡„ ìš”ì²­ ì ‘ìˆ˜: {request.firstName} (ë²„ì „: {request.version}, bookId: {request.bookId})")
    print(f"   ì‹œê°: {request.timestamp}")
    print("-" * 40)

    return JSONResponse(content={
        "success": True,
        "message": "ì¸ì‡„ ìš”ì²­ì´ ì ‘ìˆ˜ë˜ì—ˆìŠµë‹ˆë‹¤.",
        "bookId": request.bookId,
    })

# ========== SAM2 ì•„ì´ ì„¸ê·¸ë©˜í…Œì´ì…˜ API ==========

@app.post("/segment-child")
async def segment_child(
    file: UploadFile = File(...),
    point_x: float = Form(default=0, description="ì•„ì´ ì–¼êµ´ ì¤‘ì‹¬ X ì¢Œí‘œ"),
    point_y: float = Form(default=0, description="ì•„ì´ ì–¼êµ´ ì¤‘ì‹¬ Y ì¢Œí‘œ"),
    neg_points: str = Form(default="", description="ì–´ë¥¸ ì–¼êµ´ ì¤‘ì‹¬ ì¢Œí‘œ JSON: [[x1,y1],[x2,y2],...]"),
    pos_points: str = Form(default="", description="ViTPose ì•„ì´ keypoints JSON: [[x1,y1],[x2,y2],...] (positive prompts)"),
    box: str = Form(default="", description="Box prompt JSON: [x1,y1,x2,y2] (Grounding DINO bbox)"),
    combine: bool = Form(default=False, description="Trueì´ë©´ boxì™€ pointë¥¼ ë™ì‹œì— ì‚¬ìš© (ê°€ë ¤ì§„ ì‹ ì²´ ë³µì›ì— íš¨ê³¼ì )"),
):
    """
    SAM2 ê¸°ë°˜ ì•„ì´ ì„¸ê·¸ë©˜í…Œì´ì…˜

    face-api.jsì—ì„œ ê°ì§€í•œ ì•„ì´ ì–¼êµ´ ì¤‘ì‹¬ ì¢Œí‘œë¥¼ point promptë¡œ ì‚¬ìš©í•˜ì—¬
    SAM2ê°€ ì•„ì´ë§Œ ì„¸ê·¸ë¨¼íŠ¸í•©ë‹ˆë‹¤. ì–´ë¥¸ ì–¼êµ´ ì¢Œí‘œëŠ” negative promptë¡œ ì‚¬ìš©.
    pos_pointsê°€ ì œê³µë˜ë©´ ViTPose keypointsë¥¼ multi-point positive promptë¡œ ì‚¬ìš©.

    Returns: ì•„ì´ë§Œ ì¶”ì¶œëœ íˆ¬ëª… ë°°ê²½ WebP ì´ë¯¸ì§€
    """
    print("-" * 40)
    print(f"ğŸ‘¶ SAM2 ì•„ì´ ì„¸ê·¸ë©˜í…Œì´ì…˜ ìš”ì²­: {file.filename}")
    print(f"   ì•„ì´ ì¢Œí‘œ: ({point_x:.0f}, {point_y:.0f})")
    start_time = time.time()

    if not SAM2_AVAILABLE:
        raise HTTPException(status_code=500, detail="SAM2 ëª¨ë“ˆì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    # íŒŒì¼ ê²€ì¦
    if not is_allowed_image(file):
        raise HTTPException(status_code=400, detail="ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤.")

    image_data = await file.read()
    if len(image_data) > MAX_FILE_SIZE:
        raise HTTPException(status_code=400, detail="íŒŒì¼ì´ ë„ˆë¬´ í½ë‹ˆë‹¤.")

    try:
        image = Image.open(io.BytesIO(image_data))
        image = ImageOps.exif_transpose(image)
        image = image.convert("RGB")
    except Exception:
        raise HTTPException(status_code=400, detail="ì˜¬ë°”ë¥¸ ì´ë¯¸ì§€ í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤.")

    try:
        predictor = get_sam2_predictor()

        # Box prompt + Point prompt êµ¬ì„±
        box_coords = None
        if box:
            try:
                box_list = json.loads(box)
                if len(box_list) == 4:
                    box_coords = np.array([box_list], dtype=np.float32)
                    print(f"   ğŸ“¦ Box prompt: [{box_list[0]:.0f}, {box_list[1]:.0f}, {box_list[2]:.0f}, {box_list[3]:.0f}]")
            except (json.JSONDecodeError, TypeError):
                print(f"   âš ï¸ box íŒŒì‹± ì‹¤íŒ¨")

        # Point prompts êµ¬ì„± (combine=Trueì´ë©´ boxì™€ í•¨ê»˜ ì‚¬ìš©)
        points = []
        labels = []

        if combine or not box_coords:
            # pos_points: ViTPose multi-point positive prompts
            if pos_points:
                try:
                    pos_list = json.loads(pos_points)
                    for pp in pos_list:
                        if len(pp) == 2:
                            points.append([float(pp[0]), float(pp[1])])
                            labels.append(1)  # foreground
                    print(f"   ViTPose positive points: {len(pos_list)}ê°œ")
                except (json.JSONDecodeError, TypeError):
                    print(f"   âš ï¸ pos_points íŒŒì‹± ì‹¤íŒ¨, point_x/y fallback")

            # pos_pointsê°€ ì—†ê±°ë‚˜ íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê¸°ì¡´ point_x/point_y ì‚¬ìš©
            if not points:
                points.append([point_x, point_y])
                labels.append(1)  # foreground (ì•„ì´)

            # negative points íŒŒì‹± (ì–´ë¥¸ ì–¼êµ´/keypoints ì¢Œí‘œ)
            if neg_points:
                try:
                    neg_list = json.loads(neg_points)
                    for np_coord in neg_list:
                        if len(np_coord) == 2:
                            points.append([float(np_coord[0]), float(np_coord[1])])
                            labels.append(0)  # background (ì–´ë¥¸)
                    print(f"   Negative points: {len(neg_list)}ê°œ")
                except (json.JSONDecodeError, TypeError):
                    print(f"   âš ï¸ neg_points íŒŒì‹± ì‹¤íŒ¨, ë¬´ì‹œ")

            print(f"   ì´ points: {len(points)}ê°œ (pos={sum(1 for l in labels if l==1)}, neg={sum(1 for l in labels if l==0)})")

        point_coords_arr = np.array(points, dtype=np.float32) if points else None
        point_labels_arr = np.array(labels, dtype=np.int32) if labels else None

        if combine and box_coords is not None and point_coords_arr is not None:
            print(f"   ğŸ”— Combine ëª¨ë“œ: box + {len(points)}ê°œ point ë™ì‹œ ì‚¬ìš©")

        # SAM2 ì¶”ë¡  (GPU ì‘ì—…ì´ë¯€ë¡œ to_thread ì‚¬ìš©)
        def _run_sam2():
            img_np = np.array(image)
            with sam2_lock, torch.inference_mode():
                predictor.set_image(img_np)
                masks, scores, logits = predictor.predict(
                    point_coords=point_coords_arr,
                    point_labels=point_labels_arr,
                    box=box_coords,
                    multimask_output=True,
                )
            # ê°€ì¥ ë†’ì€ ì ìˆ˜ì˜ ë§ˆìŠ¤í¬ ì„ íƒ
            best_idx = np.argmax(scores)
            best_mask = masks[best_idx]
            best_score = float(scores[best_idx])
            parts = []
            if box_coords is not None: parts.append("box")
            if point_coords_arr is not None: parts.append(f"pointÃ—{len(points)}")
            prompt_type = "+".join(parts) if parts else "none"
            print(f"   SAM2 ë§ˆìŠ¤í¬ {len(masks)}ê°œ ìƒì„± ({prompt_type}), ìµœê³  ì ìˆ˜: {best_score:.3f} (idx={best_idx})")
            return best_mask, best_score

        mask_np, mask_score = await asyncio.to_thread(_run_sam2)

        # ë§ˆìŠ¤í¬ë¥¼ PIL Imageë¡œ ë³€í™˜
        mask_uint8 = (mask_np * 255).astype(np.uint8)
        mask_pil = Image.fromarray(mask_uint8)

        # ì›ë³¸ ì´ë¯¸ì§€ì— ë§ˆìŠ¤í¬ ì ìš©
        result = image.copy()
        result.putalpha(mask_pil)

        # ì•ŒíŒŒ ì±„ë„ ê¸°ì¤€ í¬ë¡­
        alpha = result.split()[-1]
        alpha_clean = alpha.point(lambda x: 0 if x < 30 else x)
        bbox = alpha_clean.getbbox()

        original_w, original_h = image.size
        crop_x, crop_y = 0, 0

        if bbox:
            padding = 20
            x1, y1, x2, y2 = bbox
            crop_x = max(0, x1 - padding)
            crop_y = max(0, y1 - padding)
            x2 = min(result.width, x2 + padding)
            y2 = min(result.height, y2 + padding)
            result = result.crop((crop_x, crop_y, x2, y2))
            print(f"   âœ‚ï¸ í¬ë¡­: ({crop_x},{crop_y}) â†’ {result.size}")

        # WebPë¡œ ì¸ì½”ë”©
        img_byte_arr = io.BytesIO()
        result.save(img_byte_arr, format='WEBP', quality=90)

        elapsed = time.time() - start_time
        print(f"âš¡ SAM2 ì™„ë£Œ! ì†Œìš”ì‹œê°„: {elapsed:.2f}ì´ˆ")
        print("-" * 40)

        headers = {
            "X-Original-Width": str(original_w),
            "X-Original-Height": str(original_h),
            "X-Crop-X": str(crop_x),
            "X-Crop-Y": str(crop_y),
            "X-Crop-Width": str(result.width),
            "X-Crop-Height": str(result.height),
            "X-SAM2-Score": f"{mask_score:.3f}",
        }

        clear_gpu_memory()
        return Response(content=img_byte_arr.getvalue(), media_type="image/webp", headers=headers)

    except Exception as e:
        clear_gpu_memory()
        print(f"âŒ SAM2 ì˜¤ë¥˜: {str(e)}")
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"SAM2 ì„¸ê·¸ë©˜í…Œì´ì…˜ ì˜¤ë¥˜: {str(e)}")

# ========== SAM2 ì „ì²´ ì˜¤ë¸Œì íŠ¸ ì„¸ê·¸ë©˜í…Œì´ì…˜ API ==========

@app.post("/segment-all")
async def segment_all(
    file: UploadFile = File(...),
    max_masks: int = Query(default=30, ge=1, le=100, description="ìµœëŒ€ ë§ˆìŠ¤í¬ ìˆ˜"),
    min_area_pct: float = Query(default=0.5, ge=0.0, le=50.0, description="ìµœì†Œ ë©´ì  ë¹„ìœ¨ (%)"),
):
    """
    SAM2 AutomaticMaskGeneratorë¡œ ì´ë¯¸ì§€ ë‚´ ëª¨ë“  ì˜¤ë¸Œì íŠ¸ ìë™ ì„¸ê·¸ë©˜í…Œì´ì…˜.
    label map (grayscale PNG, pixel=segment index, 0=background)ê³¼ ë©”íƒ€ë°ì´í„°ë¥¼ ë°˜í™˜.
    """
    print("-" * 40)
    print(f"ğŸ¯ SAM2 ì „ì²´ ì„¸ê·¸ë©˜í…Œì´ì…˜ ìš”ì²­: {file.filename} (max_masks={max_masks}, min_area_pct={min_area_pct}%)")
    start_time = time.time()

    if not SAM2_AVAILABLE:
        raise HTTPException(status_code=500, detail="SAM2 ëª¨ë“ˆì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    if not is_allowed_image(file):
        raise HTTPException(status_code=400, detail="ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤.")

    image_data = await file.read()
    if len(image_data) > MAX_FILE_SIZE:
        raise HTTPException(status_code=400, detail="íŒŒì¼ì´ ë„ˆë¬´ í½ë‹ˆë‹¤.")

    try:
        image = Image.open(io.BytesIO(image_data))
        image = ImageOps.exif_transpose(image)
        image = image.convert("RGB")
    except Exception:
        raise HTTPException(status_code=400, detail="ì˜¬ë°”ë¥¸ ì´ë¯¸ì§€ í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤.")

    try:
        orig_w, orig_h = image.size

        # ì„±ëŠ¥ ìµœì í™”: max 1024pxë¡œ ë¦¬ì‚¬ì´ì¦ˆ í›„ ì²˜ë¦¬
        MAX_SIDE = 1024
        scale = 1.0
        if max(orig_w, orig_h) > MAX_SIDE:
            scale = MAX_SIDE / max(orig_w, orig_h)
            new_w = int(orig_w * scale)
            new_h = int(orig_h * scale)
            image_small = image.resize((new_w, new_h), Image.Resampling.LANCZOS)
            print(f"   ğŸ“ ë¦¬ì‚¬ì´ì¦ˆ: {orig_w}x{orig_h} â†’ {new_w}x{new_h}")
        else:
            image_small = image
            new_w, new_h = orig_w, orig_h

        generator = get_sam2_mask_generator()

        def _run_auto_mask():
            img_np = np.array(image_small)
            with sam2_lock, torch.inference_mode():
                masks = generator.generate(img_np)
            return masks

        raw_masks = await asyncio.to_thread(_run_auto_mask)
        print(f"   SAM2 ìë™ ë§ˆìŠ¤í¬ {len(raw_masks)}ê°œ ìƒì„±")

        # ë©´ì  í•„í„°ë§ & ì •ë ¬ (ë©´ì  í° ìˆœ)
        total_area = new_w * new_h
        min_area = total_area * (min_area_pct / 100.0)
        filtered = [m for m in raw_masks if m['area'] >= min_area]
        filtered.sort(key=lambda m: m['area'], reverse=True)
        filtered = filtered[:max_masks]
        print(f"   í•„í„°ë§ í›„ {len(filtered)}ê°œ (min_area={min_area:.0f}px)")

        # label map êµ¬ì„± (ì‘ì€ í•´ìƒë„ ê¸°ì¤€)
        label_map_small = np.zeros((new_h, new_w), dtype=np.uint8)
        segments = []
        for i, m in enumerate(filtered):
            idx = i + 1  # 1-based (0=background)
            mask = m['segmentation']  # bool array (new_h, new_w)
            label_map_small[mask] = idx

            # bboxë¥¼ ì›ë³¸ í•´ìƒë„ë¡œ ë³€í™˜
            bx, by, bw, bh = m['bbox']  # XYWH format
            if scale != 1.0:
                bx = int(bx / scale)
                by = int(by / scale)
                bw = int(bw / scale)
                bh = int(bh / scale)
            orig_area = int(m['area'] / (scale * scale))

            segments.append({
                "index": idx,
                "bbox": [bx, by, bx + bw, by + bh],
                "area": orig_area,
                "area_pct": round(orig_area / (orig_w * orig_h) * 100, 2),
                "score": round(float(m.get('predicted_iou', m.get('stability_score', 0))), 3),
            })

        # label mapì„ ì›ë³¸ í¬ê¸°ë¡œ ë³µì› (NEAREST ë³´ê°„ìœ¼ë¡œ ê²½ê³„ ìœ ì§€)
        label_map_pil = Image.fromarray(label_map_small, mode='L')
        if scale != 1.0:
            label_map_pil = label_map_pil.resize((orig_w, orig_h), Image.Resampling.NEAREST)

        # PNGë¡œ ì¸ì½”ë”© â†’ base64
        buf = io.BytesIO()
        label_map_pil.save(buf, format='PNG')
        label_map_b64 = base64.b64encode(buf.getvalue()).decode('ascii')

        elapsed = time.time() - start_time
        print(f"âš¡ SAM2 ì „ì²´ ì„¸ê·¸ë©˜í…Œì´ì…˜ ì™„ë£Œ! {len(segments)}ê°œ ì„¸ê·¸ë¨¼íŠ¸, {elapsed:.2f}ì´ˆ")
        print("-" * 40)

        clear_gpu_memory()
        return JSONResponse(content={
            "segments": segments,
            "label_map": label_map_b64,
            "image_width": orig_w,
            "image_height": orig_h,
        })

    except Exception as e:
        clear_gpu_memory()
        print(f"âŒ SAM2 ì „ì²´ ì„¸ê·¸ë©˜í…Œì´ì…˜ ì˜¤ë¥˜: {str(e)}")
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"SAM2 ì „ì²´ ì„¸ê·¸ë©˜í…Œì´ì…˜ ì˜¤ë¥˜: {str(e)}")

# ========== ì•„ì´ ê°ì§€ API (DINO / MM-DINO / DINO-Base / Florence-2) ==========
# âš ï¸ VRAM ì°¸ê³ : 4ê°œ ëª¨ë¸ ì „ë¶€ ë¡œë“œ ì‹œ ~2.7GB. RTX 4070S(12GB)ì—ì„œ ë‹¤ë¥¸ ëª¨ë¸ê³¼ í•©ì‚° ì‹œ ì£¼ì˜.

def _build_detections(boxes_list, scores_list, labels_list):
    """ê°ì§€ ê²°ê³¼ë¥¼ í†µì¼ëœ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    detections = []
    for i, (box, score) in enumerate(zip(boxes_list, scores_list)):
        x1, y1, x2, y2 = box
        w = x2 - x1
        h = y2 - y1
        detections.append({
            "box": [float(x1), float(y1), float(x2), float(y2)],
            "score": float(score),
            "label": labels_list[i] if i < len(labels_list) else "unknown",
            "width": float(w),
            "height": float(h),
            "area": float(w * h),
            "cx": float(x1 + w / 2),
            "cy": float(y1 + h / 2),
        })
    detections.sort(key=lambda d: d["area"], reverse=True)
    return detections

@app.post("/detect-child")
async def detect_child(
    file: UploadFile = File(...),
    prompt: str = Query(default="child . person", description="ê°ì§€í•  í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ (ë§ˆì¹¨í‘œë¡œ êµ¬ë¶„)"),
    threshold: float = Query(default=0.25, ge=0.05, le=0.9, description="ê°ì§€ ì„ê³„ê°’"),
    model: Literal["gdino", "mmdino", "gdino-base", "florence2"] = Query(default="gdino", description="ê°ì§€ ëª¨ë¸"),
    task: Literal["od", "grounding"] = Query(default="od", description="Florence-2 íƒœìŠ¤í¬"),
):
    """
    ì´ë¯¸ì§€ì—ì„œ ì•„ì´/ì¸ë¬¼ ê°ì§€ (ë‹¤ì¤‘ ëª¨ë¸ ì§€ì›)
    - gdino: Grounding DINO Tiny (ê¸°ë³¸, 48.4 AP)
    - mmdino: MM-DINO Tiny (50.6 AP)
    - gdino-base: Grounding DINO Base (52.5 AP)
    - florence2: Florence-2-large-ft (ë©€í‹°íƒœìŠ¤í¬)
    """
    MODEL_LABELS = {"gdino": "DINO-Tiny", "mmdino": "MM-DINO", "gdino-base": "DINO-Base", "florence2": "Florence-2"}
    model_label = MODEL_LABELS.get(model, model)

    print("-" * 40)
    if model == "florence2":
        print(f"ğŸ” {model_label} ê°ì§€ ìš”ì²­: {file.filename} (model: {model}, task: {task}, prompt: '{prompt}')")
    else:
        print(f"ğŸ” {model_label} ê°ì§€ ìš”ì²­: {file.filename} (model: {model}, prompt: '{prompt}', threshold: {threshold})")
    start_time = time.time()

    if model in ("gdino", "mmdino", "gdino-base") and not GDINO_AVAILABLE:
        raise HTTPException(status_code=500, detail="Grounding DINOê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    if model == "florence2" and not FLORENCE2_AVAILABLE:
        raise HTTPException(status_code=500, detail="Florence-2ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    if not is_allowed_image(file):
        raise HTTPException(status_code=400, detail="ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤.")

    image_data = await file.read()
    if len(image_data) > MAX_FILE_SIZE:
        raise HTTPException(status_code=400, detail="íŒŒì¼ì´ ë„ˆë¬´ í½ë‹ˆë‹¤.")

    try:
        image = Image.open(io.BytesIO(image_data))
        image = ImageOps.exif_transpose(image)
        image = image.convert("RGB")
    except Exception:
        raise HTTPException(status_code=400, detail="ì˜¬ë°”ë¥¸ ì´ë¯¸ì§€ í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤.")

    try:
        # ---- DINO-like ëª¨ë¸ (gdino, mmdino, gdino-base) ----
        if model in ("gdino", "mmdino", "gdino-base"):
            if model == "mmdino":
                m, proc = get_mmdino_model()
            elif model == "gdino-base":
                m, proc = get_gdino_base_model()
            else:
                m, proc = get_gdino_model()

            def _run_dino_like():
                gdino_prompt = prompt.strip()
                if not gdino_prompt.endswith('.'):
                    gdino_prompt += '.'
                inputs = proc(images=image, text=gdino_prompt, return_tensors="pt").to(device)
                with torch.no_grad():
                    outputs = m(**inputs)
                results = proc.post_process_grounded_object_detection(
                    outputs,
                    inputs.input_ids,
                    threshold=threshold,
                    text_threshold=threshold,
                    target_sizes=[image.size[::-1]],
                )[0]
                return results

            results = await asyncio.to_thread(_run_dino_like)
            boxes = results["boxes"].cpu().numpy().tolist()
            scores = results["scores"].cpu().numpy().tolist()
            labels = results["labels"]
            detections = _build_detections(boxes, scores, labels)

        # ---- Florence-2 ----
        elif model == "florence2":
            f2_model, f2_proc = get_florence2_model()

            def _run_florence2():
                if task == "grounding":
                    task_prompt = "<CAPTION_TO_PHRASE_GROUNDING>"
                    text_input = prompt.strip()
                else:
                    task_prompt = "<OD>"
                    text_input = task_prompt

                inputs = f2_proc(text=text_input, images=image, return_tensors="pt")
                inputs = {k: v.to(device) if hasattr(v, 'to') else v for k, v in inputs.items()}
                # FP16 ë³€í™˜
                if inputs.get("pixel_values") is not None:
                    inputs["pixel_values"] = inputs["pixel_values"].to(torch.float16)

                with torch.no_grad():
                    generated_ids = f2_model.generate(
                        input_ids=inputs["input_ids"],
                        pixel_values=inputs["pixel_values"],
                        max_new_tokens=1024,
                        num_beams=3,
                    )
                generated_text = f2_proc.batch_decode(generated_ids, skip_special_tokens=False)[0]
                parsed = f2_proc.post_process_generation(
                    generated_text,
                    task=task_prompt,
                    image_size=(image.width, image.height),
                )
                return parsed, task_prompt

            parsed, task_prompt = await asyncio.to_thread(_run_florence2)

            f2_boxes = []
            f2_labels = []

            if task == "grounding" and "<CAPTION_TO_PHRASE_GROUNDING>" in parsed:
                result = parsed["<CAPTION_TO_PHRASE_GROUNDING>"]
                raw_boxes = result.get("bboxes", [])
                raw_labels = result.get("labels", [])
                for bbox, lbl in zip(raw_boxes, raw_labels):
                    f2_boxes.append(bbox)
                    f2_labels.append(lbl)
            elif "<OD>" in parsed:
                result = parsed["<OD>"]
                raw_boxes = result.get("bboxes", [])
                raw_labels = result.get("labels", [])
                _PERSON_KEYWORDS = {"person", "child", "human", "man", "woman", "boy", "girl", "baby", "kid", "toddler", "infant"}
                for bbox, lbl in zip(raw_boxes, raw_labels):
                    # OD ëª¨ë“œ: ì¸ë¬¼ ê´€ë ¨ ë¼ë²¨ë§Œ í•„í„° (ë‹¨ì–´ ë‹¨ìœ„ ë§¤ì¹­)
                    lbl_words = set(lbl.lower().split())
                    if lbl_words & _PERSON_KEYWORDS:
                        f2_boxes.append(bbox)
                        f2_labels.append(lbl)

            # Florence-2ëŠ” confidence score ì—†ìŒ â†’ 1.0 ê³ ì •
            f2_scores = [1.0] * len(f2_boxes)
            detections = _build_detections(f2_boxes, f2_scores, f2_labels)

        else:
            raise HTTPException(status_code=400, detail=f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸: {model}. gdino|mmdino|gdino-base|florence2 ì¤‘ ì„ íƒ")

        elapsed = time.time() - start_time
        print(f"   ê°ì§€ ê²°ê³¼: {len(detections)}ê°œ ({model_label})")
        for d in detections:
            print(f"   - [{d['label']}] {d['score']:.2f} box=({d['box'][0]:.0f},{d['box'][1]:.0f},{d['box'][2]:.0f},{d['box'][3]:.0f})")
        print(f"âš¡ ì™„ë£Œ! ì†Œìš”ì‹œê°„: {elapsed:.2f}ì´ˆ")
        print("-" * 40)

        clear_gpu_memory()
        return JSONResponse(content={
            "success": True,
            "detections": detections,
            "model": model,
            "image_width": image.width,
            "image_height": image.height,
        })

    except HTTPException:
        raise
    except Exception as e:
        clear_gpu_memory()
        print(f"âŒ {model_label} ê°ì§€ ì˜¤ë¥˜: {str(e)}")
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"{model_label} ê°ì§€ ì˜¤ë¥˜: {str(e)}")

# ========== ViTMatte ì•ŒíŒŒ ë§¤íŒ… API ==========

@app.post("/vitmatte")
async def run_vitmatte(
    file: UploadFile = File(...),
    mask: UploadFile = File(...),
    erode_size: int = Query(default=10, ge=1, le=50, description="Trimap foreground erode í¬ê¸°"),
    dilate_size: int = Query(default=20, ge=1, le=100, description="Trimap unknown ì˜ì—­ dilate í¬ê¸°"),
):
    """
    ViTMatte ì•ŒíŒŒ ë§¤íŒ…

    SAM2 ë“±ì˜ rough maskë¥¼ trimapìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì •ë°€ ì•ŒíŒŒ ë§¤íŠ¸ ìƒì„±.
    ë¨¸ë¦¬ì¹´ë½ í•œ ì˜¬ ë‹¨ìœ„ì˜ ë°˜íˆ¬ëª… ì²˜ë¦¬ ê°€ëŠ¥.

    - file: ì›ë³¸ ì´ë¯¸ì§€
    - mask: ë°”ì´ë„ˆë¦¬ ë§ˆìŠ¤í¬ (í°ìƒ‰=ì „ê²½, ê²€ì •=ë°°ê²½, ì›ë³¸ê³¼ ë™ì¼ í¬ê¸°)
    """
    print("-" * 40)
    print(f"ğŸ¨ ViTMatte ìš”ì²­: {file.filename} (erode={erode_size}, dilate={dilate_size})")
    start_time = time.time()

    if not VITMATTE_AVAILABLE:
        raise HTTPException(status_code=500, detail="ViTMatteê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    # íŒŒì¼ ì½ê¸°
    image_data = await file.read()
    mask_data = await mask.read()

    try:
        image = Image.open(io.BytesIO(image_data))
        image = ImageOps.exif_transpose(image)
        image = image.convert("RGB")
    except Exception:
        raise HTTPException(status_code=400, detail="ì˜¬ë°”ë¥¸ ì´ë¯¸ì§€ í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤.")

    try:
        mask_img = Image.open(io.BytesIO(mask_data)).convert("L")
        # ë§ˆìŠ¤í¬ë¥¼ ì›ë³¸ í¬ê¸°ì— ë§ì¶”ê¸°
        if mask_img.size != image.size:
            mask_img = mask_img.resize(image.size, Image.Resampling.LANCZOS)
    except Exception:
        raise HTTPException(status_code=400, detail="ì˜¬ë°”ë¥¸ ë§ˆìŠ¤í¬ í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤.")

    try:
        import cv2

        vit_model, vit_processor = get_vitmatte_model()

        mask_np = np.array(mask_img)

        # Trimap ìƒì„±: erode â†’ definite FG, dilate â†’ unknown boundary
        kernel_e = np.ones((erode_size, erode_size), np.uint8)
        kernel_d = np.ones((dilate_size, dilate_size), np.uint8)
        fg = cv2.erode(mask_np, kernel_e, iterations=1)
        dilated = cv2.dilate(mask_np, kernel_d, iterations=1)

        trimap = np.zeros_like(mask_np, dtype=np.uint8)
        trimap[fg > 128] = 255           # definite foreground
        trimap[(dilated > 128) & (fg <= 128)] = 128  # unknown
        # rest stays 0 = definite background

        trimap_pil = Image.fromarray(trimap)
        print(f"   Trimap ìƒì„±: FG={np.sum(trimap==255)}, Unknown={np.sum(trimap==128)}, BG={np.sum(trimap==0)}")

        # GPU VRAM ì ˆì•½: í° ì´ë¯¸ì§€ëŠ” ë¦¬ì‚¬ì´ì¦ˆ í›„ ì²˜ë¦¬ â†’ ì•ŒíŒŒë§µë§Œ ì›ë³¸ í¬ê¸°ë¡œ ë³µì›
        MAX_VITMATTE_DIM = 1024
        orig_w, orig_h = image.size
        if max(orig_w, orig_h) > MAX_VITMATTE_DIM:
            scale = MAX_VITMATTE_DIM / max(orig_w, orig_h)
            new_w = int(orig_w * scale)
            new_h = int(orig_h * scale)
            image_small = image.resize((new_w, new_h), Image.Resampling.LANCZOS)
            trimap_small = trimap_pil.resize((new_w, new_h), Image.Resampling.NEAREST)
            print(f"   ğŸ“ ViTMatte ë¦¬ì‚¬ì´ì¦ˆ: {orig_w}x{orig_h} â†’ {new_w}x{new_h}")
        else:
            image_small = image
            trimap_small = trimap_pil

        def _run_vitmatte():
            inputs = vit_processor(images=image_small, trimaps=trimap_small, return_tensors="pt")
            inputs = {k: v.to(device).half() if v.dtype == torch.float32 else v.to(device) for k, v in inputs.items()}
            with torch.no_grad():
                output = vit_model(**inputs)
            alpha = output.alphas[0, 0].float().cpu().numpy()
            alpha = np.clip(alpha * 255, 0, 255).astype(np.uint8)
            return alpha

        alpha_np = await asyncio.to_thread(_run_vitmatte)
        alpha_pil = Image.fromarray(alpha_np)
        # ë¦¬ì‚¬ì´ì¦ˆí–ˆìœ¼ë©´ ì•ŒíŒŒë§µì„ ì›ë³¸ í¬ê¸°ë¡œ ë³µì›
        if alpha_pil.size != (orig_w, orig_h):
            alpha_pil = alpha_pil.resize((orig_w, orig_h), Image.Resampling.LANCZOS)
            print(f"   ğŸ“ ì•ŒíŒŒë§µ ë³µì›: {alpha_np.shape[1]}x{alpha_np.shape[0]} â†’ {orig_w}x{orig_h}")

        # ì›ë³¸ì— ì•ŒíŒŒ ì ìš©
        result = image.copy()
        result.putalpha(alpha_pil)

        # í¬ë¡­ (ì•ŒíŒŒ ê¸°ì¤€)
        alpha_clean = alpha_pil.point(lambda x: 0 if x < 10 else x)
        bbox = alpha_clean.getbbox()
        crop_x, crop_y = 0, 0

        if bbox:
            padding = 20
            x1, y1, x2, y2 = bbox
            crop_x = max(0, x1 - padding)
            crop_y = max(0, y1 - padding)
            x2 = min(result.width, x2 + padding)
            y2 = min(result.height, y2 + padding)
            result = result.crop((crop_x, crop_y, x2, y2))
            print(f"   âœ‚ï¸ í¬ë¡­: ({crop_x},{crop_y}) â†’ {result.size}")

        # WebP ì¸ì½”ë”©
        img_byte_arr = io.BytesIO()
        result.save(img_byte_arr, format='WEBP', quality=90)

        elapsed = time.time() - start_time
        print(f"âš¡ ViTMatte ì™„ë£Œ! ì†Œìš”ì‹œê°„: {elapsed:.2f}ì´ˆ")
        print("-" * 40)

        headers = {
            "X-Original-Width": str(image.width),
            "X-Original-Height": str(image.height),
            "X-Crop-X": str(crop_x),
            "X-Crop-Y": str(crop_y),
            "X-Crop-Width": str(result.width),
            "X-Crop-Height": str(result.height),
        }

        clear_gpu_memory()
        return Response(content=img_byte_arr.getvalue(), media_type="image/webp", headers=headers)

    except Exception as e:
        clear_gpu_memory()
        print(f"âŒ ViTMatte ì˜¤ë¥˜: {str(e)}")
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"ViTMatte ì˜¤ë¥˜: {str(e)}")

# ========== MEMatte ì•ŒíŒŒ ë§¤íŒ… API ==========

mematte_model = None

def get_mematte_model():
    """MEMatte ëª¨ë¸ ë¡œë“œ (Lazy Loading)"""
    global mematte_model
    if mematte_model is not None:
        return mematte_model

    import sys
    mematte_dir = os.path.join(os.path.dirname(__file__), "models", "mematte")
    if mematte_dir not in sys.path:
        sys.path.insert(0, mematte_dir)

    from detectron2.config import LazyConfig, instantiate
    from detectron2.checkpoint import DetectionCheckpointer

    print("ğŸ“‚ MEMatte ëª¨ë¸ ë¡œë”© ì¤‘...")
    cfg = LazyConfig.load(os.path.join(mematte_dir, "configs", "MEMatte_S_topk0.25_win_global_long.py"))
    cfg.model.teacher_backbone = None
    cfg.model.backbone.max_number_token = 18000
    model = instantiate(cfg.model)
    model.to(device)
    model.eval()
    ckpt_path = os.path.join(mematte_dir, "checkpoints", "MEMatte_ViTS_DIM.pth")
    DetectionCheckpointer(model).load(ckpt_path)
    print("âœ… MEMatte ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    mematte_model = model
    return mematte_model

@app.post("/mematte")
async def run_mematte(
    file: UploadFile = File(...),
    mask: UploadFile = File(...),
    erode_size: int = Query(default=10, ge=1, le=50, description="Trimap foreground erode í¬ê¸°"),
    dilate_size: int = Query(default=20, ge=1, le=100, description="Trimap unknown ì˜ì—­ dilate í¬ê¸°"),
):
    """
    MEMatte ì•ŒíŒŒ ë§¤íŒ… (ViTMatte ëŒ€ë¹„ ë©”ëª¨ë¦¬ 88% ì ˆì•½, ë™ì¼ í’ˆì§ˆ)

    ViTMatteì™€ ë™ì¼í•˜ê²Œ rough maskë¥¼ trimapìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì •ë°€ ì•ŒíŒŒ ë§¤íŠ¸ ìƒì„±.
    """
    print("-" * 40)
    print(f"ğŸ§  MEMatte ìš”ì²­: {file.filename} (erode={erode_size}, dilate={dilate_size})")
    start_time = time.time()

    if not is_allowed_image(file):
        raise HTTPException(status_code=400, detail="ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤.")

    image_data = await file.read()
    mask_data = await mask.read()
    if len(image_data) > MAX_FILE_SIZE:
        raise HTTPException(status_code=400, detail="íŒŒì¼ì´ ë„ˆë¬´ í½ë‹ˆë‹¤.")

    try:
        image = Image.open(io.BytesIO(image_data))
        image = ImageOps.exif_transpose(image)
        image = image.convert("RGB")

        mask_img = Image.open(io.BytesIO(mask_data)).convert("L")

        orig_w, orig_h = image.size

        # Trimap ìƒì„± (ViTMatteì™€ ë™ì¼ ë¡œì§)
        import cv2
        mask_np = np.array(mask_img)
        kernel_e = np.ones((erode_size, erode_size), np.uint8)
        kernel_d = np.ones((dilate_size, dilate_size), np.uint8)
        fg = cv2.erode(mask_np, kernel_e, iterations=1)
        dilated = cv2.dilate(mask_np, kernel_d, iterations=1)

        trimap = np.zeros_like(mask_np, dtype=np.uint8)
        trimap[fg > 128] = 255
        trimap[(dilated > 128) & (fg <= 128)] = 128

        trimap_pil = Image.fromarray(trimap)
        print(f"   Trimap ìƒì„±: FG={np.sum(trimap==255)}, Unknown={np.sum(trimap==128)}, BG={np.sum(trimap==0)}")

        model = get_mematte_model()

        from torchvision.transforms import functional as TF
        import torch

        # ì…ë ¥ ì¤€ë¹„: image(3ch) + trimap(1ch) â†’ 4ch tensor
        img_tensor = TF.to_tensor(image)  # [3, H, W]
        tri_tensor = TF.to_tensor(trimap_pil)[0:1, :, :]  # [1, H, W]

        data = {
            'image': img_tensor.unsqueeze(0).to(device),
            'trimap': tri_tensor.unsqueeze(0).to(device),
        }

        def _run_mematte():
            with torch.no_grad():
                output, _, _ = model(data, patch_decoder=True)
                alpha = output['phas'].flatten(0, 2)  # [H, W]
                # Trimap enforce
                tri_flat = tri_tensor.squeeze(0).squeeze(0)
                alpha[tri_flat == 0] = 0
                alpha[tri_flat == 1] = 1
                return alpha.cpu()

        alpha = await asyncio.to_thread(_run_mematte)

        alpha_np = (alpha.numpy() * 255).astype(np.uint8)
        alpha_pil = Image.fromarray(alpha_np).resize((orig_w, orig_h), Image.Resampling.LANCZOS)

        # RGBA ê²°ê³¼ ìƒì„±
        result = image.copy()
        result.putalpha(alpha_pil)

        # í¬ë¡­ (ë¶ˆíˆ¬ëª… ì˜ì—­ë§Œ)
        bbox = result.getbbox()
        if bbox:
            result = result.crop(bbox)
            print(f"   âœ‚ï¸ í¬ë¡­: ({bbox[0]},{bbox[1]}) í¬ê¸°({bbox[2]-bbox[0]}, {bbox[3]-bbox[1]})")

        buf = io.BytesIO()
        result.save(buf, format="WEBP", quality=95)
        buf.seek(0)

        elapsed = time.time() - start_time
        print(f"âœ… MEMatte ì™„ë£Œ! ì†Œìš”ì‹œê°„: {elapsed:.2f}ì´ˆ")

        return Response(content=buf.getvalue(), media_type="image/webp")

    except Exception as e:
        clear_gpu_memory()
        print(f"âŒ MEMatte ì˜¤ë¥˜: {str(e)}")
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"MEMatte ì˜¤ë¥˜: {str(e)}")

# ============================================================
# BiRefNet-HR-matting (trimap-free, ê³ í•´ìƒë„ ë§¤íŒ…)
# ============================================================

_birefnet_matting_model = None

def get_birefnet_matting():
    global _birefnet_matting_model
    if _birefnet_matting_model is not None:
        return _birefnet_matting_model
    from transformers import AutoModelForImageSegmentation
    print("ğŸ“¦ BiRefNet-HR-matting ëª¨ë¸ ë¡œë”©...")
    model = AutoModelForImageSegmentation.from_pretrained(
        "ZhengPeng7/BiRefNet_HR-matting", trust_remote_code=True
    )
    model.to(device, dtype=torch.float16)
    model.eval()
    _birefnet_matting_model = model
    print(f"âœ… BiRefNet-HR-matting ë¡œë”© ì™„ë£Œ ({sum(p.numel() for p in model.parameters()) / 1e6:.1f}M, FP16)")
    return model


@app.post("/birefnet-matting")
async def run_birefnet_matting(
    file: UploadFile = File(...),
    resolution: int = Query(default=2048, ge=512, le=4096, description="ì²˜ë¦¬ í•´ìƒë„ (ê¸´ ìª½ ê¸°ì¤€)"),
):
    """
    BiRefNet-HR-matting â€” trimap ì—†ì´ ì´ë¯¸ì§€ë§Œìœ¼ë¡œ ê³ í’ˆì§ˆ ì•ŒíŒŒ ë§¤íŒ….
    ë¨¸ë¦¬ì¹´ë½/ë°˜íˆ¬ëª… ê²½ê³„ë¥¼ ì •ë°€í•˜ê²Œ ì²˜ë¦¬.
    """
    print("-" * 40)
    print(f"ğŸ¨ BiRefNet-HR-matting ìš”ì²­: {file.filename} (resolution={resolution})")
    start_time = time.time()

    image_data = await file.read()
    try:
        image = Image.open(io.BytesIO(image_data))
        image = ImageOps.exif_transpose(image)
        image = image.convert("RGB")
    except Exception:
        raise HTTPException(status_code=400, detail="ì˜¬ë°”ë¥¸ ì´ë¯¸ì§€ í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤.")

    try:
        from torchvision import transforms

        model = get_birefnet_matting()
        orig_w, orig_h = image.size

        # í•´ìƒë„ ì¡°ì •
        scale = min(resolution / max(orig_w, orig_h), 1.0)
        proc_w = int(orig_w * scale)
        proc_h = int(orig_h * scale)
        # 32ë°°ìˆ˜ ì •ë ¬
        proc_w = (proc_w + 31) // 32 * 32
        proc_h = (proc_h + 31) // 32 * 32

        transform = transforms.Compose([
            transforms.Resize((proc_h, proc_w)),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
        ])
        input_tensor = transform(image).unsqueeze(0).to(device, dtype=torch.float16)

        with torch.no_grad():
            preds = model(input_tensor)[-1].sigmoid()

        alpha = preds[0, 0].cpu().float().numpy()
        alpha = (alpha * 255).astype(np.uint8)

        del input_tensor, preds
        torch.cuda.empty_cache()

        # ì›ë³¸ í¬ê¸°ë¡œ ë³µì›
        alpha_img = Image.fromarray(alpha).resize((orig_w, orig_h), Image.Resampling.LANCZOS)

        # RGBA í•©ì„±
        result = image.copy()
        result.putalpha(alpha_img)

        elapsed = time.time() - start_time
        print(f"âœ… BiRefNet-HR-matting ì™„ë£Œ: {orig_w}x{orig_h} â†’ {proc_w}x{proc_h} | {elapsed:.2f}ì´ˆ")

        buf = io.BytesIO()
        result.save(buf, format="WEBP", quality=95, lossless=False)
        buf.seek(0)
        return Response(content=buf.getvalue(), media_type="image/webp")

    except Exception as e:
        clear_gpu_memory()
        print(f"âŒ BiRefNet-HR-matting ì˜¤ë¥˜: {str(e)}")
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"BiRefNet-HR-matting ì˜¤ë¥˜: {str(e)}")


# ============================================================
# DiffMatte (diffusion ê¸°ë°˜ ë§¤íŒ…, trimap í•„ìš”)
# ============================================================

_diffmatte_model = None
DIFFMATTE_DIR = r"C:\Documents and Settings\connect\automation-prototype\DiffMatte"

def get_diffmatte():
    global _diffmatte_model
    if _diffmatte_model is not None:
        return _diffmatte_model

    import sys as _sys
    if DIFFMATTE_DIR not in _sys.path:
        _sys.path.insert(0, DIFFMATTE_DIR)

    from detectron2.config import LazyConfig, instantiate
    from detectron2.checkpoint import DetectionCheckpointer
    from re import findall

    config_path = os.path.join(DIFFMATTE_DIR, "configs", "ViTB.py")
    checkpoint_path = os.path.join(DIFFMATTE_DIR, "checkpoints", "DiffMatte-ViTB.pth")
    sample_strategy = "ddim10"

    print(f"ğŸ“¦ DiffMatte-ViTB ëª¨ë¸ ë¡œë”©... ({checkpoint_path})")
    cfg = LazyConfig.load(config_path)

    cfg.difmatte.args["use_ddim"] = True if "ddim" in sample_strategy else False
    cfg.diffusion.steps = int(findall(r"\d+", sample_strategy)[0])

    model = instantiate(cfg.model)
    diffusion = instantiate(cfg.diffusion)
    cfg.difmatte.model = model
    cfg.difmatte.diffusion = diffusion
    difmatte = instantiate(cfg.difmatte)
    difmatte.to(device)
    difmatte.eval()
    DetectionCheckpointer(difmatte).load(checkpoint_path)

    _diffmatte_model = difmatte
    print(f"âœ… DiffMatte-ViTB ë¡œë”© ì™„ë£Œ (FP32, max_sizeë¡œ VRAM ê´€ë¦¬)")
    return difmatte


@app.post("/diffmatte")
async def run_diffmatte(
    file: UploadFile = File(...),
    mask: UploadFile = File(...),
    erode_size: int = Query(default=10, ge=1, le=50),
    dilate_size: int = Query(default=20, ge=1, le=100),
    max_size: int = Query(default=1024, ge=256, le=2048, description="ì²˜ë¦¬ í•´ìƒë„ (ê¸´ ìª½ ê¸°ì¤€). ViT ì–´í…ì…˜ íŠ¹ì„±ìƒ í° ì´ë¯¸ì§€ëŠ” OOM ìœ„í—˜"),
):
    """
    DiffMatte â€” Diffusion ê¸°ë°˜ ë§¤íŒ… (ECCV 2024, Composition-1k SOTAê¸‰).
    trimapì´ í•„ìš”í•©ë‹ˆë‹¤ (maskì—ì„œ ìë™ ìƒì„±).
    """
    print("-" * 40)
    print(f"ğŸ¨ DiffMatte ìš”ì²­: {file.filename} (erode={erode_size}, dilate={dilate_size}, max_size={max_size})")
    start_time = time.time()

    image_data = await file.read()
    mask_data = await mask.read()

    try:
        image = Image.open(io.BytesIO(image_data))
        image = ImageOps.exif_transpose(image)
        image = image.convert("RGB")
    except Exception:
        raise HTTPException(status_code=400, detail="ì˜¬ë°”ë¥¸ ì´ë¯¸ì§€ í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤.")

    orig_size = image.size  # (W, H) â€” ì¶œë ¥ì€ ì›ë³¸ í¬ê¸°ë¡œ ë³µì›

    try:
        mask_img = Image.open(io.BytesIO(mask_data)).convert("L")
        if mask_img.size != image.size:
            mask_img = mask_img.resize(image.size, Image.Resampling.LANCZOS)
    except Exception:
        raise HTTPException(status_code=400, detail="ì˜¬ë°”ë¥¸ ë§ˆìŠ¤í¬ í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤.")

    # ë¦¬ì‚¬ì´ì¦ˆ (ViT ì–´í…ì…˜ O(nÂ²) ë•Œë¬¸ì— VRAM ì ˆì•½ í•„ìˆ˜)
    w, h = image.size
    if max(w, h) > max_size:
        scale = max_size / max(w, h)
        new_w, new_h = int(w * scale), int(h * scale)
        image = image.resize((new_w, new_h), Image.Resampling.LANCZOS)
        mask_img = mask_img.resize((new_w, new_h), Image.Resampling.LANCZOS)
        print(f"   ë¦¬ì‚¬ì´ì¦ˆ: {w}x{h} â†’ {new_w}x{new_h}")

    try:
        import cv2
        from torchvision.transforms import functional as TF

        model = get_diffmatte()

        # Trimap ìƒì„±
        mask_np = np.array(mask_img)
        kernel_e = np.ones((erode_size, erode_size), np.uint8)
        kernel_d = np.ones((dilate_size, dilate_size), np.uint8)
        fg = cv2.erode(mask_np, kernel_e, iterations=1)
        dilated = cv2.dilate(mask_np, kernel_d, iterations=1)

        trimap_np = np.zeros_like(mask_np, dtype=np.uint8)
        trimap_np[fg > 128] = 255
        trimap_np[(dilated > 128) & (fg <= 128)] = 128

        # í…ì„œ ë³€í™˜
        image_tensor = TF.to_tensor(image).unsqueeze(0)
        trimap_tensor = TF.to_tensor(Image.fromarray(trimap_np).convert("L")).unsqueeze(0)

        # trimapì„ 3ë‹¨ê³„ ê°’ìœ¼ë¡œ ì •ê·œí™”
        trimap_tensor[trimap_tensor > 0.9] = 1.0
        trimap_tensor[(trimap_tensor >= 0.1) & (trimap_tensor <= 0.9)] = 0.5
        trimap_tensor[trimap_tensor < 0.1] = 0.0

        input_data = {"image": image_tensor.to(device), "trimap": trimap_tensor.to(device)}

        print(f"   ì¶”ë¡  ì‹œì‘ (ì…ë ¥: {image_tensor.shape})")
        with torch.no_grad():
            output = model(input_data)

        # GPU í…ì„œ ì •ë¦¬
        del input_data, image_tensor, trimap_tensor
        torch.cuda.empty_cache()

        print(f"   ì¶”ë¡  ì™„ë£Œ, ì¶œë ¥ íƒ€ì…: {type(output)}, shape: {getattr(output, 'shape', 'N/A')}")

        # outputì€ numpy array (H, W) values 0-255
        if isinstance(output, np.ndarray):
            alpha_np = output
        elif hasattr(output, 'cpu'):
            alpha_np = output.cpu().float().numpy()
        else:
            alpha_np = np.array(output)

        if alpha_np.ndim == 3:
            alpha_np = alpha_np[0] if alpha_np.shape[0] == 1 else alpha_np.squeeze()

        if alpha_np.max() <= 1.0:
            alpha_np = np.clip(alpha_np * 255, 0, 255).astype(np.uint8)
        else:
            alpha_np = np.clip(alpha_np, 0, 255).astype(np.uint8)

        # ì›ë³¸ í¬ê¸°ë¡œ alpha ë³µì›
        alpha_img = Image.fromarray(alpha_np)
        if alpha_img.size != orig_size:
            alpha_img = alpha_img.resize(orig_size, Image.Resampling.LANCZOS)

        # RGBA í•©ì„± (ì›ë³¸ í¬ê¸° ì´ë¯¸ì§€ ì‚¬ìš©)
        orig_image = Image.open(io.BytesIO(image_data))
        orig_image = ImageOps.exif_transpose(orig_image).convert("RGB")
        result = orig_image.copy()
        result.putalpha(alpha_img)

        elapsed = time.time() - start_time
        print(f"âœ… DiffMatte ì™„ë£Œ: {orig_size[0]}x{orig_size[1]} (ì²˜ë¦¬: {image.size[0]}x{image.size[1]}) | {elapsed:.2f}ì´ˆ")

        buf = io.BytesIO()
        result.save(buf, format="WEBP", quality=95, lossless=False)
        buf.seek(0)
        return Response(content=buf.getvalue(), media_type="image/webp")

    except Exception as e:
        clear_gpu_memory()
        print(f"âŒ DiffMatte ì˜¤ë¥˜: {str(e)}")
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"DiffMatte ì˜¤ë¥˜: {str(e)}")


@app.get("/health")
async def health_check():
    """ì„œë²„ ìƒíƒœ í™•ì¸"""
    return JSONResponse(content={
        "status": "ok",
        "device": device,
        "dtype": str(dtype),
        "ryan_engine": RYAN_ENGINE_AVAILABLE,
        "loaded_models": list(loaded_models.keys()) + (["ben2"] if ben2_model is not None else []) + (["sam2"] if sam2_predictor is not None else []) + (["sam2_amg"] if sam2_mask_generator is not None else []) + (["mematte"] if mematte_model is not None else []),
        "sam2_available": SAM2_AVAILABLE,
        "gdino_available": GDINO_AVAILABLE,
        "vitmatte_available": VITMATTE_AVAILABLE
    })

if __name__ == "__main__":
    import uvicorn
    import os
    port = int(os.environ.get("PORT", 5001))
    workers = int(os.environ.get("WORKERS", 1))
    uvicorn.run("server:app", host="0.0.0.0", port=port, workers=workers,
                h11_max_incomplete_event_size=1024*1024,
                timeout_keep_alive=120)