# server.py (ìµœì í™” ë²„ì „: FP16 + Warmup + ë³´ì•ˆ ê°•í™”)
from fastapi import FastAPI, File, UploadFile, HTTPException, Query, Form, Body
from fastapi.responses import Response, JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from transformers import AutoModelForImageSegmentation
from torchvision import transforms
from pydantic import BaseModel
from typing import Optional, List, Dict, Any
from PIL import Image, ImageOps
from pillow_heif import register_heif_opener
register_heif_opener()
import torch
import gc
import io
import time
import asyncio
import numpy as np
import os
import json
import re
import traceback
import httpx
from pathlib import Path

# Ryan Engine ì„í¬íŠ¸
import sys
sys.path.insert(0, str(Path(__file__).parent))
try:
    from ryan_engine import JosaUtils, BookGenerator
    RYAN_ENGINE_AVAILABLE = True
    print("âœ… Ryan Engine ë¡œë“œ ì™„ë£Œ")
except ImportError as e:
    RYAN_ENGINE_AVAILABLE = False
    print(f"âš ï¸ Ryan Engine ë¡œë“œ ì‹¤íŒ¨: {e}")

# BGQA (ë°°ê²½ ì œê±° í’ˆì§ˆ í‰ê°€) ì„í¬íŠ¸
try:
    from bgqa import evaluate as bgqa_evaluate
    BGQA_AVAILABLE = True
    print("âœ… BGQA ë¡œë“œ ì™„ë£Œ")
except ImportError as e:
    BGQA_AVAILABLE = False
    print(f"âš ï¸ BGQA ë¡œë“œ ì‹¤íŒ¨: {e}")

# PNG ì €ì¥ í´ë” ì„¤ì •
PNG_OUTPUT_DIR = Path("./png")
PNG_OUTPUT_DIR.mkdir(exist_ok=True)

# ViTPose transformers ë²„ê·¸ íŒ¨ì¹˜ (inv í•¨ìˆ˜ ëˆ„ë½ ë¬¸ì œ)
try:
    import transformers.models.vitpose.image_processing_vitpose as vitpose_module
    import numpy.linalg
    # ëª¨ë“ˆì˜ ê¸€ë¡œë²Œ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì— inv í•¨ìˆ˜ ì£¼ì…
    vitpose_module.__dict__['inv'] = numpy.linalg.inv
    # scipy_warp_affine í•¨ìˆ˜ì˜ ê¸€ë¡œë²Œì—ë„ ì£¼ì…
    if hasattr(vitpose_module, 'scipy_warp_affine'):
        vitpose_module.scipy_warp_affine.__globals__['inv'] = numpy.linalg.inv
    print("âœ… ViTPose íŒ¨ì¹˜ ì ìš© ì™„ë£Œ (inv í•¨ìˆ˜ ì£¼ì…)")
except Exception as e:
    print(f"âš ï¸ ViTPose íŒ¨ì¹˜ ìŠ¤í‚µ: {e}")

app = FastAPI()

# í—ˆìš©ëœ Origin ëª©ë¡ (í”„ë¡œë•ì…˜ì—ì„œëŠ” ì‹¤ì œ ë„ë©”ì¸ìœ¼ë¡œ ë³€ê²½)
ALLOWED_ORIGINS = [
    "http://localhost:3000",
    "http://localhost:5500",
    "http://localhost:8080",
    "http://localhost:8888",
    "http://127.0.0.1:3000",
    "http://127.0.0.1:5500",
    "http://127.0.0.1:8080",
    "http://127.0.0.1:8888",
    "null",  # file:// í”„ë¡œí† ì½œìš©
]

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"] if os.environ.get("CORS_ALLOW_ALL", "1") == "1" else ALLOWED_ORIGINS,
    allow_credentials=True,
    allow_methods=["GET", "POST"],  # GET ì¶”ê°€ (í—¬ìŠ¤ì²´í¬ ë“±)
    allow_headers=["Content-Type"],  # í•„ìš”í•œ í—¤ë”ë§Œ í—ˆìš©
    expose_headers=["X-Original-Width", "X-Original-Height", "X-Crop-X", "X-Crop-Y", "X-Crop-Width", "X-Crop-Height", "X-BGQA-Score", "X-BGQA-Passed", "X-BGQA-Issues", "X-BGQA-CaseType"],  # í´ë¼ì´ì–¸íŠ¸ì—ì„œ ì½ì„ ìˆ˜ ìˆëŠ” ì»¤ìŠ¤í…€ í—¤ë”
)

# íŒŒì¼ ê²€ì¦ ìƒìˆ˜
MAX_FILE_SIZE = 20 * 1024 * 1024  # 20MB
ALLOWED_CONTENT_TYPES = {"image/jpeg", "image/png", "image/webp", "image/gif", "image/heic", "image/heif"}
ALLOWED_EXTENSIONS = {".jpg", ".jpeg", ".png", ".webp", ".gif", ".heic", ".heif"}

def is_allowed_image(file) -> bool:
    """content_type ë˜ëŠ” í™•ì¥ìë¡œ ì´ë¯¸ì§€ íŒŒì¼ ì—¬ë¶€ í™•ì¸"""
    if file.content_type in ALLOWED_CONTENT_TYPES:
        return True
    if file.filename:
        ext = os.path.splitext(file.filename)[1].lower()
        if ext in ALLOWED_EXTENSIONS:
            return True
    return False

# 1. ë””ë°”ì´ìŠ¤ ì„¤ì •
if torch.backends.mps.is_available():
    device = "mps"
    dtype = torch.float16  # [ìµœì í™”] ë§¥ë¶ì€ float16ì´ í›¨ì”¬ ë¹ ë¦„
elif torch.cuda.is_available():
    device = "cuda"
    dtype = torch.float16
else:
    device = "cpu"
    dtype = torch.float32

# Portrait ëª¨ë¸ì„ CPUë¡œ ëŒë ¤ BEN2(GPU)ì™€ ë³‘ë ¬ ì²˜ë¦¬
PORTRAIT_ON_CPU = False

print(f"\nğŸš€ ì´ˆê³ ì† AI ì„œë²„ ëŒ€ê¸° ì¤‘... (Device: {device}, Type: {dtype})")
if PORTRAIT_ON_CPU:
    print(f"   â†³ Portrait ëª¨ë¸: CPU (float32) â€” BEN2ì™€ ë³‘ë ¬ ì²˜ë¦¬ ê°€ëŠ¥")

def clear_gpu_memory():
    """GPU ë©”ëª¨ë¦¬ ìºì‹œ í•´ì œ"""
    gc.collect()
    if device == "mps":
        torch.mps.empty_cache()
    elif device == "cuda":
        torch.cuda.empty_cache()

# BEN2 ì„í¬íŠ¸
try:
    from ben2 import BEN_Base
    BEN2_AVAILABLE = True
    print("âœ… BEN2 ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ")
except ImportError:
    BEN2_AVAILABLE = False
    print("âš ï¸ BEN2 ëª¨ë“ˆ ì—†ìŒ (pip install ben2)")

# remove.bg API ì„¤ì •
REMOVEBG_API_KEY = os.environ.get("REMOVEBG_API_KEY", "D8B2GQyMvmfbXXfH2mZukPi4")

# 2. ëª¨ë¸ ì„¤ì • (Lazy Loading)
# ì§€ì›ë˜ëŠ” BiRefNet ëª¨ë¸ë“¤ (ëª¨ë‘ ë¡œì»¬)
BIREFNET_MODELS = {
    "portrait": "./models/birefnet-portrait",
    "hr": "./models/birefnet-hr",
    "hr-matting": "./models/birefnet-hr-matting",
    "dynamic": "./models/birefnet-dynamic",
    "rmbg2": "./models/rmbg2",
}

# torch.compile ê°€ìš©ì„± ì²´í¬ (Triton í•„ìš”)
TORCH_COMPILE_OK = False
if os.environ.get("TORCH_COMPILE", "1") == "1":
    try:
        import triton
        TORCH_COMPILE_OK = True
        print("âœ… Triton ê°ì§€ â€” torch.compile í™œì„±í™”")
    except ImportError:
        print("âš ï¸ Triton ë¯¸ì„¤ì¹˜ â€” torch.compile ë¹„í™œì„±í™” (WindowsëŠ” ë¯¸ì§€ì›)")

# ë¡œë“œëœ ëª¨ë¸ ìºì‹œ
loaded_models = {}
ben2_model = None

def get_ben2_model():
    """BEN2 ëª¨ë¸ ë¡œë“œ (Lazy Loading)"""
    global ben2_model
    if ben2_model is not None:
        return ben2_model
    if not BEN2_AVAILABLE:
        raise ValueError("BEN2 ëª¨ë“ˆì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install ben2")
    print("ğŸ“‚ BEN2 ëª¨ë¸ ë¡œë”© ì¤‘...")
    ben2_model = BEN_Base.from_pretrained("PramaLLC/BEN2")
    ben2_model.to(device)
    ben2_model.eval()
    print("âœ… BEN2 ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    return ben2_model

async def call_removebg_api(image_data: bytes) -> Image.Image:
    """remove.bg API í˜¸ì¶œí•˜ì—¬ ë°°ê²½ ì œê±°ëœ RGBA ì´ë¯¸ì§€ ë°˜í™˜"""
    if not REMOVEBG_API_KEY:
        raise ValueError("REMOVEBG_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    async with httpx.AsyncClient(timeout=60.0) as client:
        resp = await client.post(
            "https://api.remove.bg/v1.0/removebg",
            headers={"X-Api-Key": REMOVEBG_API_KEY},
            files={"image_file": ("image.jpg", image_data, "image/jpeg")},
            data={"size": "auto", "format": "png", "channels": "rgba"},
        )
    if resp.status_code != 200:
        error_detail = resp.json().get("errors", [{}])[0].get("title", resp.text) if resp.headers.get("content-type", "").startswith("application/json") else resp.text[:200]
        raise ValueError(f"remove.bg API ì˜¤ë¥˜ ({resp.status_code}): {error_detail}")
    return Image.open(io.BytesIO(resp.content)).convert("RGBA")

def get_birefnet_model(model_type: str = "portrait") -> AutoModelForImageSegmentation:
    """BiRefNet ëª¨ë¸ ë¡œë“œ (Lazy Loading)"""
    global loaded_models

    if model_type in loaded_models:
        return loaded_models[model_type]

    model_path = BIREFNET_MODELS.get(model_type)
    if not model_path:
        raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸: {model_type}")

    print(f"ğŸ“‚ {model_type} ëª¨ë¸ ë¡œë”© ì¤‘... ({model_path})")

    try:
        model = AutoModelForImageSegmentation.from_pretrained(
            model_path,
            trust_remote_code=True,
            local_files_only=True
        )
    except OSError as e:
        print(f"âŒ ì˜¤ë¥˜: ëª¨ë¸ í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤. ({model_path})")
        raise e

    # Portrait â†’ CPU(float32), ë‚˜ë¨¸ì§€ â†’ GPU(float16)ë¡œ ë³‘ë ¬ ì²˜ë¦¬ ê°€ëŠ¥
    target_device = device
    if PORTRAIT_ON_CPU and model_type == "portrait":
        target_device = "cpu"

    model.to(target_device)
    if target_device != "cpu":
        model.half()
    model.eval()
    print(f"   â†³ ë””ë°”ì´ìŠ¤: {target_device}")

    # torch.compile ìµœì í™” (Triton í•„ìš” â€” Linux/WSLë§Œ ì§€ì›)
    if TORCH_COMPILE_OK:
        try:
            model = torch.compile(model)
            print(f"   â†³ torch.compile ì ìš©")
        except Exception as e:
            print(f"   âš ï¸ torch.compile ìŠ¤í‚µ: {e}")

    loaded_models[model_type] = model
    print(f"âœ… {model_type} ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    return model

# 3. ëª¨ë“  ëª¨ë¸ ì‚¬ì „ ë¡œë“œ + ì›Œë°ì—…
# ì„œë²„ ì‹œì‘ ì‹œ 3ê°œ ëª¨ë¸ ëª¨ë‘ VRAMì— ì˜¬ë ¤ë‘ê¸° (ì²« ìš”ì²­ ì§€ì—° ì œê±°)

def warmup_birefnet(model, name):
    """BiRefNet ëª¨ë¸ ì›Œë°ì—… (torch.compile ì²« ì‹¤í–‰ ê·¸ë˜í”„ ìƒì„± í¬í•¨)"""
    model_device = next(model.parameters()).device
    print(f"ğŸ”¥ {name} ì›Œë°ì—… ì¤‘ ({model_device})...")
    with torch.no_grad():
        dummy = torch.randn(1, 3, 1024, 1024).to(model_device)
        if model_device.type != "cpu":
            dummy = dummy.half()
        model(dummy)
        del dummy
    clear_gpu_memory()
    print(f"   âœ… {name} ì›Œë°ì—… ì™„ë£Œ")

# Portrait ëª¨ë¸
print("ğŸ“‚ ëª¨ë“  ë°°ê²½ ì œê±° ëª¨ë¸ ì‚¬ì „ ë¡œë”© ì¤‘...")
try:
    portrait_model = get_birefnet_model("portrait")
except OSError:
    print(f"âŒ ì˜¤ë¥˜: portrait ëª¨ë¸ í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤.")
    exit()
warmup_birefnet(portrait_model, "portrait")

# hr-matting ëª¨ë¸
try:
    hrmatting_model = get_birefnet_model("hr-matting")
    warmup_birefnet(hrmatting_model, "hr-matting")
except Exception as e:
    print(f"âš ï¸ hr-matting ì‚¬ì „ ë¡œë“œ ì‹¤íŒ¨: {e}")

# BEN2 ëª¨ë¸
if BEN2_AVAILABLE:
    try:
        ben2 = get_ben2_model()
        # BEN2 ì›Œë°ì—…: ë”ë¯¸ ì´ë¯¸ì§€ë¡œ inference í•œ ë²ˆ
        print(f"ğŸ”¥ BEN2 ì›Œë°ì—… ì¤‘ ({device})...")
        dummy_img = Image.new("RGB", (512, 512), (128, 128, 128))
        with torch.no_grad():
            ben2.inference(dummy_img)
        del dummy_img
        clear_gpu_memory()
        print(f"   âœ… BEN2 ì›Œë°ì—… ì™„ë£Œ")
    except Exception as e:
        print(f"âš ï¸ BEN2 ì‚¬ì „ ë¡œë“œ ì‹¤íŒ¨: {e}")

print("âœ… ëª¨ë“  ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ!")

# ì •ê·œí™” ì„¤ì •
transform_normalize = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])

def process_image_fast(image: Image.Image, max_size: int = 1440, model_type: str = "portrait") -> Image.Image:
    """
    ì´ë¯¸ì§€ ë°°ê²½ ì œê±° ì²˜ë¦¬
    max_size: ì²˜ë¦¬ í•´ìƒë„ (720=ë¹ ë¦„, 1024=ì¤‘ê°„, 1440=ê¶Œì¥, 2048=ìµœê³ í’ˆì§ˆ, 9999=ì›ë³¸)
    model_type: BiRefNet ëª¨ë¸ ì¢…ë¥˜ (portrait, hr, hr-matting, dynamic)
    """
    w, h = image.size

    # ì›ë³¸ í™”ì§ˆ ëª¨ë“œ (9999 ì´ìƒì´ë©´ ë¦¬ì‚¬ì´ì¦ˆ ì•ˆí•¨)
    if max_size >= 9999:
        # ì›ë³¸ í¬ê¸° ì‚¬ìš© (32ì˜ ë°°ìˆ˜ë¡œë§Œ ì¡°ì •)
        new_w = (w // 32) * 32
        new_h = (h // 32) * 32
        print(f"ğŸ“ ì›ë³¸ í™”ì§ˆ ëª¨ë“œ: {w}x{h} â†’ {new_w}x{new_h}")
    else:
        # í—ˆìš©ëœ í•´ìƒë„ ë²”ìœ„ë¡œ ì œí•œ (ë³´ì•ˆ)
        max_size = max(512, min(2500, max_size))
        scale = min(max_size / w, max_size / h)
        new_w = int(w * scale)
        new_h = int(h * scale)
        new_w = (new_w // 32) * 32
        new_h = (new_h // 32) * 32

    # MPSëŠ” ê³ í•´ìƒë„ convolution ë¯¸ì§€ì› â†’ ì•ˆì „í•œ ìµœëŒ€ í•´ìƒë„ë¡œ í´ë¨í•‘
    # Portrait on CPUë©´ ì´ ì œí•œ ì ìš© ì•ˆ í•¨
    MPS_MAX_SIDE = 2560
    model_on_mps = not (PORTRAIT_ON_CPU and model_type == "portrait") and device == "mps"
    if model_on_mps and max(new_w, new_h) > MPS_MAX_SIDE:
        scale_down = MPS_MAX_SIDE / max(new_w, new_h)
        new_w = (int(new_w * scale_down) // 32) * 32
        new_h = (int(new_h * scale_down) // 32) * 32
        print(f"âš ï¸ MPS í•œê³„ â†’ ì²˜ë¦¬ í•´ìƒë„ ì¶•ì†Œ: {new_w}x{new_h}")

    # ë¦¬ì‚¬ì´ì§•
    image_resized = image.resize((new_w, new_h), Image.Resampling.LANCZOS)

    # ëª¨ë¸ ê°€ì ¸ì˜¤ê¸° (Lazy Loading)
    model = get_birefnet_model(model_type)

    # ëª¨ë¸ ë””ë°”ì´ìŠ¤ ìë™ ê°ì§€ (portrait=CPU, ë‚˜ë¨¸ì§€=GPU)
    model_device = next(model.parameters()).device

    # í…ì„œ ë³€í™˜ â€” ëª¨ë¸ ë””ë°”ì´ìŠ¤ì— ë§ì¶¤
    input_tensor = transform_normalize(image_resized).unsqueeze(0).to(model_device)

    # GPU(float16) / CPU(float32) ìë™ íŒë³„
    if model_device.type != "cpu":
        input_tensor = input_tensor.half()

    # ì¶”ë¡ 
    with torch.no_grad():
        preds = model(input_tensor)[-1].sigmoid().cpu()

    # ë§ˆìŠ¤í¬ ë³µì›
    pred = preds[0].squeeze().float() # ë‹¤ì‹œ float32ë¡œ ë³€í™˜ (ì´ë¯¸ì§€ ì €ì¥ìš©)
    pred_pil = transforms.ToPILImage()(pred)
    mask = pred_pil.resize((w, h), Image.Resampling.LANCZOS)

    return mask

# ========== ë§ˆìŠ¤í¬ ë¦¬íŒŒì¸ í•¨ìˆ˜ë“¤ ==========
def refine_guided_filter(image: Image.Image, mask: Image.Image, r: int = 8, eps: float = 1e-3) -> Image.Image:
    """Guided Filter: ì›ë³¸ ì´ë¯¸ì§€ ì—£ì§€ë¥¼ ì°¸ì¡°í•˜ì—¬ ë§ˆìŠ¤í¬ ê²½ê³„ ì •ì œ"""
    import cv2
    guide = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2GRAY).astype(np.float32) / 255.0
    src = np.array(mask).astype(np.float32) / 255.0
    refined = cv2.ximgproc.guidedFilter(guide, src, radius=r, eps=eps)
    refined = np.clip(refined * 255, 0, 255).astype(np.uint8)
    return Image.fromarray(refined)

def refine_pymatting(image: Image.Image, mask: Image.Image) -> Image.Image:
    """PyMatting: ì•ŒíŒŒ ë§¤íŒ…ìœ¼ë¡œ ë°˜íˆ¬ëª… ê²½ê³„ ì •ë°€ ì²˜ë¦¬"""
    from pymatting import estimate_alpha_cf
    # ì²˜ë¦¬ ì†ë„ë¥¼ ìœ„í•´ ìµœëŒ€ 1024ë¡œ ì¶•ì†Œ í›„ ë‹¤ì‹œ ë³µì›
    orig_size = mask.size
    max_side = 1024
    if max(orig_size) > max_side:
        scale = max_side / max(orig_size)
        small_size = (int(orig_size[0] * scale), int(orig_size[1] * scale))
        image_small = image.resize(small_size, Image.Resampling.LANCZOS)
        mask_small = mask.resize(small_size, Image.Resampling.LANCZOS)
    else:
        image_small = image
        mask_small = mask
        small_size = orig_size

    img_np = np.array(image_small).astype(np.float64) / 255.0
    mask_np = np.array(mask_small).astype(np.float64) / 255.0
    # trimap ìƒì„±: í™•ì‹¤í•œ ì „ê²½/ë°°ê²½ + ë¶ˆí™•ì‹¤ ì˜ì—­
    trimap = np.zeros_like(mask_np)
    trimap[mask_np > 0.9] = 1.0
    trimap[(mask_np > 0.1) & (mask_np <= 0.9)] = 0.5

    alpha = estimate_alpha_cf(img_np, trimap)
    alpha = np.clip(alpha * 255, 0, 255).astype(np.uint8)
    result = Image.fromarray(alpha)
    if small_size != orig_size:
        result = result.resize(orig_size, Image.Resampling.LANCZOS)
    return result

def refine_foreground_color(image: Image.Image, mask: Image.Image, r: int = 90) -> Image.Image:
    """Fast Foreground Estimation: ì „ê²½ ìƒ‰ìƒ ì¶”ì •ìœ¼ë¡œ ë°˜íˆ¬ëª… ì˜ì—­ ê°œì„ 
    ë§ˆìŠ¤í¬ê°€ ì•„ë‹Œ ì „ê²½ ì´ë¯¸ì§€ë¥¼ ë°˜í™˜ (ì•ŒíŒŒ í•©ì„± ì‹œ ìƒ‰ë²ˆì§ ì œê±°)"""
    import cv2
    # ì†ë„ë¥¼ ìœ„í•´ ìµœëŒ€ 1024ë¡œ ì¶•ì†Œ í›„ ì²˜ë¦¬
    orig_size = image.size
    max_side = 1024
    if max(orig_size) > max_side:
        scale = max_side / max(orig_size)
        small_size = (int(orig_size[0] * scale), int(orig_size[1] * scale))
        image_s = image.resize(small_size, Image.Resampling.LANCZOS)
        mask_s = mask.resize(small_size, Image.Resampling.LANCZOS)
    else:
        image_s = image
        mask_s = mask
        small_size = orig_size

    img_np = np.array(image_s).astype(np.float32) / 255.0
    mask_np = np.array(mask_s).astype(np.float32) / 255.0
    if mask_np.ndim == 2:
        mask_np = mask_np[:, :, np.newaxis]

    # Blur fusion (2íšŒ ë°˜ë³µì´ë©´ ì¶©ë¶„)
    for _ in range(2):
        blurred_img = cv2.GaussianBlur(img_np, (0, 0), sigmaX=r, sigmaY=r)
        blurred_mask = cv2.GaussianBlur(mask_np[:, :, 0], (0, 0), sigmaX=r, sigmaY=r)
        blurred_mask = np.maximum(blurred_mask, 1e-6)[:, :, np.newaxis]
        foreground = np.clip(blurred_img / blurred_mask, 0, 1)
        img_np = img_np * mask_np + foreground * (1 - mask_np)

    result = Image.fromarray((img_np * 255).astype(np.uint8))
    if small_size != orig_size:
        result = result.resize(orig_size, Image.Resampling.LANCZOS)
    return result

@app.post("/remove-bg")
async def remove_background(
    file: UploadFile = File(...),
    max_size: int = Query(default=1440, ge=512, le=9999, description="ì²˜ë¦¬ í•´ìƒë„ (512-2500, 9999=ì›ë³¸)"),
    model: str = Query(default="portrait", pattern="^(portrait|hr|hr-matting|dynamic|rmbg2|ben2|removebg)$", description="ë°°ê²½ ì œê±° ëª¨ë¸"),
    case_type: str = Query(default="auto", description="í”¼ì‚¬ì²´ ìœ í˜•: auto, KID_PERSON, ADULT_PERSON, TOY_OBJECT"),
    has_face: bool = Query(default=True, description="ì–¼êµ´ ê°ì§€ ì—¬ë¶€ (Face API ê²°ê³¼)"),
    refine: str = Query(default="none", pattern="^(none|guided|pymatting|fg_estimate)$", description="ë§ˆìŠ¤í¬ ë¦¬íŒŒì¸ ë°©ë²•")
):
    print("-" * 40)
    print(f"ğŸ“¸ ìš”ì²­: {file.filename} (í’ˆì§ˆ: {max_size}px, ëª¨ë¸: {model}, ë¦¬íŒŒì¸: {refine})")
    start_time = time.time()

    # 1. íŒŒì¼ íƒ€ì… ê²€ì¦
    if file.content_type not in ALLOWED_CONTENT_TYPES:
        raise HTTPException(
            status_code=400,
            detail=f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤. í—ˆìš©: {', '.join(ALLOWED_CONTENT_TYPES)}"
        )

    # 2. íŒŒì¼ ì½ê¸° ë° í¬ê¸° ê²€ì¦
    image_data = await file.read()
    if len(image_data) > MAX_FILE_SIZE:
        raise HTTPException(
            status_code=400,
            detail=f"íŒŒì¼ì´ ë„ˆë¬´ í½ë‹ˆë‹¤. ìµœëŒ€ {MAX_FILE_SIZE // (1024*1024)}MBê¹Œì§€ í—ˆìš©ë©ë‹ˆë‹¤."
        )

    # 3. ì´ë¯¸ì§€ ìœ íš¨ì„± ê²€ì¦
    try:
        image = Image.open(io.BytesIO(image_data))
        image.verify()  # ì´ë¯¸ì§€ íŒŒì¼ì¸ì§€ ê²€ì¦
        # verify() í›„ì—ëŠ” ë‹¤ì‹œ ì—´ì–´ì•¼ í•¨
        image = Image.open(io.BytesIO(image_data))
        image = ImageOps.exif_transpose(image)  # EXIF íšŒì „ ì ìš©
        image = image.convert("RGB")
    except Exception as e:
        raise HTTPException(
            status_code=400,
            detail="ì†ìƒëœ ì´ë¯¸ì§€ íŒŒì¼ì´ê±°ë‚˜ ì˜¬ë°”ë¥¸ ì´ë¯¸ì§€ í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤."
        )

    try:
        # ì›ë³¸ í¬ê¸° ì €ì¥ (í¬ë¡­ ì •ë³´ í—¤ë”ìš©)
        original_w, original_h = image.size

        if model == "removebg":
            # remove.bg API í˜¸ì¶œ (ì™¸ë¶€ ì„œë¹„ìŠ¤)
            result_rgba = await call_removebg_api(image_data)
            # ì›ë³¸ê³¼ í¬ê¸°ê°€ ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì›ë³¸ í¬ê¸°ë¡œ ë¦¬ì‚¬ì´ì¦ˆ
            if result_rgba.size != (original_w, original_h):
                result_rgba = result_rgba.resize((original_w, original_h), Image.Resampling.LANCZOS)
            mask = result_rgba.split()[-1]
        elif model == "ben2":
            # BEN2ëŠ” ìì²´ inference API ì‚¬ìš© (GPUì—ì„œ ì‹¤í–‰)
            # asyncio.to_threadë¡œ ì´ë²¤íŠ¸ ë£¨í”„ ë¸”ë¡œí‚¹ ë°©ì§€ â†’ portrait(CPU)ì™€ ë³‘ë ¬ ê°€ëŠ¥
            ben2 = get_ben2_model()
            def _run_ben2():
                with torch.no_grad():
                    return ben2.inference(image)
            result_rgba = await asyncio.to_thread(_run_ben2)
            # RGBA ê²°ê³¼ì—ì„œ ì•ŒíŒŒ ì±„ë„ì„ ë§ˆìŠ¤í¬ë¡œ ì¶”ì¶œ
            mask = result_rgba.split()[-1]
        else:
            # portrait ë“± BiRefNet ëª¨ë¸ (CPU ë˜ëŠ” GPU)
            # asyncio.to_threadë¡œ ì´ë²¤íŠ¸ ë£¨í”„ ë¸”ë¡œí‚¹ ë°©ì§€ â†’ ben2(GPU)ì™€ ë³‘ë ¬ ê°€ëŠ¥
            mask = await asyncio.to_thread(process_image_fast, image, max_size, model)

        # ë§ˆìŠ¤í¬ ë¦¬íŒŒì¸ ì ìš©
        if refine != "none":
            refine_start = time.time()
            if refine == "guided":
                mask = refine_guided_filter(image, mask)
                print(f"ğŸ”§ Guided Filter ë¦¬íŒŒì¸ ì™„ë£Œ ({time.time() - refine_start:.2f}ì´ˆ)")
            elif refine == "pymatting":
                mask = refine_pymatting(image, mask)
                print(f"ğŸ”§ PyMatting ë¦¬íŒŒì¸ ì™„ë£Œ ({time.time() - refine_start:.2f}ì´ˆ)")
            elif refine == "fg_estimate":
                # ì „ê²½ ìƒ‰ìƒ ì¶”ì •ì€ ë§ˆìŠ¤í¬ ì ìš© í›„ ì²˜ë¦¬ (ì•„ë˜ì—ì„œ)
                pass

        # BGQA í’ˆì§ˆ í‰ê°€ â€” í˜„ì¬ í”„ë¦¬ë·°ì—ì„œ ë¯¸ì‚¬ìš©, ìŠ¤í‚µí•˜ì—¬ ì†ë„ í–¥ìƒ
        bgqa_score = 100.0
        bgqa_passed = True
        bgqa_issues = []
        bgqa_case_type = "KID_PERSON"

        # fg_estimate: ì „ê²½ ìƒ‰ìƒ ì¶”ì •ìœ¼ë¡œ ë°˜íˆ¬ëª… ì˜ì—­ ìƒ‰ë²ˆì§ ì œê±°
        if refine == "fg_estimate":
            refine_start = time.time()
            refined_fg = refine_foreground_color(image, mask)
            image = refined_fg
            print(f"ğŸ”§ Foreground Estimation ë¦¬íŒŒì¸ ì™„ë£Œ ({time.time() - refine_start:.2f}ì´ˆ)")

        image.putalpha(mask)

        # ì•ŒíŒŒ ì±„ë„ ê¸°ì¤€ìœ¼ë¡œ ì½˜í…ì¸  ì˜ì—­ í¬ë¡­ (ë¹ˆ ê³µê°„ ì œê±°)
        alpha = image.split()[-1]  # ì•ŒíŒŒ ì±„ë„ ì¶”ì¶œ
        # ì•ŒíŒŒê°’ 30 ë¯¸ë§Œì€ íˆ¬ëª… ì²˜ë¦¬ (ë°°ê²½ ì”ì—¬ë¬¼/ë…¸ì´ì¦ˆ ì œê±°)
        alpha_clean = alpha.point(lambda x: 0 if x < 30 else x)
        bbox = alpha_clean.getbbox()  # ë¶ˆíˆ¬ëª… í”½ì…€ì˜ ë°”ìš´ë”© ë°•ìŠ¤

        # í¬ë¡­ ì¢Œí‘œ ì´ˆê¸°í™”
        crop_x, crop_y = 0, 0

        if bbox:
            # íŒ¨ë”© ì¶”ê°€ (20px)
            padding = 20
            x1, y1, x2, y2 = bbox
            crop_x = max(0, x1 - padding)
            crop_y = max(0, y1 - padding)
            x2 = min(image.width, x2 + padding)
            y2 = min(image.height, y2 + padding)

            # í¬ë¡­
            original_size = image.size
            image = image.crop((crop_x, crop_y, x2, y2))
            print(f"âœ‚ï¸  í¬ë¡­: {original_size} â†’ {image.size} (íŒ¨ë”© {padding}px)")

        img_byte_arr = io.BytesIO()
        # WebPë¡œ ì €ì¥ (PNGë³´ë‹¤ ì¸ì½”ë”© 2ë°° ë¹ ë¦„, ìš©ëŸ‰ 50% ê°ì†Œ)
        image.save(img_byte_arr, format='WEBP', quality=90)

        # PNG ì €ì¥ ìŠ¤í‚µ â€” í”„ë¦¬ë·° ì†ë„ ìš°ì„ 

        print(f"âš¡ ì™„ë£Œ! ì†Œìš”ì‹œê°„: {time.time() - start_time:.2f}ì´ˆ")
        print("-" * 40)

        # í¬ë¡­ ì •ë³´ë¥¼ í—¤ë”ì— í¬í•¨ (ë§ˆì»¤ ì¢Œí‘œ ë³´ì •ìš©)
        headers = {
            "X-Original-Width": str(original_w),
            "X-Original-Height": str(original_h),
            "X-Crop-X": str(crop_x),
            "X-Crop-Y": str(crop_y),
            "X-Crop-Width": str(image.width),
            "X-Crop-Height": str(image.height),
            "X-BGQA-Score": str(bgqa_score),
            "X-BGQA-Passed": str(bgqa_passed).lower(),
            "X-BGQA-Issues": ",".join(bgqa_issues) if bgqa_issues else "",
            "X-BGQA-CaseType": bgqa_case_type,
        }

        clear_gpu_memory()
        return Response(content=img_byte_arr.getvalue(), media_type="image/webp", headers=headers)
    except Exception as e:
        clear_gpu_memory()
        print(f"âŒ ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail="ì´ë¯¸ì§€ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì´ë¯¸ì§€ë¥¼ ì‹œë„í•´ì£¼ì„¸ìš”."
        )

# ========== ViTPose ëª¨ë¸ (Lazy Loading) ==========
vitpose_model = None
vitpose_huge_model = None
vitpose_processor = None

def load_vitpose_model(model_type="vitpose"):
    """ViTPose ëª¨ë¸ ë¡œë“œ (ì²˜ìŒ ìš”ì²­ ì‹œì—ë§Œ)"""
    global vitpose_model, vitpose_huge_model, vitpose_processor

    try:
        from transformers import AutoProcessor, AutoModel, VitPoseForPoseEstimation

        model_name = "usyd-community/vitpose-base-simple" if model_type == "vitpose" else "usyd-community/vitpose-huge-simple"

        if model_type == "vitpose" and vitpose_model is None:
            print(f"ğŸ“‚ ViTPose ëª¨ë¸ ë¡œë”© ì¤‘... ({model_name})")
            vitpose_processor = AutoProcessor.from_pretrained(model_name)
            vitpose_model = VitPoseForPoseEstimation.from_pretrained(model_name)
            vitpose_model.to(device)
            vitpose_model.eval()
            print("âœ… ViTPose ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            return vitpose_model, vitpose_processor

        elif model_type == "vitpose-huge" and vitpose_huge_model is None:
            print(f"ğŸ“‚ ViTPose-Huge ëª¨ë¸ ë¡œë”© ì¤‘... ({model_name})")
            if vitpose_processor is None:
                vitpose_processor = AutoProcessor.from_pretrained(model_name)
            vitpose_huge_model = VitPoseForPoseEstimation.from_pretrained(model_name)
            vitpose_huge_model.to(device)
            vitpose_huge_model.eval()
            print("âœ… ViTPose-Huge ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            return vitpose_huge_model, vitpose_processor

        # ì´ë¯¸ ë¡œë“œëœ ëª¨ë¸ ë°˜í™˜
        if model_type == "vitpose":
            return vitpose_model, vitpose_processor
        else:
            return vitpose_huge_model, vitpose_processor

    except ImportError as e:
        print(f"âŒ Import ì˜¤ë¥˜: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"ViTPose ëª¨ë¸ì„ ì‚¬ìš©í•˜ë ¤ë©´ transformers>=4.45.0ì´ í•„ìš”í•©ë‹ˆë‹¤. ì˜¤ë¥˜: {str(e)}"
        )
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë“œ ì˜¤ë¥˜: {str(e)}")
        traceback.print_exc()
        raise HTTPException(
            status_code=500,
            detail=f"ViTPose ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {str(e)}"
        )

# COCO 17ê°œ í‚¤í¬ì¸íŠ¸ë¥¼ BlazePose 33ê°œì— ë§¤í•‘ (í˜¸í™˜ì„±)
COCO_TO_BLAZEPOSE = {
    0: 0,    # nose
    1: 2,    # left_eye
    2: 5,    # right_eye
    3: 7,    # left_ear
    4: 8,    # right_ear
    5: 11,   # left_shoulder
    6: 12,   # right_shoulder
    7: 13,   # left_elbow
    8: 14,   # right_elbow
    9: 15,   # left_wrist
    10: 16,  # right_wrist
    11: 23,  # left_hip
    12: 24,  # right_hip
    13: 25,  # left_knee
    14: 26,  # right_knee
    15: 27,  # left_ankle
    16: 28,  # right_ankle
}

# COCO ì†ëª© í‚¤í¬ì¸íŠ¸ ì¸ë±ìŠ¤
COCO_LEFT_WRIST = 9
COCO_RIGHT_WRIST = 10


def extract_wrist_keypoints(image: Image.Image, min_score: float = 0.3) -> list:
    """
    ViTPoseë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ì—ì„œ ì†ëª© í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ

    Args:
        image: PIL Image
        min_score: ìµœì†Œ ì‹ ë¢°ë„ (ê¸°ë³¸ê°’ 0.3)

    Returns:
        [(x1, y1), (x2, y2)] í˜•íƒœì˜ ì†ëª© ì¢Œí‘œ ë¦¬ìŠ¤íŠ¸
        ì‹ ë¢°ë„ê°€ ë‚®ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
    """
    global vitpose_model, vitpose_processor

    try:
        # ëª¨ë¸ ë¡œë“œ (Lazy)
        pose_model, processor = load_vitpose_model("vitpose")

        # ì „ì²´ ì´ë¯¸ì§€ë¥¼ í•˜ë‚˜ì˜ person bboxë¡œ ì²˜ë¦¬
        boxes = [[[0, 0, image.width, image.height]]]
        inputs = processor(images=image, boxes=boxes, return_tensors="pt")
        inputs = {k: v.to(device) for k, v in inputs.items()}

        # ì¶”ë¡ 
        with torch.no_grad():
            outputs = pose_model(**inputs)

        # ê²°ê³¼ ì²˜ë¦¬
        results = processor.post_process_pose_estimation(outputs, boxes=boxes)[0][0]
        keypoints_xy = results['keypoints'].cpu().numpy()
        scores = results['scores'].cpu().numpy()

        # ì†ëª© í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ
        wrist_keypoints = []

        # ì™¼ìª½ ì†ëª©
        if scores[COCO_LEFT_WRIST] >= min_score:
            kp = keypoints_xy[COCO_LEFT_WRIST]
            wrist_keypoints.append((float(kp[0]), float(kp[1])))

        # ì˜¤ë¥¸ìª½ ì†ëª©
        if scores[COCO_RIGHT_WRIST] >= min_score:
            kp = keypoints_xy[COCO_RIGHT_WRIST]
            wrist_keypoints.append((float(kp[0]), float(kp[1])))

        return wrist_keypoints

    except Exception as e:
        print(f"âš ï¸ ì†ëª© í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        return []

@app.post("/detect-pose")
async def detect_pose(
    file: UploadFile = File(...),
    model: str = Query(default="vitpose", pattern="^(vitpose|vitpose-huge)$", description="ëª¨ë¸ ì„ íƒ")
):
    """ViTPoseë¥¼ ì‚¬ìš©í•œ í¬ì¦ˆ ê°ì§€"""
    print("-" * 40)
    print(f"ğŸ¦´ í¬ì¦ˆ ê°ì§€ ìš”ì²­: {file.filename} (ëª¨ë¸: {model})")
    start_time = time.time()

    # íŒŒì¼ ê²€ì¦
    if not is_allowed_image(file):
        raise HTTPException(status_code=400, detail="ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤.")

    image_data = await file.read()
    if len(image_data) > MAX_FILE_SIZE:
        raise HTTPException(status_code=400, detail="íŒŒì¼ì´ ë„ˆë¬´ í½ë‹ˆë‹¤.")

    try:
        image = Image.open(io.BytesIO(image_data)).convert("RGB")
    except Exception:
        raise HTTPException(status_code=400, detail="ì˜¬ë°”ë¥¸ ì´ë¯¸ì§€ í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤.")

    try:
        # ëª¨ë¸ ë¡œë“œ (Lazy)
        pose_model, processor = load_vitpose_model(model)

        # ì´ë¯¸ì§€ ì „ì²˜ë¦¬ - ViTPoseëŠ” bounding box í•„ìš”
        # ì „ì²´ ì´ë¯¸ì§€ë¥¼ í•˜ë‚˜ì˜ person bboxë¡œ ì²˜ë¦¬
        boxes = [[[0, 0, image.width, image.height]]]  # batch, num_persons, 4
        inputs = processor(images=image, boxes=boxes, return_tensors="pt")
        inputs = {k: v.to(device) for k, v in inputs.items()}

        # ì¶”ë¡ 
        with torch.no_grad():
            outputs = pose_model(**inputs)

        # ê²°ê³¼ ì²˜ë¦¬
        # ViTPose ì¶œë ¥: pose_logits [batch, num_persons, num_keypoints, height, width]
        # post_process_pose_estimationìœ¼ë¡œ í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ
        results = processor.post_process_pose_estimation(outputs, boxes=boxes)[0][0]

        # keypoints: [17, 2], scores: [17]
        keypoints_xy = results['keypoints'].cpu().numpy()
        scores = results['scores'].cpu().numpy()

        print(f"ğŸ¦´ ê°ì§€ëœ í‚¤í¬ì¸íŠ¸: {len(keypoints_xy)}ê°œ")

        # BlazePose í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (33ê°œ í‚¤í¬ì¸íŠ¸, ì—†ëŠ” ê±´ 0ìœ¼ë¡œ)
        blazepose_keypoints = []
        for i in range(33):
            # COCOì—ì„œ ë§¤í•‘ëœ í‚¤í¬ì¸íŠ¸ ì°¾ê¸°
            coco_idx = None
            for coco_i, blaze_i in COCO_TO_BLAZEPOSE.items():
                if blaze_i == i:
                    coco_idx = coco_i
                    break

            if coco_idx is not None and coco_idx < len(keypoints_xy):
                kp = keypoints_xy[coco_idx]
                score = float(scores[coco_idx])
                blazepose_keypoints.append({
                    "x": float(kp[0]),
                    "y": float(kp[1]),
                    "score": score,
                    "name": f"keypoint_{i}"
                })
            else:
                # ë§¤í•‘ë˜ì§€ ì•Šì€ í‚¤í¬ì¸íŠ¸ëŠ” 0ìœ¼ë¡œ
                blazepose_keypoints.append({
                    "x": 0,
                    "y": 0,
                    "score": 0,
                    "name": f"keypoint_{i}"
                })

        # ë°œëª© í‚¤í¬ì¸íŠ¸ í™•ì¸ ë¡œê·¸
        ankle_left = blazepose_keypoints[27]
        ankle_right = blazepose_keypoints[28]
        print(f"ğŸ¦¶ ë°œëª© í‚¤í¬ì¸íŠ¸ - ì™¼ìª½(27): score={ankle_left['score']:.3f}, ì˜¤ë¥¸ìª½(28): score={ankle_right['score']:.3f}")
        print(f"âš¡ ì™„ë£Œ! ì†Œìš”ì‹œê°„: {time.time() - start_time:.2f}ì´ˆ")
        print("-" * 40)

        clear_gpu_memory()
        return JSONResponse(content={
            "success": True,
            "model": model,
            "keypoints": blazepose_keypoints,
            "image_width": image.width,
            "image_height": image.height
        })

    except HTTPException:
        raise
    except Exception as e:
        clear_gpu_memory()
        print(f"âŒ í¬ì¦ˆ ê°ì§€ ì˜¤ë¥˜: {str(e)}")
        traceback.print_exc()
        raise HTTPException(
            status_code=500,
            detail=f"í¬ì¦ˆ ê°ì§€ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )

# ========== HEIC ë³€í™˜ API ==========

@app.post("/convert-heic")
async def convert_heic(file: UploadFile = File(...)):
    """HEIC/HEIF â†’ JPEG ë³€í™˜"""
    if not is_allowed_image(file):
        raise HTTPException(status_code=400, detail="ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤.")

    image_data = await file.read()
    try:
        image = Image.open(io.BytesIO(image_data))
        image = ImageOps.exif_transpose(image)
        image = image.convert("RGB")
    except Exception:
        raise HTTPException(status_code=400, detail="ì˜¬ë°”ë¥¸ ì´ë¯¸ì§€ í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤.")

    buf = io.BytesIO()
    image.save(buf, format="JPEG", quality=95)
    buf.seek(0)

    from starlette.responses import StreamingResponse
    return StreamingResponse(buf, media_type="image/jpeg")

# ========== Smart Crop API ==========

@app.post("/smart-crop")
async def smart_crop(
    file: UploadFile = File(...),
    padding_ratio: float = Query(default=0.25, ge=0.0, le=1.0, description="í¬ë¡­ íŒ¨ë”© ë¹„ìœ¨"),
    min_score: float = Query(default=0.3, ge=0.0, le=1.0, description="í‚¤í¬ì¸íŠ¸ ìµœì†Œ ì‹ ë¢°ë„"),
    seg_size: int = Query(default=512, ge=128, le=1024, description="ì„¸ê·¸ë©˜í…Œì´ì…˜ ë§ˆìŠ¤í¬ í•´ìƒë„"),
    crop_mode: str = Query(default="person", description="í¬ë¡­ ëª¨ë“œ: person(ì¸ë¬¼) ë˜ëŠ” object(ë¬¼ê±´)"),
):
    """ViTPose í‚¤í¬ì¸íŠ¸ + ì„¸ê·¸ë©˜í…Œì´ì…˜ ë§ˆìŠ¤í¬ ê¸°ë°˜ ìŠ¤ë§ˆíŠ¸ í¬ë¡­"""
    print("-" * 40)
    mode_label = "ì¸ë¬¼" if crop_mode == "person" else "ë¬¼ê±´"
    print(f"âœ‚ï¸ ìŠ¤ë§ˆíŠ¸ í¬ë¡­ ìš”ì²­: {file.filename} (ëª¨ë“œ: {mode_label}, seg_size: {seg_size})")
    start_time = time.time()

    # íŒŒì¼ ê²€ì¦
    if not is_allowed_image(file):
        raise HTTPException(status_code=400, detail="ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤.")

    image_data = await file.read()
    if len(image_data) > MAX_FILE_SIZE:
        raise HTTPException(status_code=400, detail="íŒŒì¼ì´ ë„ˆë¬´ í½ë‹ˆë‹¤.")

    try:
        image = Image.open(io.BytesIO(image_data))
        image.verify()
        image = Image.open(io.BytesIO(image_data))
        image = ImageOps.exif_transpose(image)
        image = image.convert("RGB")
    except Exception:
        raise HTTPException(status_code=400, detail="ì˜¬ë°”ë¥¸ ì´ë¯¸ì§€ í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤.")

    # === ë¬¼ê±´ ëª¨ë“œ: ë§ˆìŠ¤í¬ë§Œìœ¼ë¡œ í¬ë¡­ ===
    if crop_mode == "object":
        try:
            seg_start = time.time()
            seg_scale = min(seg_size / image.width, seg_size / image.height)
            seg_w = max(32, (int(image.width * seg_scale) // 32) * 32)
            seg_h = max(32, (int(image.height * seg_scale) // 32) * 32)

            seg_resized = image.resize((seg_w, seg_h), Image.Resampling.LANCZOS)
            seg_model = get_birefnet_model("portrait")
            seg_dev = next(seg_model.parameters()).device
            seg_tensor = transform_normalize(seg_resized).unsqueeze(0).to(seg_dev)
            if seg_dev.type != "cpu":
                seg_tensor = seg_tensor.half()

            with torch.no_grad():
                seg_pred = seg_model(seg_tensor)[-1].sigmoid().cpu()

            seg_mask = seg_pred[0].squeeze().float().numpy()
            mask_binary = seg_mask > 0.5
            rows = np.any(mask_binary, axis=1)
            cols = np.any(mask_binary, axis=0)

            if not rows.any() or not cols.any():
                print(f"âš ï¸ ë§ˆìŠ¤í¬ì—ì„œ ëŒ€ìƒ ë¯¸ê°ì§€")
                clear_gpu_memory()
                return JSONResponse(content={"cropped": False, "reason": "ëŒ€ìƒ ë¯¸ê°ì§€"})

            r_min, r_max = np.where(rows)[0][[0, -1]]
            c_min, c_max = np.where(cols)[0][[0, -1]]
            scale_x = image.width / seg_w
            scale_y = image.height / seg_h
            mask_x_min = c_min * scale_x
            mask_y_min = r_min * scale_y
            mask_x_max = (c_max + 1) * scale_x
            mask_y_max = (r_max + 1) * scale_y

            # ìƒí•˜ì¢Œìš° 10% íŒ¨ë”©
            mask_w = mask_x_max - mask_x_min
            mask_h = mask_y_max - mask_y_min
            mask_x_min = max(0, mask_x_min - mask_w * 0.1)
            mask_y_min = max(0, mask_y_min - mask_h * 0.1)
            mask_x_max = min(image.width, mask_x_max + mask_w * 0.1)
            mask_y_max = min(image.height, mask_y_max + mask_h * 0.1)

            mask_bbox = {"x_min": float(mask_x_min), "y_min": float(mask_y_min), "x_max": float(mask_x_max), "y_max": float(mask_y_max)}

            crop_x = max(0, int(mask_x_min))
            crop_y = max(0, int(mask_y_min))
            crop_x2 = min(image.width, int(mask_x_max))
            crop_y2 = min(image.height, int(mask_y_max))
            crop_w = crop_x2 - crop_x
            crop_h = crop_y2 - crop_y

            # í¬ë¡­ ì˜ì—­ì´ ì›ë³¸ì˜ 90% ì´ìƒì´ë©´ ìŠ¤í‚µ
            crop_area = crop_w * crop_h
            image_area = image.width * image.height
            is_cropped = crop_area < image_area * 0.9

            print(f"   ğŸ­ ë§ˆìŠ¤í¬ bbox: ({mask_x_min:.0f}, {mask_y_min:.0f})â†’({mask_x_max:.0f}, {mask_y_max:.0f}) [{time.time() - seg_start:.2f}ì´ˆ]")
            if not is_cropped:
                print(f"âš ï¸ í¬ë¡­ ì˜ì—­ì´ ì›ë³¸ì˜ {crop_area / image_area * 100:.0f}%ë¡œ í¬ë¡­ ë¶ˆí•„ìš”")
            else:
                print(f"âœ‚ï¸ í¬ë¡­ ì¢Œí‘œ: ({crop_x}, {crop_y}) {crop_w}x{crop_h}")
            print(f"âš¡ ì™„ë£Œ! ì†Œìš”ì‹œê°„: {time.time() - start_time:.2f}ì´ˆ")
            print("-" * 40)

            clear_gpu_memory()
            return JSONResponse(content={
                "cropped": is_cropped,
                "reason": None if is_cropped else "í¬ë¡­ ë¶ˆí•„ìš” (90% ì´ìƒ)",
                "crop": {"x": crop_x, "y": crop_y, "width": crop_w, "height": crop_h},
                "image_width": image.width,
                "image_height": image.height,
                "mask_bbox": mask_bbox,
            })
        except Exception as e:
            clear_gpu_memory()
            print(f"âŒ ë¬¼ê±´ í¬ë¡­ ì˜¤ë¥˜: {str(e)}")
            traceback.print_exc()
            raise HTTPException(status_code=500, detail=f"ë¬¼ê±´ í¬ë¡­ ì¤‘ ì˜¤ë¥˜: {str(e)}")

    try:
        # ViTPose ëª¨ë¸ ë¡œë“œ ë° ì¶”ë¡  (ì¸ë¬¼ ëª¨ë“œ)
        pose_model, processor = load_vitpose_model("vitpose")

        boxes = [[[0, 0, image.width, image.height]]]
        inputs = processor(images=image, boxes=boxes, return_tensors="pt")
        inputs = {k: v.to(device) for k, v in inputs.items()}

        with torch.no_grad():
            outputs = pose_model(**inputs)

        results = processor.post_process_pose_estimation(outputs, boxes=boxes)[0][0]
        keypoints_xy = results['keypoints'].cpu().numpy()
        scores = results['scores'].cpu().numpy()

        # score > min_scoreì¸ í‚¤í¬ì¸íŠ¸ë§Œ ì‚¬ìš©
        valid_mask = scores > min_score
        valid_count = int(valid_mask.sum())

        if valid_count < 3:
            print(f"âš ï¸ ìœ íš¨ í‚¤í¬ì¸íŠ¸ ë¶€ì¡±: {valid_count}ê°œ (ìµœì†Œ 3ê°œ í•„ìš”)")
            clear_gpu_memory()
            return JSONResponse(content={"cropped": False, "reason": "ìœ íš¨ í‚¤í¬ì¸íŠ¸ ë¶€ì¡±"})

        valid_kps = keypoints_xy[valid_mask]

        # === ì†ê°€ë½ ë ì¶”ì •: ì–´ê¹¨â†’íŒ”ê¿ˆì¹˜ 100% AND íŒ”ê¿ˆì¹˜â†’ì†ëª© 50% ë‘˜ ë‹¤ ì¶”ê°€ ===
        # COCO: 5=L_shoulder, 7=L_elbow, 9=L_wrist, 6=R_shoulder, 8=R_elbow, 10=R_wrist
        HAND_SETS = [
            (5, 7, 9),   # ì™¼ìª½: shoulder, elbow, wrist
            (6, 8, 10),  # ì˜¤ë¥¸ìª½: shoulder, elbow, wrist
        ]

        extra_points = []
        for sh_idx, el_idx, wr_idx in HAND_SETS:
            # ì–´ê¹¨â†’íŒ”ê¿ˆì¹˜ ë°©í–¥ìœ¼ë¡œ íŒ”ê¿ˆì¹˜ì—ì„œ +100% ì—°ì¥
            if scores[sh_idx] > min_score and scores[el_idx] > min_score:
                sx, sy = keypoints_xy[sh_idx]
                ex, ey = keypoints_xy[el_idx]
                dx, dy = ex - sx, ey - sy
                cx = max(0, min(image.width, ex + dx * 1.5))
                cy = max(0, min(image.height, ey + dy * 1.5))
                extra_points.append((cx, cy))
                print(f"   ğŸ–ï¸ finger ì¶”ì •: ({cx:.0f}, {cy:.0f}) [shoulderâ†’elbow+150%]")
            # íŒ”ê¿ˆì¹˜â†’ì†ëª© ë°©í–¥ìœ¼ë¡œ ì†ëª©ì—ì„œ +50% ì—°ì¥
            if scores[el_idx] > min_score and scores[wr_idx] > min_score:
                ex, ey = keypoints_xy[el_idx]
                wx, wy = keypoints_xy[wr_idx]
                dx, dy = wx - ex, wy - ey
                cx = max(0, min(image.width, wx + dx * 1.0))
                cy = max(0, min(image.height, wy + dy * 1.0))
                extra_points.append((cx, cy))
                print(f"   ğŸ–ï¸ finger ì¶”ì •: ({cx:.0f}, {cy:.0f}) [elbowâ†’wrist+100%]")

        # === ë°œë ì¶”ì •: ë” ì•„ë˜ìª½ ë°œëª© ê¸°ì¤€, ì—‰ë©ì´â†’ë¬´ë¦ vs ë¬´ë¦â†’ë°œëª© 70% ì¤‘ ë” ë¨¼ ìª½ ===
        # COCO: 11=L_hip, 13=L_knee, 15=L_ankle, 12=R_hip, 14=R_knee, 16=R_ankle
        FOOT_SETS = [
            (11, 13, 15),  # ì™¼ìª½: hip, knee, ankle
            (12, 14, 16),  # ì˜¤ë¥¸ìª½: hip, knee, ankle
        ]

        # ë” ì•„ë˜(yê°€ í°) ë°œëª© ìª½ ì„ íƒ
        lower_foot = None
        lower_ankle_y = -1
        for hp_idx, kn_idx, ak_idx in FOOT_SETS:
            if scores[ak_idx] > min_score:
                if keypoints_xy[ak_idx][1] > lower_ankle_y:
                    lower_ankle_y = keypoints_xy[ak_idx][1]
                    lower_foot = (hp_idx, kn_idx, ak_idx)

        if lower_foot:
            hp_idx, kn_idx, ak_idx = lower_foot
            hx, hy = keypoints_xy[hp_idx]
            kx, ky = keypoints_xy[kn_idx]
            ax, ay = keypoints_xy[ak_idx]

            # ì—‰ë©ì´â†’ë¬´ë¦ 100% ì—°ì¥
            if scores[hp_idx] > min_score and scores[kn_idx] > min_score:
                dx, dy = kx - hx, ky - hy
                cx = max(0, min(image.width, kx + dx * 1.0))
                cy = max(0, min(image.height, ky + dy * 1.0))
                extra_points.append((cx, cy))
                print(f"   ğŸ¦¶ toe ì¶”ì •: ({cx:.0f}, {cy:.0f}) [hipâ†’knee 100%]")
            # ë¬´ë¦â†’ë°œëª© +150% ì—°ì¥
            if scores[kn_idx] > min_score and scores[ak_idx] > min_score:
                dx, dy = ax - kx, ay - ky
                cx = max(0, min(image.width, ax + dx * 1.5))
                cy = max(0, min(image.height, ay + dy * 1.5))
                extra_points.append((cx, cy))
                print(f"   ğŸ¦¶ toe ì¶”ì •: ({cx:.0f}, {cy:.0f}) [kneeâ†’ankle+150%]")

        # === ê·€ ì¶”ì •: ì½”â†’ê·€ ë°©í–¥ìœ¼ë¡œ ê·€ì—ì„œ +100% ì—°ì¥ ===
        # COCO: 0=nose, 3=L_ear, 4=R_ear
        for ear_idx in [3, 4]:
            if scores[0] > min_score and scores[ear_idx] > min_score:
                nx, ny = keypoints_xy[0]
                ex, ey = keypoints_xy[ear_idx]
                dx, dy = ex - nx, ey - ny
                cx = max(0, min(image.width, ex + dx * 1.0))
                cy = max(0, min(image.height, ey + dy * 1.0))
                extra_points.append((cx, cy))
                side = "L" if ear_idx == 3 else "R"
                print(f"   ğŸ‘‚ ear ì¶”ì •: ({cx:.0f}, {cy:.0f}) [noseâ†’{side}_ear+100%]")

        # === ë¨¸ë¦¬ ê¼­ëŒ€ê¸° ì¶”ì • (ì–´ê¹¨ ì¤‘ì â†’ëˆˆ ì¤‘ì  ë²¡í„° 170% ì—°ì¥) ===
        # COCO: 1=L_eye, 2=R_eye, 5=L_shoulder, 6=R_shoulder
        has_eyes = scores[1] > min_score and scores[2] > min_score
        has_shoulders = scores[5] > min_score and scores[6] > min_score

        if has_eyes and has_shoulders:
            mid_eye_x = (keypoints_xy[1][0] + keypoints_xy[2][0]) / 2
            mid_eye_y = (keypoints_xy[1][1] + keypoints_xy[2][1]) / 2
            mid_sh_x = (keypoints_xy[5][0] + keypoints_xy[6][0]) / 2
            mid_sh_y = (keypoints_xy[5][1] + keypoints_xy[6][1]) / 2
            dx = mid_eye_x - mid_sh_x
            dy = mid_eye_y - mid_sh_y
            crown_x = mid_eye_x + dx * 1.7
            crown_y = mid_eye_y + dy * 1.7
            crown_x = max(0, min(image.width, crown_x))
            crown_y = max(0, min(image.height, crown_y))
            extra_points.append((crown_x, crown_y))
            print(f"   ğŸ‘¤ ë¨¸ë¦¬ ê¼­ëŒ€ê¸° ì¶”ì •: ({crown_x:.0f}, {crown_y:.0f}) [ì–´ê¹¨â†’ëˆˆ 170%]")

        # ë°”ìš´ë”© ë°•ìŠ¤ ê³„ì‚° (ìœ íš¨ í‚¤í¬ì¸íŠ¸ + ì¶”ì • í¬ì¸íŠ¸ í•©ì‚°)
        all_x = [float(kp[0]) for kp in valid_kps] + [p[0] for p in extra_points]
        all_y = [float(kp[1]) for kp in valid_kps] + [p[1] for p in extra_points]

        kp_x_min = min(all_x)
        kp_y_min = min(all_y)
        kp_x_max = max(all_x)
        kp_y_max = max(all_y)

        # === ì €í•´ìƒë„ ì„¸ê·¸ë©˜í…Œì´ì…˜ ë§ˆìŠ¤í¬ë¡œ ì‹¤ë£¨ì—£ bbox ë³´ì™„ ===
        try:
            seg_start = time.time()
            seg_scale = min(seg_size / image.width, seg_size / image.height)
            seg_w = (int(image.width * seg_scale) // 32) * 32
            seg_h = (int(image.height * seg_scale) // 32) * 32
            seg_w = max(32, seg_w)
            seg_h = max(32, seg_h)

            seg_resized = image.resize((seg_w, seg_h), Image.Resampling.LANCZOS)
            seg_model = get_birefnet_model("portrait")
            seg_dev = next(seg_model.parameters()).device
            seg_tensor = transform_normalize(seg_resized).unsqueeze(0).to(seg_dev)
            if seg_dev.type != "cpu":
                seg_tensor = seg_tensor.half()

            with torch.no_grad():
                seg_pred = seg_model(seg_tensor)[-1].sigmoid().cpu()

            seg_mask = seg_pred[0].squeeze().float().numpy()
            # ì„ê³„ê°’ 0.5ë¡œ ì´ì§„í™”
            mask_binary = seg_mask > 0.5
            rows = np.any(mask_binary, axis=1)
            cols = np.any(mask_binary, axis=0)

            if rows.any() and cols.any():
                r_min, r_max = np.where(rows)[0][[0, -1]]
                c_min, c_max = np.where(cols)[0][[0, -1]]
                # ì›ë³¸ í•´ìƒë„ë¡œ ì¢Œí‘œ ë³€í™˜
                scale_x = image.width / seg_w
                scale_y = image.height / seg_h
                mask_x_min = c_min * scale_x
                mask_y_min = r_min * scale_y
                mask_x_max = (c_max + 1) * scale_x
                mask_y_max = (r_max + 1) * scale_y

                # ë§ˆìŠ¤í¬ bbox íŒ¨ë”©: ìœ„ 10%, ì¢Œìš° 5% (ê°€ëŠ” ë¨¸ë¦¬ì¹´ë½/íŒ” ë³´í˜¸)
                mask_h = mask_y_max - mask_y_min
                mask_w = mask_x_max - mask_x_min
                mask_y_min = max(0, mask_y_min - mask_h * 0.1)
                mask_x_min = max(0, mask_x_min - mask_w * 0.05)
                mask_x_max = min(image.width, mask_x_max + mask_w * 0.05)

                # í‚¤í¬ì¸íŠ¸ bboxì™€ ë§ˆìŠ¤í¬ bboxì˜ í•©ì§‘í•©
                x_min = min(kp_x_min, mask_x_min)
                y_min = min(kp_y_min, mask_y_min)
                x_max = max(kp_x_max, mask_x_max)
                y_max = max(kp_y_max, mask_y_max)
                mask_bbox = {"x_min": float(mask_x_min), "y_min": float(mask_y_min), "x_max": float(mask_x_max), "y_max": float(mask_y_max)}
                print(f"   ğŸ­ ë§ˆìŠ¤í¬ bbox: ({mask_x_min:.0f}, {mask_y_min:.0f})â†’({mask_x_max:.0f}, {mask_y_max:.0f}) [{time.time() - seg_start:.2f}ì´ˆ]")
            else:
                x_min, y_min, x_max, y_max = kp_x_min, kp_y_min, kp_x_max, kp_y_max
                mask_bbox = None
                print(f"   âš ï¸ ë§ˆìŠ¤í¬ì—ì„œ ì¸ë¬¼ ë¯¸ê°ì§€, í‚¤í¬ì¸íŠ¸ë§Œ ì‚¬ìš©")
        except Exception as seg_err:
            x_min, y_min, x_max, y_max = kp_x_min, kp_y_min, kp_x_max, kp_y_max
            mask_bbox = None
            print(f"   âš ï¸ ì„¸ê·¸ë©˜í…Œì´ì…˜ ì‹¤íŒ¨: {seg_err}, í‚¤í¬ì¸íŠ¸ë§Œ ì‚¬ìš©")

        kp_bbox = {"x_min": float(kp_x_min), "y_min": float(kp_y_min), "x_max": float(kp_x_max), "y_max": float(kp_y_max)}

        # íŒ¨ë”© ì—†ìŒ â€” ê´€ì ˆ ì¶”ì • + ë§ˆìŠ¤í¬ í•©ì§‘í•©ìœ¼ë¡œ ì»¤ë²„
        crop_x = max(0, int(x_min))
        crop_y = max(0, int(y_min))
        crop_x2 = min(image.width, int(x_max))
        crop_y2 = min(image.height, int(y_max))

        crop_w = crop_x2 - crop_x
        crop_h = crop_y2 - crop_y

        # í‚¤í¬ì¸íŠ¸ ì •ë³´ êµ¬ì„± (COCO 17)
        COCO_NAMES = [
            "nose", "left_eye", "right_eye", "left_ear", "right_ear",
            "left_shoulder", "right_shoulder", "left_elbow", "right_elbow",
            "left_wrist", "right_wrist", "left_hip", "right_hip",
            "left_knee", "right_knee", "left_ankle", "right_ankle",
        ]
        keypoints_list = []
        for i in range(len(keypoints_xy)):
            keypoints_list.append({
                "name": COCO_NAMES[i] if i < len(COCO_NAMES) else f"kp_{i}",
                "x": float(keypoints_xy[i][0]),
                "y": float(keypoints_xy[i][1]),
                "score": float(scores[i]),
            })

        # í¬ë¡­ ì˜ì—­ì´ ì›ë³¸ì˜ 90% ì´ìƒì´ë©´ í¬ë¡­ ë¶ˆí•„ìš” (ì •ë³´ëŠ” ë°˜í™˜)
        crop_area = crop_w * crop_h
        image_area = image.width * image.height
        if crop_area >= image_area * 0.9:
            print(f"âš ï¸ í¬ë¡­ ì˜ì—­ì´ ì›ë³¸ì˜ {crop_area / image_area * 100:.0f}%ë¡œ í¬ë¡­ ë¶ˆí•„ìš”")
            print(f"âš¡ ì™„ë£Œ! ì†Œìš”ì‹œê°„: {time.time() - start_time:.2f}ì´ˆ")
            print("-" * 40)
            clear_gpu_memory()
            response = {
                "cropped": False,
                "reason": "í¬ë¡­ ë¶ˆí•„ìš” (90% ì´ìƒ)",
                "crop": {"x": crop_x, "y": crop_y, "width": crop_w, "height": crop_h},
                "image_width": image.width,
                "image_height": image.height,
                "valid_keypoints": valid_count,
                "keypoints": keypoints_list,
                "kp_bbox": kp_bbox,
            }
            if mask_bbox:
                response["mask_bbox"] = mask_bbox
            return JSONResponse(content=response)

        print(f"âœ‚ï¸ í¬ë¡­ ì¢Œí‘œ: ({crop_x}, {crop_y}) {crop_w}x{crop_h} (ìœ íš¨ í‚¤í¬ì¸íŠ¸: {valid_count}ê°œ)")
        print(f"âš¡ ì™„ë£Œ! ì†Œìš”ì‹œê°„: {time.time() - start_time:.2f}ì´ˆ")
        print("-" * 40)

        clear_gpu_memory()
        response = {
            "cropped": True,
            "crop": {
                "x": crop_x,
                "y": crop_y,
                "width": crop_w,
                "height": crop_h,
            },
            "image_width": image.width,
            "image_height": image.height,
            "valid_keypoints": valid_count,
            "keypoints": keypoints_list,
            "kp_bbox": kp_bbox,
        }
        if mask_bbox:
            response["mask_bbox"] = mask_bbox
        return JSONResponse(content=response)

    except HTTPException:
        raise
    except Exception as e:
        clear_gpu_memory()
        print(f"âŒ ìŠ¤ë§ˆíŠ¸ í¬ë¡­ ì˜¤ë¥˜: {str(e)}")
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"ìŠ¤ë§ˆíŠ¸ í¬ë¡­ ì¤‘ ì˜¤ë¥˜: {str(e)}")

# ========== Ryan Book Automation API ==========

class ChildData(BaseModel):
    firstName: str
    lastName: str = ""
    fullName: str = ""
    gender: str = "boy"
    birthday: Optional[str] = None
    photo: Optional[str] = None
    photoNoBg: Optional[str] = None

class FavoriteObject(BaseModel):
    name: str
    photo: Optional[str] = None
    photoNoBg: Optional[str] = None
    emoji: str = "â“"
    josaMode: str = "friend"

class FamilyMember(BaseModel):
    id: str
    relation: str
    emoji: str
    photo: Optional[str] = None
    customName: Optional[str] = None

class BookRequest(BaseModel):
    child: ChildData
    objects: List[FavoriteObject] = []
    familyMembers: List[FamilyMember] = []

@app.get("/josa-preview")
async def josa_preview(name: str = Query(..., min_length=1, description="ì¡°ì‚¬ë¥¼ ì ìš©í•  ì´ë¦„")):
    """
    í•œê¸€ ì¡°ì‚¬ ë¯¸ë¦¬ë³´ê¸° API

    ì´ë¦„ì„ ì…ë ¥í•˜ë©´ 9ê°€ì§€ ì¡°ì‚¬ í˜•íƒœì˜ ì˜ˆì‹œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì‹¤ì‹œê°„ ì¡°ì‚¬ ë¯¸ë¦¬ë³´ê¸°ì— ì‚¬ìš©ë©ë‹ˆë‹¤.
    """
    if not RYAN_ENGINE_AVAILABLE:
        raise HTTPException(status_code=500, detail="Ryan Engineì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    josa = JosaUtils()
    demo = josa.generate_josa_demo(name)

    return JSONResponse(content={
        "success": True,
        "name": demo['name'],
        "hasBatchim": demo['has_batchim'],
        "examples": demo['examples']
    })

@app.post("/generate-book")
async def generate_book(request: BookRequest):
    """
    Ryan Book ìë™ ìƒì„± API

    ì‚¬ìš©ì ë°ì´í„°ë¥¼ ë°›ì•„ ì™„ì „í•œ ì±… ìŠ¤í™(final_book_spec.json)ì„ ìƒì„±í•©ë‹ˆë‹¤.

    ìš”ì²­ ì˜ˆì‹œ:
    {
        "child": {
            "firstName": "ë„í˜„",
            "lastName": "ê¹€",
            "gender": "boy"
        },
        "objects": [
            {"name": "í† ë¼", "emoji": "ğŸ°", "josaMode": "friend"},
            {"name": "í† ë§ˆí† ", "emoji": "ğŸ…", "josaMode": "object"}
        ],
        "familyMembers": [
            {"id": "mom", "relation": "ì—„ë§ˆ", "emoji": "ğŸ‘©"}
        ]
    }
    """
    if not RYAN_ENGINE_AVAILABLE:
        raise HTTPException(status_code=500, detail="Ryan Engineì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    print("-" * 40)
    print(f"ğŸ“š ì±… ìƒì„± ìš”ì²­: {request.child.firstName}")
    start_time = time.time()

    try:
        # í…Œë§ˆ íŒŒì¼ ê²½ë¡œ
        theme_path = Path(__file__).parent / "ryan_engine" / "themes" / "theme_ryan.json"

        if not theme_path.exists():
            raise HTTPException(status_code=500, detail="í…Œë§ˆ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        # BookGenerator ìƒì„±
        generator = BookGenerator(str(theme_path))

        # ìš”ì²­ ë°ì´í„°ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
        user_data = {
            'child': request.child.model_dump(),
            'objects': [obj.model_dump() for obj in request.objects],
            'familyMembers': [fam.model_dump() for fam in request.familyMembers],
        }

        # ì±… ìƒì„±
        book_spec = generator.generate_from_dict(user_data)

        # JSON ë³€í™˜
        book_json = generator.to_json(book_spec)

        # íŒŒì¼ë¡œ ì €ì¥ (ì„ íƒì )
        output_dir = Path(__file__).parent / "output"
        output_dir.mkdir(exist_ok=True)
        safe_name = re.sub(r'[^a-zA-Z0-9ê°€-í£_\-]', '_', request.child.firstName)
        output_path = output_dir / f"book_{safe_name}_{int(time.time())}.json"
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(book_json)

        print(f"âœ… ì±… ìƒì„± ì™„ë£Œ! íŒŒì¼: {output_path}")
        print(f"âš¡ ì†Œìš”ì‹œê°„: {time.time() - start_time:.2f}ì´ˆ")
        print("-" * 40)

        return JSONResponse(content={
            "success": True,
            "bookSpec": json.loads(book_json),
            "savedTo": str(output_path)
        })

    except HTTPException:
        raise
    except Exception as e:
        print(f"âŒ ì±… ìƒì„± ì˜¤ë¥˜: {str(e)}")
        traceback.print_exc()
        raise HTTPException(
            status_code=500,
            detail=f"ì±… ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )

@app.get("/health")
async def health_check():
    """ì„œë²„ ìƒíƒœ í™•ì¸"""
    return JSONResponse(content={
        "status": "ok",
        "device": device,
        "dtype": str(dtype),
        "ryan_engine": RYAN_ENGINE_AVAILABLE,
        "loaded_models": list(loaded_models.keys()) + (["ben2"] if ben2_model is not None else [])
    })

if __name__ == "__main__":
    import uvicorn
    import os
    port = int(os.environ.get("PORT", 5001))
    workers = int(os.environ.get("WORKERS", 1))
    uvicorn.run("server:app", host="0.0.0.0", port=port, workers=workers)